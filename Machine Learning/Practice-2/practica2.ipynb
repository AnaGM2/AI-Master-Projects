{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MApu3NB-bd2z"
      },
      "source": [
        "# Prﾃ｡ctica 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta prﾃ｡ctica ha sido realizada por:  \n",
        "- **Ana Gil Molina**  \n",
        "- **Ignacio Ruiz Chicano**  \n",
        "- **Juan Jesﾃｺs Torralba Mateos**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introducciﾃｳn\n",
        "\n",
        "En esta prﾃ｡ctica se presenta un problema de predicciﾃｳn de series temporales multivaluadas, que consiste en la predicciﾃｳn a 7 dﾃｭas del ﾃｳxido Nﾃｭtrico (`NOX`). Se dispone de una serie de datasets entre los aﾃｱos 2017 y 2022, con medidas horarias de una estaciﾃｳn sensora situada en la Alojorra. Dichos datasets constan de dos grupos de datos: datos sobre calidad del aire y datos climﾃ｡ticos.\n",
        "\n",
        "| Tipo de Datos              | Variables              | Unidad de Medida |\n",
        "|----------------------------|------------------------|------------------|\n",
        "| **Datos Calidad del Aire** | ﾃ度ido Nﾃｭtrico (NO)     | 撩搗/搗堋ｳ搗         |\n",
        "|  **Datos Calidad del Aire**                           | Diﾃｳxido de Nitrﾃｳgeno (NO2) | 撩搗/搗堋ｳ搗         |\n",
        "|   **Datos Calidad del Aire**                          | Disulfuro de Azufre (SO2) | 撩搗/搗堋ｳ搗         |\n",
        "| **Datos Calidad del Aire** | Ozono (O3)            | 撩搗/搗堋ｳ搗         |\n",
        "| **Datos Calidad del Aire** | ﾃ度ido de Nitrﾃｳgeno (NOX)            | 撩搗/搗堋ｳ搗         |\n",
        "|   **Datos Calidad del Aire**                          | PM10                  | 撩搗/搗堋ｳ搗         |\n",
        "|  **Datos Calidad del Aire**                           | Benceno (C6H6)        | 撩搗/搗堋ｳ搗         |\n",
        "|   **Datos Calidad del Aire**                          | Tolueno (C7H8)        | 撩搗/搗堋ｳ搗         |\n",
        "|   **Datos Calidad del Aire**                          | Xileno (XIL)          | 撩搗/搗堋ｳ搗         |\n",
        "| **Datos Meteorolﾃｳgicos**   | Temperatura media (TMP) | ﾂｺC              |\n",
        "| **Datos Meteorolﾃｳgicos**                           | Humedad Relativa (HR) | % H.R.           |\n",
        "| **Datos Meteorolﾃｳgicos**                           | Direcciﾃｳn del viento   | ﾂｺ                |\n",
        "| **Datos Meteorolﾃｳgicos**                           | Presiﾃｳn atmosfﾃｩrica (PRB) | Milibares      |\n",
        "| **Datos Meteorolﾃｳgicos**                           | Velocidad del Viento (W) | 搗/搗            |\n",
        "|  **Datos Meteorolﾃｳgicos**                          | Radiaciﾃｳn Solar (RS)   | 搗/搗堋ｳ           |\n",
        "\n",
        "A lo largo de esta prﾃ｡ctica, utilizaremos una variedad de modelos, comenzando con enfoques mﾃ｡s simples en los que ﾃｺnicamente emplearemos la variable `NOX` para entrenar el modelo y hacer las predicciones. Posteriormente, crearemos modelos mﾃ｡s complejos, en los cuales primero incorporaremos los datos de calidad del aire, y seguidamente, usaremos estos mismos modelos, pero aﾃｱadiendo ademﾃ｡s los datos meteorolﾃｳgicos para evaluar su impacto en las predicciones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUZgmC5ybd21"
      },
      "source": [
        "## Librerias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la realizaciﾃｳn de esta prﾃ｡ctica hemos utilizado las siguientes librerﾃｭas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imtg-Q2fbd21"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import boxcox, shapiro, kurtosis, skew\n",
        "from scipy.special import inv_boxcox\n",
        "import statsmodels.api as sm\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.metrics import (\n",
        "    mean_absolute_error, \n",
        "    root_mean_squared_error, \n",
        "    mean_squared_error, \n",
        "    make_scorer, \n",
        "    mean_absolute_percentage_error\n",
        ")\n",
        "\n",
        "import skforecast\n",
        "from skforecast.datasets import fetch_dataset\n",
        "from skforecast.model_selection import backtesting_forecaster, TimeSeriesFold\n",
        "from skforecast.recursive import ForecasterRecursive, ForecasterEquivalentDate\n",
        "\n",
        "from IPython.core.magic import register_cell_magic\n",
        "import IPython\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
        "from skforecast.model_selection import grid_search_forecaster\n",
        "from skforecast.model_selection import TimeSeriesFold\n",
        "\n",
        "from skforecast.exceptions import LongTrainingWarning\n",
        "from skforecast.utils.utils import load_forecaster, save_forecaster\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "\n",
        "import warnings\n",
        "\n",
        "import joblib\n",
        "from skforecast.direct import ForecasterDirectMultiVariate\n",
        "from skforecast.preprocessing import RollingFeatures\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import platform\n",
        "import importlib.metadata as imp\n",
        "\n",
        "print(f'Python Version == {platform.python_version()}')\n",
        "\n",
        "paquetes = [\n",
        "    'matplotlib',\n",
        "    'numpy',\n",
        "    'pandas',\n",
        "    'seaborn',\n",
        "    'scikit-learn',\n",
        "    'scipy',\n",
        "    'xgboost',\n",
        "    'skforecast',\n",
        "    'IPython',\n",
        "    'statsmodels'\n",
        "]\n",
        "\n",
        "for paq in paquetes:\n",
        "    print(f'{paq} == {imp.version(paq)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
        "warnings.simplefilter('ignore', category=LongTrainingWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG1XeJ0Bbd22"
      },
      "outputs": [],
      "source": [
        "# Establecemos una semilla\n",
        "semilla = 123\n",
        "np.random.seed(semilla)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_XSAgi2bd23"
      },
      "source": [
        "## Modo de ejecuciﾃｳn del notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6m8Dhz9bd23"
      },
      "source": [
        "Dependiendo del modo de ejecuciﾃｳn del notebook, se ejecutarﾃ｡n todas las celdas o todas las celdas excepto las que contienen la creaciﾃｳn de los modelos.\n",
        "Por defecto, la variable `saltarse_generacion_modelos` estﾃ｡ establecida en `True`, por lo que se omitirﾃ｡ la generaciﾃｳn de los modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yPZP9Uyjbd23"
      },
      "outputs": [],
      "source": [
        "saltarse_generacion_modelos = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8knGf-gbd23"
      },
      "source": [
        "Esta es la funciﾃｳn que nos ayudarﾃ｡ a poder ejecutar el notebook en modo de generaciﾃｳn de modelos o en modo de carga de modelos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HFbXB3y0bd23"
      },
      "outputs": [],
      "source": [
        "@register_cell_magic\n",
        "def skip_if(line, cell):\n",
        "    # Evalﾃｺa la condiciﾃｳn en 'line' y decide si ejecutar la celda\n",
        "    if eval(line):\n",
        "        return  # Si la condiciﾃｳn es True, no ejecuta la celda\n",
        "    # Si la condiciﾃｳn es False, ejecuta la celda\n",
        "    get_ipython().run_cell(cell)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yuBRe6Qbd24"
      },
      "source": [
        "## Cargamos los datos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru88AT0mbd24"
      },
      "source": [
        "Vamos a leer todos los archivos `.xls` por separado y los vamos a concatenar en un ﾃｺnico DataFrame. Ademﾃ｡s, eliminamos las unidades de medida que acompaﾃｱan al nombre de las columnas para una mejor legibilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0KgCSh4bd24",
        "outputId": "f8b1d6af-a824-4503-e716-e50f0cc76d64"
      },
      "outputs": [],
      "source": [
        "rutaBase = './data/'\n",
        "nombreComun = 'Aljorrahorarias'\n",
        "anos = ['2017', '2018', '2019', '2020', '2021', '2022']\n",
        "\n",
        "dataframes = []\n",
        "\n",
        "for ano in anos:\n",
        "    archivo = f\"{rutaBase}{nombreComun}{ano}.xls\"\n",
        "    if os.path.exists(archivo):\n",
        "        df = pd.read_excel(archivo, engine=\"xlrd\", header=1)\n",
        "        dataframes.append(df)\n",
        "\n",
        "df_concatenado = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "df_concatenado.columns = df.columns.str.replace(r\"\\s*\\(.*?\\)\", \"\", regex=True)\n",
        "df_concatenado.to_csv(f\"{rutaBase}{nombreComun}2017_2022.csv\", index=False)\n",
        "\n",
        "#df_concatenado_aire = df_concatenado.drop(columns=['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "#df_concatenado_aire.to_csv(f\"{rutaBase}{nombreComun}2017_2022_aire.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a visualizar las columnas para ver si hemos eliminado bien las unidades de medida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_c19QyZbd25",
        "outputId": "82f896a3-d097-4c50-9dad-d06a6807ca53"
      },
      "outputs": [],
      "source": [
        "df_concatenado.columns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvhnKQ95bd25"
      },
      "source": [
        "## Anﾃ｡lisis exploratorio de los datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq7Oa4grbd25",
        "outputId": "c340edd9-f58d-4c03-db72-62d78998df0e"
      },
      "outputs": [],
      "source": [
        "print(df_concatenado.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iA5In2f6bd25"
      },
      "source": [
        "Tenemos 15 variables numﾃｩricas donde tenemos datos procedentes de dos fuentes distintas: La primera fuente es de la calidad del aire y la segunda son datos meteorolﾃｳgicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6Q__mS4bd26"
      },
      "source": [
        "Lo primero que haremos serﾃ｡ pasar los meses en espaﾃｱol a inglﾃｩs, para poder trabajar con ellos de forma mﾃ｡s sencilla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxKyhOgIbd26"
      },
      "outputs": [],
      "source": [
        "meses_espanol_a_ingles = {\n",
        "    'ene': 'Jan', 'feb': 'Feb', 'mar': 'Mar', 'abr': 'Apr', 'may': 'May', 'jun': 'Jun',\n",
        "    'jul': 'Jul', 'ago': 'Aug', 'sep': 'Sep', 'oct': 'Oct', 'nov': 'Nov', 'dic': 'Dec'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Preprocesamiento de las fechas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Convertimos las fechas a minﾃｺsculas y reemplazamos los nombres de los meses en espaﾃｱol por sus equivalentes en inglﾃｩs. Luego, transformamos ambos dataframes al formato `datetime` y configuramos la columna de fecha como ﾃｭndice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpeZNbrxbd26"
      },
      "outputs": [],
      "source": [
        "df_concatenado['Fecha'] = df_concatenado['Fecha'].str.lower().replace(meses_espanol_a_ingles, regex=True)\n",
        "df_concatenado['Fecha'] = pd.to_datetime(df_concatenado['Fecha'], format='%d %b. %Y %H:%M', errors='coerce')\n",
        "df_concatenado.set_index('Fecha', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estudio de los valores nulos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KXHL0j4Jbd26"
      },
      "source": [
        "Queremos conocer la cantidad de valores nulos por columna. Para ello, basta con inspeccionar el dataframe `df_concatenado`, ya que `df_concatenado_aire` es un subconjunto de este.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 607
        },
        "id": "hwQc5Hdibd26",
        "outputId": "d4fd8728-9c22-42ce-a843-d34c895ca624"
      },
      "outputs": [],
      "source": [
        "nulos = df_concatenado.isnull().sum()\n",
        "no_nulos = df_concatenado.notnull().sum()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(df_concatenado.columns, no_nulos, label='Valores No Nulos', color='#98FB98', edgecolor='black')\n",
        "plt.bar(df_concatenado.columns, nulos, bottom=no_nulos, label='Valores Nulos', color='#FFA07A', edgecolor='black')\n",
        "\n",
        "plt.title('Valores Nulos y No Nulos por Columna', fontsize=16)\n",
        "plt.xlabel('Columnas', fontsize=12)\n",
        "plt.ylabel('Cantidad de valores', fontsize=12)\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.legend(loc = 'lower left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lGoOLVUrbd26"
      },
      "source": [
        "Como podemos observar todas las columnas tienen valores nulos, sin embargo, destaca una de ellas respecto a todas las demﾃ｡s, la columna `Ruido`, la cuﾃ｡l tiene todos los valores `NaN`. Por ello procedemos a eliminarla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poNFgwfpbd26"
      },
      "outputs": [],
      "source": [
        "df_concatenado.drop(columns=['Ruido'], inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGmyeB-cbd27"
      },
      "source": [
        "Aﾃｺn seguimos teniendo 3 variables que destacan respecto a las demﾃ｡s en cuanto a valores nulos, estas son: `C6H6`, `C7H8` y `XIL`. Vamos a ver cuﾃ｡ntos valores con valor `NaN` poseen de manera numﾃｩrica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C33Svl74bd27",
        "outputId": "d3f38176-b30a-4c6d-e6ba-371c88b89ba3"
      },
      "outputs": [],
      "source": [
        "columna_xil = 'XIL'\n",
        "columna_c7h8 = 'C7H8'\n",
        "columna_c6h6 = 'C6H6'\n",
        "\n",
        "valores_nulos_penultima = df_concatenado[columna_xil].isnull().sum()\n",
        "valores_nulos_antepenultima = df_concatenado[columna_c7h8].isnull().sum()\n",
        "valores_nulos_anteantepenultima = df_concatenado[columna_c6h6].isnull().sum()\n",
        "\n",
        "print(f\"Valores nulos de la penﾃｺltima columna ({columna_xil}):\")\n",
        "print(valores_nulos_penultima)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"Valores nulos de la antepenﾃｺltima columna ({columna_c7h8}):\")\n",
        "print(valores_nulos_antepenultima)\n",
        "print(\"\\n\")\n",
        "\n",
        "print(f\"Valores nulos de la anteantepenﾃｺltima columna ({columna_c6h6}):\")\n",
        "print(valores_nulos_anteantepenultima)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROaMMjeObd27"
      },
      "source": [
        "Observamos que las tres columnas tienen exactamente la misma cantidad de valores nulos. Ahora, queremos verificar si estos valores nulos se distribuyen de manera similar a lo largo del tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "cC6FZMoZbd27",
        "outputId": "6f2344b0-b6e2-42ca-8df8-87b471ebf82c"
      },
      "outputs": [],
      "source": [
        "null_penultima = df_concatenado[columna_xil].isnull()\n",
        "null_antepenultima = df_concatenado[columna_c7h8].isnull()\n",
        "null_anteantepenultima = df_concatenado[columna_c6h6].isnull()\n",
        "\n",
        "counts_penultima = df_concatenado[null_penultima].resample('ME').size()\n",
        "counts_antepenultima = df_concatenado[null_antepenultima].resample('ME').size()\n",
        "counts_anteantepenultima = df_concatenado[null_anteantepenultima].resample('ME').size()\n",
        "\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "plt.subplot(3, 1, 1)\n",
        "counts_penultima.plot(color='blue', label=columna_xil)\n",
        "plt.title(f\"Distribuciﾃｳn de valores nulos en la columna {columna_xil}\")\n",
        "plt.xlabel(\"Fecha\")\n",
        "plt.ylabel(\"Cantidad de valores nulos\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 2)\n",
        "counts_antepenultima.plot(color='green', label=columna_c7h8)\n",
        "plt.title(f\"Distribuciﾃｳn de valores nulos en la columna {columna_c7h8}\")\n",
        "plt.xlabel(\"Fecha\")\n",
        "plt.ylabel(\"Cantidad de valores nulos\")\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(3, 1, 3)\n",
        "counts_anteantepenultima.plot(color='red', label=columna_c6h6)\n",
        "plt.title(f\"Distribuciﾃｳn de valores nulos en la columna {columna_c6h6}\")\n",
        "plt.xlabel(\"Fecha\")\n",
        "plt.ylabel(\"Cantidad de valores nulos\")\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aunque parece que siguen la misma distribuciﾃｳn a lo largo del tiempo, para visualizarlo con mayor claridad, vamos a superponer los valores nulos de las tres columnas en un solo grﾃ｡fico.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "counts_penultima.plot(color='blue', label=columna_xil)\n",
        "counts_antepenultima.plot(color='green', label=columna_c7h8)\n",
        "counts_anteantepenultima.plot(color='red', label=columna_c6h6)\n",
        "\n",
        "plt.title(\"Distribuciﾃｳn superpuesta de valores no nulos en las columnas\")\n",
        "plt.xlabel(\"Fecha\")\n",
        "plt.ylabel(\"Cantidad de valores no nulos\")\n",
        "plt.legend()  \n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5LMxtPbbd27"
      },
      "source": [
        "Como se puede observar, las tres variables tienen los valores nulos distribuidos de la misma manera. Dada la alta cantidad de valores nulos en estas columnas, decidimos eliminarlas de los datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZjORfdebd27",
        "outputId": "1664bb67-944a-4a56-e1a8-51ff3bcb2b16"
      },
      "outputs": [],
      "source": [
        "df_concatenado.drop(columns=[columna_c6h6, columna_c7h8, columna_xil], inplace=True)\n",
        "\n",
        "print(\"Columnas de df_concatenado:\")\n",
        "print(df_concatenado.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estudio de la correlaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3K3JvcSvbd28"
      },
      "source": [
        "Tras decidir quﾃｩ variables eliminar debido a la gran cantidad de valores nulos, procederemos a analizar la correlaciﾃｳn entre las variables restantes. Para ello, utilizaremos un mapa de calor.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 807
        },
        "id": "Am1wK9uAbd28",
        "outputId": "0d0a2ffe-112c-4504-b6f2-5953af02d87b"
      },
      "outputs": [],
      "source": [
        "correlation_matrix = df_concatenado.corr()\n",
        "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool), k=1)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', cbar=True, square=True, mask=mask)\n",
        "plt.title(\"Matriz de Correlaciﾃｳn - Parte Inferior\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7U2AWttbd2_"
      },
      "source": [
        "Lo mﾃ｡s destacable del mapa de calor es que las variable `NOX` tiene una correlaciﾃｳn muy alta con las variables `NO` y `NO2`, lo cual es lﾃｳgico ya que el `NOX` (ﾃ度idos de Nitrﾃｳgeno) es la suma de `NO` (ﾃ度ido Nﾃｭtrico) y `NO2` (Diﾃｳxido de Nitrﾃｳgeno)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Anﾃ｡lisis a travﾃｩs de boxplots"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Todas las variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nU3OGoYbbd2_"
      },
      "source": [
        "Vamos a observar a travﾃｩs de grﾃ｡ficas de boxplot la distribuciﾃｳn de los valores de las variables predictoras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        },
        "id": "ahHhoNjSbd2_",
        "outputId": "a3bb9914-ead0-4baa-a445-8585b8362ac6"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "sns.set_palette('pastel')\n",
        "sns.boxplot(data=df_concatenado.select_dtypes(include='number'))\n",
        "\n",
        "plt.xlabel('Variables numﾃｩricas')\n",
        "plt.ylabel('Distribuciﾃｳn')\n",
        "plt.title('Boxplot de todas las variables numﾃｩricas')\n",
        "plt.xticks(rotation=45) \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos diferentes rangos de valores en las variables predictoras y muchos outliers que pueden ser debidos a los fallos de los sensores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Solo la variable NOX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Realmente el grﾃ｡fico anterior no nos dice mucho de la variable `NOX`, por lo que vamos a realizar un boxplot de la variable `NOX` a lo largo de diferentes periodos temporales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para ello lo primero que vamos a hacer serﾃ｡ crear una copia temporal del dataframe `df_concatenado` para poder trabajar con ella sin modificar el dataframe original."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_temp = df_concatenado.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a visualizar la distribuciﾃｳn de la variable `NOX` por las horas del dﾃｭa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "df_temp['Hora'] = df_temp.index.hour + 1\n",
        "df_temp.boxplot(column='NOX', by='Hora', ax=ax, flierprops={'markersize': 3, 'alpha': 0.3})\n",
        "df_temp.groupby('Hora')['NOX'].median().plot(style='o-', linewidth=0.8, ax=ax)\n",
        "ax.set_ylabel('NOX')\n",
        "ax.set_title('Distribuciﾃｳn de NOX por hora del dﾃｭa', fontsize=9)\n",
        "plt.suptitle(\"\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las concentraciones de NOX aumentan de forma significativa entre las **8:00 y las 12:00 horas**, coincidiendo con el inicio y desarrollo de las jornadas laborales. Otro pico notable se registra entre las **20:00 y 24:00 horas**, lo que podrﾃｭa estar relacionado con turnos nocturnos o procesos industriales que continﾃｺan operando mﾃ｡s allﾃ｡ del horario comercial habitual. En contraste, durante la **madrugada (1:00 a 6:00 horas)**, las concentraciones de NOX son considerablemente mﾃ｡s bajas, algo esperable debido a la **reducciﾃｳn de la actividad humana e industrial**, asﾃｭ como del transporte.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a visualizar la distribuciﾃｳn de la variable `NOX` por dﾃｭas de la semana.\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "df_temp['dia_semana'] = df_temp.index.day_of_week + 1\n",
        "df_temp.boxplot(column='NOX', by='dia_semana', ax=ax, flierprops={'markersize': 3, 'alpha': 0.3})\n",
        "df_temp.groupby('dia_semana')['NOX'].median().plot(style='o-', linewidth=0.8, ax=ax)\n",
        "ax.set_ylabel('NOX')\n",
        "ax.set_title('Distribuciﾃｳn de NOX por dﾃｭa de la semana', fontsize=9)\n",
        "plt.suptitle(\"\")  # Para eliminar el tﾃｭtulo adicional\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las concentraciones de NOX se mantienen relativamente **constantes a lo largo de la semana**, con ligeras variaciones y una mayor dispersiﾃｳn de valores atﾃｭpicos en los dﾃｭas **laborales (lunes a viernes)**, lo que podrﾃｭa estar asociado a la **actividad industrial y el trﾃ｡fico**. Durante el **fin de semana (sﾃ｡bado y domingo)**, las concentraciones tienden a ser ligeramente menores, reflejando una posible **reducciﾃｳn de la actividad industrial y humana**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a visualizar la distribuciﾃｳn de la variable `NOX` por meses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "df_temp['mes'] = df_temp.index.month\n",
        "df_temp.boxplot(column='NOX', by='mes', ax=ax, flierprops={'markersize': 3, 'alpha': 0.3})\n",
        "df_temp.groupby('mes')['NOX'].median().plot(style='o-', linewidth=0.8, ax=ax)\n",
        "ax.set_ylabel('NOX')\n",
        "ax.set_title('Distribuciﾃｳn de NOX por mes', fontsize=9)\n",
        "plt.suptitle(\"\")  # Para eliminar el tﾃｭtulo adicional\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las concentraciones de NOX son **mﾃ｡s altas en los primeros meses del aﾃｱo (enero a marzo)**, con una mayor dispersiﾃｳn de valores atﾃｭpicos, lo que sugiere una posible **intensificaciﾃｳn de la actividad industrial y de transporte** en esta ﾃｩpoca. A partir de **abril**, las concentraciones disminuyen progresivamente, alcanzando los niveles mﾃ｡s bajos en los meses de **verano (junio a septiembre)**, probablemente debido a una **reducciﾃｳn en la actividad industrial** y mejores condiciones atmosfﾃｩricas para la dispersiﾃｳn de contaminantes. Hacia el final del aﾃｱo (**noviembre y diciembre**), se observa un ligero repunte, posiblemente relacionado con un aumento en la **actividad comercial e industrial**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a visualizar la distribuciﾃｳn de la variable `NOX` por meses a lo largo de los aﾃｱos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_temp['YearMonth'] = df_temp.index.to_period('M')  \n",
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "sns.set_palette('pastel')\n",
        "\n",
        "sns.boxplot(x='YearMonth', y='NOX', data=df_temp, ax=ax)\n",
        "\n",
        "ax.set_xlabel('Mes y Aﾃｱo')\n",
        "ax.set_ylabel('NOX')\n",
        "ax.set_title('Distribuciﾃｳn de NOX por mes a lo largo de los aﾃｱos')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se evidencia una **tendencia bajista** desde 2017. Los valores mﾃ｡s altos y dispersos al inicio disminuyen notablemente despuﾃｩs de **2020**, coincidiendo posiblemente con la pandemia de COVID-19. Se observan **valores atﾃｭpicos frecuentes**, especialmente en los primeros meses de cada aﾃｱo (enero y febrero), lo que sugiere patrones estacionales asociados a condiciones climﾃ｡ticas o emisiones. Es importante seﾃｱalar que **en abril de 2017 no hay datos disponibles de NOX**, lo cual puede influir en el anﾃ｡lisis de esa parte del aﾃｱo. Desde **2021**, las concentraciones son mﾃ｡s bajas y estables, con menor dispersiﾃｳn y menos valores extremos. En general, la grﾃ｡fica refleja una **reducciﾃｳn sostenida** de NOX, con picos ocasionales que resaltan en ciertos periodos.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Interpolaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMBjnfmkbd3A"
      },
      "source": [
        "Vamos a probar la interpolaciﾃｳn lineal y observaremos la varianza de la columna NOX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 758
        },
        "id": "-Pj8Y0ribd3A",
        "outputId": "acf64ef7-52a7-439d-fe40-dcd4fb524d36"
      },
      "outputs": [],
      "source": [
        "df_concatenado_interpolado_lineal = df_concatenado.interpolate(method='linear')\n",
        "\n",
        "# Configuraciﾃｳn de grﾃ｡ficos\n",
        "_, axs = plt.subplots(1, 2, figsize=(20, 10))\n",
        "series = [\n",
        "    df_concatenado['NOX'],\n",
        "    df_concatenado_interpolado_lineal['NOX']\n",
        "]\n",
        "titulos = [\n",
        "    'Distribuciﾃｳn Original de NOX',\n",
        "    'Distribuciﾃｳn con Interpolaciﾃｳn Lineal'\n",
        "]\n",
        "\n",
        "sns.set(font_scale=1)\n",
        "axs = axs.flatten()\n",
        "\n",
        "for data, ax, titulo in zip(series, axs, titulos):\n",
        "    sns.histplot(data, kde=True, stat='count', ax=ax)\n",
        "    ax.set_title(titulo)\n",
        "    ax.set_xlabel('Valores de NOX')\n",
        "    legend = ax.get_legend()\n",
        "    if legend is not None:\n",
        "        legend.set_visible(False)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LSn53hjbd3A"
      },
      "source": [
        "Podemos notar que la varianza apenas ha sufrido cambios significativos. Ahora, procederemos a verificar si alguna de las columnas contiene valores nulos despuﾃｩs de realizar la interpolaciﾃｳn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZYnAw_5bd3A",
        "outputId": "63312b70-e0a5-42dd-e421-15076e1fa0c0"
      },
      "outputs": [],
      "source": [
        "print(df_concatenado_interpolado_lineal.isna().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXxfLRc5bd3A"
      },
      "source": [
        "Como lo que nos interesa es predecir la variable `NOX` a siete dﾃｭas, vamos a submuestrear los datos para pasar de observaciones de cada hora a observaciones de cada dﾃｭa. Para ello vamos a usar distintas agregaciones dependiendo de la naturaleza de la variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "<u>*Variables con Promedio Diario (Media)*</u>\n",
        "\n",
        "- **ﾃ度ido Nﾃｭtrico (NO)**\n",
        "- **Diﾃｳxido de Nitrﾃｳgeno (NO2)**\n",
        "- **Disulfuro de Azufre (SO2)**\n",
        "- **ﾃ度idos de Nitrﾃｳgeno (NOX)**\n",
        "- **Partﾃｭculas PM10**\n",
        "- **Temperatura (TMP)**\n",
        "- **Humedad Relativa (HR)**\n",
        "- **Presiﾃｳn atmosfﾃｩrica (PRB)**\n",
        "- **Velocidad del viento (VV)**\n",
        "\n",
        "Para estos contaminantes y variables meteorolﾃｳgicas, el promedio diario es el mﾃｩtodo mﾃ｡s adecuado para obtener una visiﾃｳn estable y representativa de su comportamiento a lo largo del dﾃｭa. Estas variables tienden a tener variaciones a corto plazo, pero para la predicciﾃｳn de la calidad del aire a 7 dﾃｭas, es importante obtener una visiﾃｳn general de los niveles diarios. El uso del promedio permite capturar las tendencias generales sin que los picos momentﾃ｡neos alteren excesivamente el anﾃ｡lisis.\n",
        "\n",
        "---\n",
        "\n",
        "<u>*Variables con Mﾃ｡ximo Diario (Mﾃ｡ximo)*</u>\n",
        "\n",
        "- **Ozono (O3)**\n",
        "\n",
        "El ozono puede presentar picos significativos durante ciertos perﾃｭodos del dﾃｭa, particularmente en condiciones de alta radiaciﾃｳn solar. El **mﾃ｡ximo diario** es el mﾃｩtodo adecuado para capturar estos picos y entender los momentos de mayor concentraciﾃｳn.\n",
        "\n",
        "---\n",
        "\n",
        "<u>*Variables con Suma Diaria (Suma)*</u>\n",
        "\n",
        "- **Radiaciﾃｳn Solar (RS)**\n",
        "\n",
        "La radiaciﾃｳn solar se acumula durante el dﾃｭa, lo que hace que la **suma diaria** sea el enfoque mﾃ｡s adecuado para representar la cantidad total de radiaciﾃｳn recibida en un dﾃｭa. Esto es crucial para entender la influencia de la radiaciﾃｳn en la generaciﾃｳn de ozono (O3) y otros contaminantes fotoquﾃｭmicos.\n",
        "\n",
        "---\n",
        "\n",
        "<u>*Variables con Promedio Vectorial*</u>\n",
        "\n",
        "- **Direcciﾃｳn del Viento (DD)**\n",
        "\n",
        " \n",
        "La direcciﾃｳn del viento se representa mediante un ﾃ｡ngulo, lo que hace que no sea adecuado usar un promedio aritmﾃｩtico simple. En lugar de calcular la media, se realiza un **promedio vectorial** de los componentes del viento (U y V). Este enfoque tiene en cuenta tanto la magnitud como la direcciﾃｳn del viento, lo que permite obtener un valor coherente para la direcciﾃｳn promedio del viento en un dﾃｭa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la agregaciﾃｳn por promedio vectorial vamos a usar la siguiente funciﾃｳn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def promedio_vectorial(direcciones):\n",
        "    \n",
        "    # Convertir de grados a radianes\n",
        "    radianes = np.radians(direcciones)\n",
        "    \n",
        "    # Calcular componentes cartesianas\n",
        "    x = np.cos(radianes)\n",
        "    y = np.sin(radianes)\n",
        "    \n",
        "    # Promediar las componentes x e y\n",
        "    x_promedio = np.mean(x)\n",
        "    y_promedio = np.mean(y)\n",
        "    \n",
        "    # Calcular el ﾃ｡ngulo promedio (direcciﾃｳn del viento)\n",
        "    direccion_promedio = np.degrees(np.arctan2(y_promedio, x_promedio)) % 360\n",
        "    return direccion_promedio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Apliquemos la funciﾃｳn `promedio_vectorial` a la columna `DD` para obtener la direcciﾃｳn del viento promedio diario y guardﾃｩmosla en una nueva columna llamada `dd_resample`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dd_resample = df_concatenado_interpolado_lineal['DD'].resample('D').apply(promedio_vectorial)\n",
        "print(dd_resample.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Defimos el resto de agregaciones para las variables restantes y al final aﾃｱadimos la columna `dd_resample` al dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRd8WbQNbd3A"
      },
      "outputs": [],
      "source": [
        "columnas_media = ['NO', 'NO2', 'SO2', 'NOX', 'PM10', 'TMP', 'HR', 'PRB', 'VV']\n",
        "columnas_max = ['O3']\n",
        "columnas_suma = ['RS']\n",
        "\n",
        "agg_methods = {col: 'mean' for col in columnas_media}\n",
        "agg_methods.update({col: 'max' for col in columnas_max})\n",
        "agg_methods.update({col: 'sum' for col in columnas_suma})\n",
        "\n",
        "df_concatenado_diario = df_concatenado_interpolado_lineal.resample('D').agg(agg_methods)\n",
        "df_concatenado_diario['DD'] = dd_resample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_concatenado_diario.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Descomposiciﾃｳn de la serie temporal NOX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Antes de descomponer la serie en distintos componentes vamos a visualizarla."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "zoom = ('2017-01-01', '2017-06-30')  \n",
        "\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "grid = plt.GridSpec(nrows=8, ncols=1, hspace=0.2, wspace=0)\n",
        "\n",
        "main_ax = fig.add_subplot(grid[1:3, :])\n",
        "zoom_ax = fig.add_subplot(grid[4:, :])\n",
        "\n",
        "df_concatenado_diario['NOX'].plot(ax=main_ax, c='black', alpha=0.5, linewidth=0.5)\n",
        "\n",
        "min_y = df_concatenado_diario['NOX'].min()\n",
        "max_y = df_concatenado_diario['NOX'].max()\n",
        "\n",
        "main_ax.fill_between(zoom, min_y, max_y, facecolor='blue', alpha=0.5, zorder=0)\n",
        "\n",
        "start_date = df_concatenado_diario.index.min().strftime('%Y-%m-%d')\n",
        "end_date = df_concatenado_diario.index.max().strftime('%Y-%m-%d')\n",
        "\n",
        "start_zoom = zoom[0]\n",
        "start_zoom = pd.to_datetime(start_zoom).strftime('%Y-%m-%d')\n",
        "end_zoom = zoom[1]\n",
        "end_zoom = pd.to_datetime(end_zoom).strftime('%Y-%m-%d')\n",
        "\n",
        "\n",
        "main_ax.set_xlabel('')\n",
        "main_ax.set_title(f'Concentraciﾃｳn de NOX: Desde {start_date} hasta {end_date}', fontsize=10)\n",
        "\n",
        "df_concatenado_diario.loc[zoom[0]: zoom[1], 'NOX'].plot(ax=zoom_ax, color='blue', linewidth=1)\n",
        "\n",
        "zoom_ax.set_title(f'Concentraciﾃｳn de NOX: Desde {start_zoom} hasta {end_zoom}', fontsize=10)\n",
        "zoom_ax.set_xlabel('')\n",
        "\n",
        "plt.subplots_adjust(hspace=0.2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como se puede observar, entre mediados de abril y mediados de junio de 2017, aparece una lﾃｭnea diagonal que corresponde a la interpolaciﾃｳn lineal aplicada para rellenar los valores nulos. Dado que esta lﾃｭnea no sigue la tendencia de los datos reales, se procederﾃ｡ a eliminar los registros comprendidos entre el 1 de enero de 2017 y el 30 de junio de 2017."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "id": "w87N_oevbd3B",
        "outputId": "98a1ecfb-e2ea-4c1c-a33d-a9874c852036"
      },
      "outputs": [],
      "source": [
        "start_date = '2017-01-01'\n",
        "end_date = '2017-06-30'\n",
        "\n",
        "df_concatenado_diario = df_concatenado_diario.loc[~((df_concatenado_diario.index >= start_date) & (df_concatenado_diario.index <= end_date))]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a comprobar si se ha eliminado correctamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como podemos observar, se han eliminado correctamente los registros comprendidos entre el 1 de enero de 2017 y el 30 de junio de 2017. \n",
        "\n",
        "Ahora, vamos a descomponer la serie temporal `NOX` en sus componentes: tendencia, estacionalidad y residuo.\n",
        "En el parﾃ｡metro ``period`` vamos a poner 365 ya que queremos ver la estacionalidad anual."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Nktszfcpbd3B",
        "outputId": "d61590cb-8729-4be2-dff7-e271ced62890"
      },
      "outputs": [],
      "source": [
        "columna_interes = 'NOX'\n",
        "serie_temporal = df_concatenado_diario[columna_interes]\n",
        "\n",
        "nox_dec = sm.tsa.seasonal_decompose(serie_temporal, model=\"additive\", period=365)\n",
        "\n",
        "fig = nox_dec.plot()\n",
        "fig.set_size_inches((16, 12))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Muestra que la serie tiene una clara tendencia a la baja, una estacionalidad notable con picos y valles regulares, y los residuos no parecen tener una estructura significativa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOqRycG7bd3B"
      },
      "source": [
        "### Visualizaciﾃｳn de la Autocorrelaciﾃｳn (ACF)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGqGsoLJbd3C"
      },
      "source": [
        "Vamos a visualizar la autocorrelaciﾃｳn de la variable `NOX`, para ello primero vamos a probar con lags bajos y luego con lags mﾃ｡s altos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "D0jul5iMbd3C",
        "outputId": "2ef15a18-98e9-44b9-d2ee-593caffa3b4d"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sm.graphics.tsa.plot_acf(df_concatenado_diario['NOX'], lags=1, ax=plt.gca())\n",
        "plt.title('Funciﾃｳn de Autocorrelaciﾃｳn (ACF) - Lags=1')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('Autocorrelaciﾃｳn')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sm.graphics.tsa.plot_acf(df_concatenado_diario['NOX'], lags=2, ax=plt.gca())\n",
        "plt.title('Funciﾃｳn de Autocorrelaciﾃｳn (ACF) - Lags=2')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('Autocorrelaciﾃｳn')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6LNM8m6bd3C"
      },
      "source": [
        "Como podemos apreciar con lags bajos tenemos valores altos, lo que indica que los valores recientes estﾃ｡n altamente correlacionados con los prﾃｳximos valores. Esto sugiere que la serie tiene una componente autoregresiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "ErIgGmuPbd3C",
        "outputId": "f83f21fa-d060-421d-c309-b5bb87290578"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "acf_values = sm.tsa.stattools.acf(df_concatenado_diario['NOX'], nlags=50)\n",
        "\n",
        "lags = np.arange(len(acf_values))\n",
        "\n",
        "for lag, value in zip(lags, acf_values):\n",
        "    if lag % 7 == 0 and lag != 0:\n",
        "        ax.vlines(lag, 0, value, color='red', lw=2)\n",
        "        ax.plot(lag, value, 'o', color='red')  \n",
        "    else:\n",
        "        ax.vlines(lag, 0, value, color='blue', lw=2)  \n",
        "        ax.plot(lag, value, 'o', color='blue')  \n",
        "\n",
        "ax.axhline(0, color='black', lw=1)  \n",
        "\n",
        "plt.title('Funciﾃｳn de Autocorrelaciﾃｳn (ACF)')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('Autocorrelaciﾃｳn')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D11RbL88bd3C"
      },
      "source": [
        "Como se puede apreciar en la grﾃ｡fica anterior, existe una componente estacional con correlaciﾃｳn positiva cada 7/8 lags, que tiende a disminuir con el tiempo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "wECVCm7Kbd3C",
        "outputId": "b6c0bcc3-7bda-4c29-9cb3-5172c27e6707"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "acf_values = sm.tsa.stattools.acf(df_concatenado_diario['NOX'], nlags=2000)\n",
        "\n",
        "lags = np.arange(len(acf_values))  \n",
        "\n",
        "for lag, value in zip(lags, acf_values):\n",
        "    if lag % 365 == 0 and lag != 0:  \n",
        "        ax.vlines(lag, 0, value, color='red', lw=2)  \n",
        "    ax.plot(lag, value, 'o', color='blue')  \n",
        "\n",
        "ax.axhline(0, color='black', lw=1) \n",
        "\n",
        "\n",
        "plt.title('Funciﾃｳn de Autocorrelaciﾃｳn (ACF)')\n",
        "plt.xlabel('Lags')\n",
        "plt.ylabel('Autocorrelaciﾃｳn')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b64Fr0C0bd3C"
      },
      "source": [
        "Como podemos ver cada 365 lags, es decir, cada aﾃｱo, existe un pico en el que la correlaciﾃｳn es mﾃ｡s positiva mostrando cierta estacionalidad anual, aunque la tendencia es a disminuir."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "povfydWCbd3C"
      },
      "source": [
        "## Particiﾃｳn Entrenamiento y Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la particiﾃｳn de los datos, vamos a utilizar una particiﾃｳn `Holdout` con un 90% de los datos para entrenamiento y un 10% para test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#guardame el csv de df_concatenado_diario\n",
        "\n",
        "df_concatenado_diario.to_csv(f\"{rutaBase}{nombreComun}2017_2022_diario.csv\", index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QqR-_6Mfbd3C",
        "outputId": "da8bb549-33ab-42fb-a84b-e3ce9ca3be90"
      },
      "outputs": [],
      "source": [
        "porcentaje_entrenamiento = 0.9\n",
        "df_train = df_concatenado_diario[:int(len(df_concatenado_diario) * porcentaje_entrenamiento)]\n",
        "df_test = df_concatenado_diario[int(len(df_concatenado_diario) * porcentaje_entrenamiento):]\n",
        "\n",
        "print(f'Con el porcentaje de %.2f tenemos:' %porcentaje_entrenamiento)\n",
        "print(f'Tamaﾃｱo del conjunto de training es %i' %len(df_train))\n",
        "print(f'Tamaﾃｱo del conjunto de test es %i' %len(df_test))\n",
        "print(f'El conjunto de training va de {min(df_train.index)} y {max(df_train.index)}')\n",
        "print(f'El conjunto de test va de {min(df_test.index)} y {max(df_test.index)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401
        },
        "id": "ARSpTSc2bd3D",
        "outputId": "8bdd2f99-5829-4c0b-f87d-7253bbb396e6"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "df_train['NOX'].plot(ax=ax, label='train')\n",
        "df_test['NOX'].plot(ax=ax, label='test')\n",
        "ax.legend(labels=['Train', 'Test'], loc='best')\n",
        "ax.set_xlabel('Aﾃｱo')\n",
        "ax.set_ylabel('NOX')\n",
        "ax.set_title('NOX mensual')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJIjF2Osbd3D"
      },
      "source": [
        "Separamos las variables independientes de las dependientes en ambos conjuntos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En la prﾃ｡ctica, se pide en primer lugar realizar el anﾃ｡lisis y la obtenciﾃｳn de modelos para predecir la variable `NOX` utilizando ﾃｺnicamente los datos de calidad del aire. Luego, se pide tambiﾃｩn volver a hacer el anﾃ｡lisis, pero ampliando el dataset con los datos meteorolﾃｳgicos. Por esta razﾃｳn, hemos creado dos datasets distintos, uno que contiene solo los datos sobre calidad del aire (`df_concatenado`), y otro al que le aﾃｱadimos ademﾃ｡s los datos meteorolﾃｳgicos (`df_concatenado_aire`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MP-sZYNfbd3D"
      },
      "outputs": [],
      "source": [
        "X_train = df_train.drop(columns = 'NOX')\n",
        "y_train = df_train['NOX']\n",
        "\n",
        "X_test = df_test.drop(columns='NOX')\n",
        "y_test = df_test['NOX']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZlrzyPrwbd3D"
      },
      "outputs": [],
      "source": [
        "columnas_variables_metereologicas = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS']\n",
        "\n",
        "X_train_aire = X_train.drop(columns=columnas_variables_metereologicas)\n",
        "y_train_aire = df_train['NOX']\n",
        "\n",
        "X_test_aire = X_test.drop(columns=columnas_variables_metereologicas)\n",
        "y_test_aire = df_test['NOX']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_NOX = df_train['NOX']\n",
        "test_NOX = df_test['NOX']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En ocasiones, aplicar ciertas transformaciones a los valores de una serie puede ayudar a generar mejores modelos de predicciﾃｳn. Algunas de estas transformaciones posibles son:\n",
        "\n",
        "- Raﾃｭz cuadrada: La transformaciﾃｳn mediante la raﾃｭz cuadrada se puede utilizar para estabilizar la varianza. Se calcula como:\n",
        "\n",
        "$$\\hat X_t = \\sqrt{X_t}$$\n",
        "\n",
        "- Logarﾃｭtmica: La transformaciﾃｳn logarﾃｭtmica tiene el mismo efecto que la raﾃｭz cuadrada, aunque en algunos casos puede ser mﾃ｡s interpretable. Se calcula como:\n",
        "\n",
        "$$\\hat X_t = \\ln{X_t}$$\n",
        "\n",
        "- Box-Cox: La transformaciﾃｳn Box-Cox puede ser ﾃｺtil a la hora de generar modelos de predicciﾃｳn, ya que tiene como objetivo aproximar los valores de las variables a una distribuciﾃｳn normal. Esta transformaciﾃｳn se define como:\n",
        "\n",
        "$$\n",
        "\\hat X_t(\\lambda) =\n",
        "\\begin{cases}\n",
        "\\frac{y^\\lambda - 1}{\\lambda} & \\text{si }\\lambda \\neq 0 \\\\\n",
        "\\ln(y) & \\text{si }\\lambda = 0\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaciﾃｳn, mostramos algunos estadﾃｭsticos para las variables del dataset. Estos estadﾃｭsticos incluyen:\n",
        "\n",
        "- `count`: nﾃｺmero total de datos en cada columna.\n",
        "- `mean`: media de los valores de cada variable.\n",
        "- `std`: desviaciﾃｳn estﾃ｡ndar, que mide la dispersiﾃｳn de los datos respecto a la media.\n",
        "- `min`: valor mﾃｭnimo de cada columna.\n",
        "- `25%`: percentil 25 o primer cuartil, que es el valor por debajo del cual se encuentra el 25% de los datos.\n",
        "- `50%`: percentil 50 o mediana, que es el valor por debajo del cual se encuentra el 50% de los datos.\n",
        "- `75%`: percentil 75 o tercer cuartil, que es el valor por debajo del cual se encuentra el 75% de los datos.\n",
        "- `max`: valor mﾃ｡ximo de cada columna."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al analizar la estadﾃｭstica descriptiva de los datos, se pueden extraer algunos argumentos que justifican la necesidad de aplicar alguna transformaciﾃｳn:\n",
        "\n",
        "- Algunas variables, como `PM10`, tienen una gran diferencia entre los valores mﾃｭnimos y los mﾃ｡ximos (rango de $5.29$ a $271.92$) lo cual, observando ademﾃ｡s los valores de los percentiles (por ejemplo, el percentil 75 es de $31.12$, bastante inferior al mﾃ｡ximo), sugiere que la distribuciﾃｳn de esta variable podrﾃｭa estar sesgada positivamente. Usando por ejemplo la transformaciﾃｳn logarﾃｭtmica o la Box-Cox, podremos conseguir que estas distribuciones se asemejen mﾃ｡s a una distribuciﾃｳn normal.\n",
        "\n",
        "- Algunas variables como `DD` o `RS` tienen una desviaciﾃｳn estﾃ｡ndar muy alta ($103.01$ y $2050.74$ respectivamente), y por tanto, la elevada variabilidad de estos valores podrﾃｭa dificultar la modelizaciﾃｳn. Por ejemplo, la transformaciﾃｳn de la raﾃｭz cuadrada, la logarﾃｭtmica o la Box-Cox podrﾃｭa conseguir datos mﾃ｡s uniformes y manejables.\n",
        "\n",
        "- Los percentiles muestran grandes saltos entre los cuartiles. Por ejemplo, en `PM10`, el rango intercuartil (IQR) es $31.12竏18=13.12$, pero la diferencia entre el mﾃｭnimo y el mﾃ｡ximo es mucho mayor ($271.92竏5.29=266.63$). Esto sugiere que hay valores extremos que podrﾃｭan influir excesivamente en los modelos. Usando por ejemplo la transformaciﾃｳn de la raﾃｭz cuadrada, la logarﾃｭtmica o la Box-Cox, podemos mitigar este efecto.\n",
        "\n",
        "Tras observar estos resultados, vamos a realizar un anﾃ｡lisis mﾃ｡s detallado, variable por variable, para estudiar quﾃｩ transformaciﾃｳn puede ser mﾃ｡s conveniente aplicar para cada variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed = df_concatenado_diario.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NO\n",
        "\n",
        "Para determinar quﾃｩ transformaciﾃｳn podrﾃｭa ser mﾃ｡s adecuada para la variable `NO`, comenzaremos analizando sus estadﾃｭsticas descriptivas y caracterﾃｭsticas.\n",
        "\n",
        "En primer lugar, se puede observar que la variable `NO` presenta una cierta **asimetrﾃｭa**, dado que, como su coeficiente de simetrﾃｭa es $B_1 = 2.17 > 0$, entonces su distribuciﾃｳn es sesgada positiva, lo que significa que la cola superior es mﾃ｡s larga y por tanto, hay mayor concentraciﾃｳn de datos al principio de la distribuciﾃｳn. El coeficiente de simetrﾃｭa se calcula con la siguiente fﾃｳrmula (aunque en este caso lo calcularemos con el mﾃｩtodo `.skew()` de `pandas`):\n",
        "\n",
        "$$ B_1 = \\frac{\\sum_{i=1}^n (x_i - \\bar X)^3}{(n-1) S^3}$$\n",
        "\n",
        "donde $\\bar X$ es la media muestral, $S$ es la desviaciﾃｳn estﾃ｡ndar muestral y $n$ es el nﾃｺmero de observaciones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'NO'\n",
        "coeficiente_simetria = df_concatenado_diario['NO'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'NO': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esto mismo lo podemos apreciar en el siguiente grﾃ｡fico, que muestra la distribuciﾃｳn de la variable `NO`, donde efectivamente se observa una cola superior mﾃ｡s larga:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['NO'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'NO'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'NO'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ademﾃ｡s, volviendo a los estadﾃｭsticos estudiados anteriormente, vemos que el valor mﾃ｡ximo $17.375$ de la variable `NO` es bastante superior a su media $3.647677$, lo cual podrﾃｭa indicar la presencia de **outliers**.\n",
        "\n",
        "Viendo estos resultados, la transformaciﾃｳn logarﾃｭtmica podrﾃｭa ayudar a abordar el problema de la asimetrﾃｭa. Sin embargo, dado que la transformaciﾃｳn logarﾃｭtmica es tan solo un caso particular de la transformaciﾃｳn Box-Cox, tomando $\\lambda = 0$, vamos a probar a aplicar la transformaciﾃｳn Box-Cox. De esta forma, podremos encontrar el parﾃ｡metro $\\lambda$ que da la mejor transformaciﾃｳn, en lugar de fijarlo directamente como $\\lambda = 0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn Box-Cox\n",
        "NO_boxcox, lambda_NO = boxcox(df_concatenado_diario['NO'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaciﾃｳn, para analizar si la transformaciﾃｳn ha sido efectiva, vamos a graficar los histogramas con la distribuciﾃｳn de la variable `NO` antes y despuﾃｩs de la transformaciﾃｳn, y vamos a compararlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'NO'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'NO'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['NO'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'NO' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'NO'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'NO_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(NO_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'NO' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'NO' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que tras aplicar la transformaciﾃｳn Box-Cox a la variable `NO`, la distribuciﾃｳn de la variable se vueve mﾃ｡s **simﾃｩtrica**, y se reduce la cola hacia la derecha. Se aprecia tambiﾃｩn que la distribuciﾃｳn de la variable transformada se acerca mﾃ｡s a una distribuciﾃｳn **normal** que la de la variable original.\n",
        "\n",
        "Por otro lado, tambiﾃｩn observamos que se reduce el rango de los datos y la **varianza** parece mﾃ｡s **estabilizada**, sin haber valores altos tan dispersos como en la distribuciﾃｳn de la variable original.\n",
        "\n",
        "Para asegurarnos de que efectivamente, la distribuciﾃｳn se asemeja mﾃ｡s a la normal tras la transformaciﾃｳn, podemos mostrar el Q-Q plot de la variable `NO` original y de la transformada, y compararlos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'NO'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'NO'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['NO'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'NO' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'NO_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(NO_boxcox, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'NO' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observando estos grﾃ｡ficos Q-Q plot, efectivamente se confirma que la variable `NO` tarnsformada mediante transformaciﾃｳn Box-Cox sigue una distribuciﾃｳn que se aproxima mﾃ｡s a la distribuciﾃｳn normal que la variable original sin transformar.\n",
        "\n",
        "Con todo esto, podemos concluir que la transformaciﾃｳn ha sido efectiva, y ha mejorado la distribuciﾃｳn de la variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['NO'] = NO_boxcox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### NO2\n",
        "\n",
        "Para buscar la mejor transformaciﾃｳn para la variable `NO2`, en primer lugar observamos que hay una ligera diferencia entre la media ($11.261764$) y la mediana ($9.875$), lo cual parece indicar una ligera **asimetrﾃｭa** en la distribuciﾃｳn de la variable. Para confirmar esto, podemos de nuevo calcular el coeficiente de simetrﾃｭa, que en este caso nos da $B_1 = 1.31 > 0$, por lo que nuevamente la distribuciﾃｳn es sesgada positiva."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'NO2'\n",
        "coeficiente_simetria = df_concatenado_diario['NO2'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'NO2': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Podemos ademﾃ｡s mostrar un histograma con la distribuciﾃｳn de la variable, donde se observa que hay una cola superior mﾃ｡s larga. Aunque en este caso dicha asimetrﾃｭa no es tan extrema como con la variable `NO` anterior, igualmente podrﾃｭa ser beneficioso intentar aproximar la distribuciﾃｳn de la variable `NO2` por una distribuciﾃｳn normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['NO2'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'NO2'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'NO2'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ademﾃ｡s, la desviaciﾃｳn estﾃ｡ndar $5.356303$ es relativamente alta, lo que sugiere que los datos pueden ser dispersos. El valor mﾃ｡ximo $40.958333\t$ estﾃ｡ bastante alejado de la media $11.261764$, lo cual es consistente con la presencia de una cola a la derecha.\n",
        "\n",
        "Como en este caso, el sesgo positivo observado no es tan pronunciado como en `NO`, la transformaciﾃｳn logarﾃｭtmica podrﾃｭa ser suficiente para abordar el problema de la cola larga a la derecha y estabilizar la varianza, logrando una distribuciﾃｳn aproximada a la normal sin necesidad de ajustes complejos como Box-Cox. Por ello, con la variable `NO2` vamos aprobar a aplicar la transformaciﾃｳn logarﾃｭtmica y ver si da buenos resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn logarﾃｭtmica\n",
        "NO2_log = np.log(df_concatenado_diario['NO2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Graficamos los histogramas de la variable `NO2` con y sin transformaciﾃｳn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'NO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'NO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['NO2'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'NO2' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'NO2'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'NO2_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(NO2_log, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'NO2' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'NO2' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que tras aplicar la transformaciﾃｳn logarﾃｭtmica a la variable `NO2`, su distribuciﾃｳn se vueve mﾃ｡s **simﾃｩtrica**, ademﾃ｡s de reducirse la cola hacia la derecha. Tambiﾃｩn se reduce considerablemente el rango de la variable, y su **varianza** se **estabiliza**, sin haber valores altos tan dispersos como en la distribuciﾃｳn de la variable original.\n",
        "\n",
        "Por otro lado, la distribuciﾃｳn de la variable transformada parece acercarse mﾃ｡s a una distribuciﾃｳn **normal** que la de la variable original. Para asegurarnos de esto, mostramos el Q-Q plot de la variable `NO2` original y de la transformada, y los comparamos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'NO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'NO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['NO2'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'NO2' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'NO2_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(NO2_log, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'NO2' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al observar los Q-Q plot, podemos apreciar que gracias a la transformaciﾃｳn logarﾃｭtmica, la distribuciﾃｳn de `NO2` se aproxima considerablemente por una distribuciﾃｳn normal.\n",
        "\n",
        "Tras este anﾃ｡lisis, podemos concluir que la transformaciﾃｳn logarﾃｭtmica ha sido efectiva, y no es necesario aplicar otras transformaciones mﾃ｡s complejas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['NO2'] = NO2_log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### SO2 \n",
        "\n",
        "Para esta variable, observamos en primer lugar que la media ($6.834639$) y la mediana ($6.375$) toman valores muy similares, lo cual podrﾃｭa sugerir que `SO2` tiene una distribuciﾃｳn algo mﾃ｡s simﾃｩtrica que las dos variables estudiadas con anterioridad. De hecho, al calcular el coeficiente de simetrﾃｭa se obtiene $B_1 = 0.69 > 0$, lo cual indica una ligera asﾃｭmetrﾃｭa, aunque al ser un valor cercano a $0$ no es tan pronunciada como en los casos anteriores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'SO2'\n",
        "coeficiente_simetria = df_concatenado_diario['SO2'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'SO2': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ademﾃ｡s, aunque se observa una cola a la derecha, esta no parece ser tan marcada como en los casos anteriores, tal como se puede apreciar en el histograma que se muestra a continuaciﾃｳn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['SO2'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'SO2'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'SO2'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La desviaciﾃｳn estﾃ｡ndar $4.024215$ sugiere una cierta dispersiﾃｳn en los datos. Ademﾃ｡s, hay ciertos valores altos que podrﾃｭan ser outliers. Observando la distribuciﾃｳn de la variable, no se aprecia claramente quﾃｩ transformaciﾃｳn podrﾃｭa funcionar mejor, por lo que vamos a probar a aplicar las tres transformaciones y las compararemos, estudiando cuﾃ｡l de ellas ofrece mejores resultados. Comenzamos con la transformaciﾃｳn de raﾃｭz cuadrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de raﾃｭz cuadrada\n",
        "SO2_sqrt = np.sqrt(df_concatenado_diario['SO2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mostramos a continuaciﾃｳn los histogramas con las distribuciones de la variable `SO2` antes y despuﾃｩs de la transformaciﾃｳn de raﾃｭz cuadrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'SO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'SO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['SO2'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'SO2' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'SO2'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'SO2_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(SO2_sqrt, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'SO2' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'SO2' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observando la distribuciﾃｳn de la variable transformada, no parece que se aproxime por una normal. Sin embargo, sﾃｭ se consigue reducir el rango de la variable, asﾃｭ como la asimetrﾃｭa de su distribuciﾃｳn. Representemos los Q-Q plot para comprobar si se mejora la normalidad al aplicar la transformaciﾃｳn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'SO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'SO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['SO2'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'SO2' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'SO2_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(SO2_sqrt, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'SO2' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observando los Q-Q plot, se observa una ligera mejora en cuanto a la normalidad respecto de la variable original, aunque no parece que la variable transformada con raﾃｭz cuadrada siga tampoco una distribuciﾃｳn normal. Probemos a aplicar el test de Shapiro-Wilk para hacer una comprobaciﾃｳn mﾃ｡s precisa de la normalidad:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p = shapiro(SO2_sqrt)\n",
        "print(f'Estadﾃｭstico: {stat}, p-valor: {p}')\n",
        "\n",
        "if p > 0.05:\n",
        "    print(\"La distribuciﾃｳn es aproximadamente normal.\")\n",
        "else:\n",
        "    print(\"La distribuciﾃｳn no es normal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por tanto, mediante la transformaciﾃｳn de la raﾃｭz cuadrada no se logra una distribuciﾃｳn normal para la variable `SO2`.\n",
        "\n",
        "Dado que los resultados en cuanto a normalidad no parecen ser satisfactorios, vamos a probar a analizar otras mﾃｩtricas como la asimetrﾃｭa y la curtosis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Asimetrﾃｭa raﾃｭz cuadrada: {skew(SO2_sqrt)}\")\n",
        "print(f\"Curtosis raﾃｭz cuadrada: {kurtosis(SO2_sqrt)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La asimetrﾃｭa obtenida $0.23$ es ligeramente positiva, pero al ser tan cercana a $0$, sugiere que la distribuciﾃｳn es casi simﾃｩtrica. Por otro lado, la curtosis es negativa, lo que indica que la distribuciﾃｳn es mﾃ｡s plana que la distribuciﾃｳn normal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probemos ahora con la transformaciﾃｳn logarﾃｭtmica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn logarﾃｭtmica\n",
        "SO2_log = np.log(df_concatenado_diario['SO2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mostramos los histogramas con las distribuciones de la variable original y de la variable transformada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'SO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'SO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['SO2'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'SO2' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'SO2'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'SO2_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(SO2_log, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'SO2' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'SO2' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De nuevo, mediante la transformaciﾃｳn logarﾃｭtmica tampoco parece lograrse una aproximaciﾃｳn a la distribuciﾃｳn normal, aunque sﾃｭ se consigue reducir el rango y la asimetrﾃｭa.\n",
        "\n",
        "A continuaciﾃｳn mostramos los grﾃ｡ficos Q-Q plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'SO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'SO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['SO2'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'SO2' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'SO2_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(SO2_log, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'SO2' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso se observa que la distribuciﾃｳn parece alejarse todavﾃｭa mﾃ｡s de la distribuciﾃｳn normal, en lugar de aproximarse a la misma. Esto mismo se comprueba con el test de Shapiro-Wilk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p = shapiro(SO2_log)\n",
        "print(f'Estadﾃｭstico: {stat}, p-valor: {p}')\n",
        "\n",
        "if p > 0.05:\n",
        "    print(\"La distribuciﾃｳn es aproximadamente normal.\")\n",
        "else:\n",
        "    print(\"La distribuciﾃｳn no es normal.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Asimetrﾃｭa log: {skew(SO2_log)}\")\n",
        "print(f\"Curtosis log: {kurtosis(SO2_log)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con la transformaciﾃｳn logarﾃｭtmica, se obtiene una asﾃｭmetrﾃｭa negativa, y mayor que la obtenida con la transformaciﾃｳn de raﾃｭz cuadrada. Ademﾃ｡s, la curtosis vuelve a ser negativa, pero mﾃ｡s plana que con la transformaciﾃｳn de raﾃｭz cuadrada.\n",
        "\n",
        "Por lo tanto, vamos a descartar la transformaciﾃｳn logarﾃｭtmica, ya que es la que peores resultados ha dado hasta el momento en cuanto a normalidad, asimetrﾃｭa y curtosis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por ﾃｺltimo, probamos a aplicar la transformaciﾃｳn Box-Cox y analizamos si se consigue alguna mejora considerable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn Box-Cox\n",
        "SO2_boxcox, lambda_SO2 = boxcox(df_concatenado_diario['SO2'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mostramos los histogramas de las distribuciones de la variable original y la transformada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'SO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'SO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['SO2'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'SO2' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'SO2'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'SO2_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(SO2_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'SO2' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'SO2' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "De nuevo, la distribuciﾃｳn de la variable transformada no parece aproximarse a una distribuciﾃｳn normal, aunque sﾃｭ se consigue reducir el rango y la asimetrﾃｭa.\n",
        "\n",
        "Mostramos a continuaciﾃｳn los grﾃ｡ficos Q-Q plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'SO2'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'SO2'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['SO2'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'SO2' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'SO2_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(SO2_boxcox, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'SO2' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parece haber una ligera mejora tras aplicar la transformaciﾃｳn, aunque no parece demasiado significativa, ni se logra una distribuciﾃｳn cercana a la normal, como se puede comprobar con el test de Shapiro-Wilk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p = shapiro(SO2_boxcox)\n",
        "print(f'Estadﾃｭstico: {stat}, p-valor: {p}')\n",
        "\n",
        "if p > 0.05:\n",
        "    print(\"La distribuciﾃｳn es aproximadamente normal.\")\n",
        "else:\n",
        "    print(\"La distribuciﾃｳn no es normal.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(f\"Asimetrﾃｭa Box-Cox: {skew(SO2_boxcox)}\")\n",
        "print(f\"Curtosis Box-Cox: {kurtosis(SO2_boxcox)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso, la asimetrﾃｭa es ligeramente negativa, pero es casi simﾃｩtrica, y de todas las transformacioens probadas, la Box-Cox es la que ofrece un mejor ajuste. Por otro lado, en cuanto a la curtosis, toma un valor negativo, y ligeramente peor que con la transformaciﾃｳn de raﾃｭz cuadrada, aunque sin haber mucha diferencia entre ambas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como conclusiﾃｳn final, se tiene que:\n",
        "\n",
        "- La que mejor aproxima a la distribuciﾃｳn normal es la transformaciﾃｳn de raﾃｭz cuadrada, aunque con una diferencia mﾃｭnima con la Box-Cox.\n",
        "- La variable transformada con raﾃｭz cuadrada consigue un rango menor que la transformada con Box-Cox.\n",
        "- La transformaciﾃｳn Box-Cox consigue una mejor simetrﾃｭa, pero con poca diferencia con la raﾃｭz cuadrada.\n",
        "- La transformaciﾃｳn de raﾃｭz cuadrada consigue un valor ligeramente mejor para la curtosis.\n",
        "\n",
        "Con base en estas observaciones, nos vamos a quedar con la transformaciﾃｳn de raﾃｭz cuadrada, dado que las diferencias con Box-Cox son mﾃｭnimas, pero ofrece ventajas clave como una mejor curtosis y un rango mﾃ｡s estrecho."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['SO2'] = SO2_sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PM10\n",
        "\n",
        "Analizando los estadﾃｭsticos de la variable `PM10`, observamos que esta tiene un rango muy amplio, ya que el mﾃｭnimo $5.291667$ y el mﾃ｡ximo $271.916667$ se encuentran muy separados, lo cual puede dificultar la modelizaciﾃｳn. Ademﾃ｡s, la desviaciﾃｳn estﾃ｡ndar $13.965115$ es bastante elevada, lo que muestra una dispersiﾃｳn considerable.\n",
        "\n",
        "Por otro lado, como el mﾃ｡ximo es considerablemente mayor que la mediana ($23.458333$), la distribuciﾃｳn parece estar sesgada hacia la derecha. Esto se puede confirmar al calcular el coeficiente de simetrﾃｭa $B_1 = 4.77 > 0$, el cual es bastante elevado, indicando una fuerte asimetrﾃｭa."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'PM10'\n",
        "coeficiente_simetria = df_concatenado_diario['PM10'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'PM10': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Efectivamente, al mostrar el histograma con la distribuciﾃｳn de la variable, se aprecia una cola larga a la derecha."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['PM10'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'PM10'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'PM10'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al tener una distribuciﾃｳn altamente sesgada, pasamos directamente a probar con la transformaciﾃｳn Box-Cox, que puede ajustar el parﾃ｡metro $\\lambda$ para ser mﾃ｡s efectiva ante este nivel de sesgo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn Box-Cox\n",
        "PM10_boxcox, lambda_PM10 = boxcox(df_concatenado_diario['PM10'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mostramos los histogramas con las distribuciones de la variable `PM10` con y sin la transformaciﾃｳn Box-Cox:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'PM10'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'PM10'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['PM10'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'PM10' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'PM10'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'PM10_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(PM10_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'PM10' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'PM10' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos que los resultados son bastante buenos, pues la distribuciﾃｳn parece aproximarse bastante bien por una normal, ademﾃ｡s de haberse reducido considerablemente el rango de la variable, y haberse vuelto mucho mﾃ｡s simﾃｩtrica.\n",
        "\n",
        "Podemos mostrar los grﾃ｡ficos Q-Q plot para asegurarnos de que la distribuciﾃｳn de la variable transformada se aproxima considerablemente a la distribuciﾃｳn normal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'PM10'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'PM10'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['PM10'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'PM10' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'PM10_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(PM10_boxcox, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'PM10' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Efectivamente, observando estos grﾃ｡ficos, se observa una gran mejora tras la transformaciﾃｳn Box-Cox en cuanto a la normalidad de la distribuciﾃｳn de la variable `PM10`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['PM10'] = PM10_boxcox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TMP\n",
        "\n",
        "Analizando los estadﾃｭsticos de la variable `TMP` se puede apreciar que la desviaciﾃｳn estﾃ｡ndar $5.837991$ es ligeramente alta, aunque no tan exagerada como en otras variables estudiadas con anterioridad. Por otro lado, el rango es razonablemente amplio, con un mﾃｭnimo de $7.445652$ y un mﾃ｡ximo de $33.916667$, pero no excesivo.\n",
        "\n",
        "Ademﾃ｡s, calculando el coeficiente de simetrﾃｭa se obtiene $B_1 = 0.07$, lo cual es bastante bajo, indicando que la distribuciﾃｳn no estﾃ｡ excesivamente sesgada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'TMP'\n",
        "coeficiente_simetria = df_concatenado_diario['TMP'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'TMP': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analizando el histograma, se pueden observar dos mﾃ｡ximos en el mismo, lo cual sugiere que podrﾃｭan existir dos grupos diferentes dentro de los datos (por ejemplo, temperaturas de diferentes ﾃｩpocas del aﾃｱo)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['TMP'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'TMP'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'TMP'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Comprobemos mediante el test de Shapiro-Wilk si la distribuciﾃｳn es normal:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p = shapiro(df_concatenado_diario['TMP'])\n",
        "print(f'Estadﾃｭstico: {stat}, p-valor: {p}')\n",
        "\n",
        "if p > 0.05:\n",
        "    print(\"La distribuciﾃｳn es aproximadamente normal.\")\n",
        "else:\n",
        "    print(\"La distribuciﾃｳn no es normal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que la transformaciﾃｳn Box-Cox es mﾃ｡s flexible, ya que ajusta su parﾃ｡metro $\\lambda$ para manejar mejor los sesgos y la dispersiﾃｳn, vamos a probar a aplicar esta transformaciﾃｳn para ver si de esta forma conseguimos resultados efectivos, incluso habiendo dos mﾃ｡ximos en la distribuciﾃｳn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn Box-Cox\n",
        "TMP_boxcox, lambda_TMP = boxcox(df_concatenado_diario['TMP'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Mostramos los histogramas para la variable original y la variable transformada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'TMP'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'TMP'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['TMP'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'TMP' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'TMP'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'TMP_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(TMP_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'TMP' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'TMP' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A pesar de que sigue habiendo dos mﾃ｡ximos, al menos se logra reducir el rango de la variable. Sin embargo, no se logra una distribuciﾃｳn normal, aunque mirando los grﾃ｡ficos Q-Q plot, parece apreciarse una muy ligera mejora en este sentido:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'TMP'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'TMP'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['TMP'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'TMP' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'PM10_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(TMP_boxcox, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'TMP' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esto mismo puede comprobarse con el test de Shapiro-Wilk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p = shapiro(TMP_boxcox)\n",
        "print(f'Estadﾃｭstico: {stat}, p-valor: {p}')\n",
        "\n",
        "if p > 0.05:\n",
        "    print(\"La distribuciﾃｳn es aproximadamente normal.\")\n",
        "else:\n",
        "    print(\"La distribuciﾃｳn no es normal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como conclusiﾃｳn, aunque la transformaciﾃｳn no consigue aproximar por una distribuciﾃｳn normal, sﾃｭ se logra una ligera mejora en cuanto al rango y la varianza de la variable, por lo que puede ser conveniente aplicar la transformaciﾃｳn Box-Cox a la variable `TMP`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['TMP'] = TMP_boxcox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### HR\n",
        "\n",
        "La desviaciﾃｳn estﾃ｡ndar es $14.474146$, lo que sugiere que hay una variabilidad significativa en los valores de `HR` alrededor de su media. Esto es relativamente alto, indicando que los valores de humedad relativa pueden fluctuar considerablemente.\n",
        "\n",
        "El valor mﾃｭnimo de `HR` es $32.125000$, lo que muestra que, en el conjunto de datos, no hay valores de `HR` demasiado bajos, lo cual es importante porque no hay anomalﾃｭas extremas en este sentido.\n",
        "\n",
        "La media ($71.218916$) estﾃ｡ cerca de la mediana ($72.541667$), lo que podrﾃｭa indicar que la distribuciﾃｳn no estﾃ｡ extremadamente sesgada. Esto mismo se puede comprobar con el coeficiente de simetrﾃｭa $B_1 = -0.26 <0$, que al ser negativo podrﾃｭa indicar una ligera simetrﾃｭa negativa, es decir, sesgo hacia la izquierda, aunque la asimetrﾃｭa no es muy pronunciada, ya que este valor estﾃ｡ bastante cerca de $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'HR'\n",
        "coeficiente_simetria = df_concatenado_diario['HR'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'HR': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['HR'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'HR'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'HR'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stat, p = shapiro(df_concatenado_diario['HR'])\n",
        "print(f'Estadﾃｭstico: {stat}, p-valor: {p}')\n",
        "\n",
        "if p > 0.05:\n",
        "    print(\"La distribuciﾃｳn es aproximadamente normal.\")\n",
        "else:\n",
        "    print(\"La distribuciﾃｳn no es normal.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vamos a probar con la distribuciﾃｳn Box-Cox, para ver si conseguimos aproximar por una distribuciﾃｳn normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn Box-Cox\n",
        "HR_boxcox, lambda_HR = boxcox(df_concatenado_diario['HR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'HR'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'HR'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['HR'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'HR' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'HR'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'HR_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(HR_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'HR' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'HR' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Si bien se ha conseguido reducir la asimetrﾃｭa de la distribuciﾃｳn, tambiﾃｩn podemos apreciar que el rango de valores que obtenemos, entre $100$ y $500$, es claramente excesivo y no parece apropiado para la interpretaciﾃｳn o el modelado.\n",
        "\n",
        "Por ello, vamos a probar a aplicar una transformaciﾃｳn logarﾃｭtmica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn logarﾃｭtmica\n",
        "HR_log = np.log(df_concatenado_diario['HR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'HR'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'HR'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['HR'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'HR' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'HR'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'HR_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(HR_log, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'HR' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'HR' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso, vemos que al aplicar la transformaciﾃｳn logarﾃｭtmica, aumenta la asimetrﾃｭa de la variable, por lo que no parece una transformaciﾃｳn adecuada.\n",
        "\n",
        "Por ﾃｺltimo, vamos a probar con la transformaciﾃｳn de raﾃｭz cuadrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de raﾃｭz cuadrada\n",
        "HR_sqrt = np.sqrt(df_concatenado_diario['HR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'HR'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'HR'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['HR'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'HR' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'HR'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'HR_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(HR_sqrt, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'HR' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'HR' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Con la transformaciﾃｳn de raﾃｭz cuadrada, vemos que se consigue reducir el rango de la variable a un rango mﾃ｡s razonable, sin afectar demasiado a la asimetrﾃｭa, y aunque no parece lograrse una distribuciﾃｳn totalmente normal, tampoco parece estar tan alejada de la misma, como podemos comprobar con el Q-Q plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'HR'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'HR'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['HR'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'HR' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'HR_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(HR_sqrt, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'HR' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como conclusiﾃｳn, nos vamos a quedar finalmente con la transformaciﾃｳn de raﾃｭz cuadrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['HR'] = HR_sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### PRB\n",
        "\n",
        "Como podemos observar en la grﾃ｡fica observamos que la variable `PRB` sigue una distribuciﾃｳn normal, por lo que no creemos necesario aplicar ninguna transformaciﾃｳn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['PRB'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'PRB'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'PRB'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### VV\n",
        "\n",
        "El valor medio de `VV` es $1.280236$, lo que indica que en promedio los valores de esta variable son bastante cercanos a $1$, pero pueden variar ligeramente, debido a que la desviaciﾃｳn estﾃ｡ndar es $0.269307$, lo que sugiere que la mayorﾃｭa de los valores estﾃ｡n relativamente cercanos a la media, pero tambiﾃｩn existe algo de variabilidad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'VV'\n",
        "coeficiente_simetria = df_concatenado_diario['VV'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'VV': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Dado que el coeficiente de simetrﾃｭa para la variable VV es $B_1 = 1.74 > 0$, podemos concluir que la distribuciﾃｳn de esta variable tiene una asimetrﾃｭa positiva considerable, lo que significa que tiene una cola derecha mﾃ｡s alargada. En este caso, es importante considerar una transformaciﾃｳn que ayude a reducir esa asimetrﾃｭa y acercar la distribuciﾃｳn a una forma mﾃ｡s simﾃｩtrica, o incluso normal, si es posible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['VV'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'VV'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'VV'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probamos en primer lugar con una trasnformaciﾃｳn de raﾃｭz cuadrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de raﾃｭz cuadrada\n",
        "VV_sqrt = np.sqrt(df_concatenado_diario['VV'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'VV'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'VV'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['VV'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'VV' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'VV'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'VV_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(VV_sqrt, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'VV' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'VV' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras la transformaciﾃｳn, la variable sigue siendo bastante asimﾃｩtrica, y no muestra mejoras considerables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'VV'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'VV'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['VV'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'VV' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'VV_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(VV_sqrt, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'VV' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probamos ahora con una transformaciﾃｳn logarﾃｭtmica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn logarﾃｭtmica\n",
        "VV_log = np.log(df_concatenado_diario['VV'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'VV'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'VV'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['VV'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'VV' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'VV'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'VV_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(VV_log, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'VV' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'VV' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta transformaciﾃｳn sigue sin resolver el problema de la asimetrﾃｭa, aunque observando el Q-Q plot parece que mejora ligeramente en cuanto a la normalidad: "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'VV'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'VV'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['VV'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'VV' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'VV_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(VV_log, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'VV' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por ﾃｺltimo, probamos con la transformaciﾃｳn Box-Cox:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de Box-Cox\n",
        "VV_boxcox, lambda_VV = boxcox(df_concatenado_diario['VV'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'VV'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'VV'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['VV'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'VV' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'VV'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'VV_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(VV_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'V' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'VV' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La transformaciﾃｳn Box-Cox es la que peores resultados ha dado, pues han aparecido huecos por en medio del histograma.\n",
        "\n",
        "Por tanto, parece que aunque ninguna de las transformaciones tiene un desempeﾃｱo excelente, la transformaciﾃｳn logarﾃｭtmica es la que ha dado mejores resultados, por lo que nos vamos a quedar con esta."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['VV'] = VV_log"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### O3\n",
        "\n",
        "Esta variable tiene un rango bastante grande, desde el mﾃｭnimo $8.608696$ hasta el mﾃ｡ximo $159.00$, por lo que puede ser conveniente reducir dicho rango. \n",
        "\n",
        "Por otro lado, no parece mostrar una gran asimetrﾃｭa, pues su coeficiente de simetrﾃｭa $B_1 = 0.199 > 0$ es bastante cercano a $0$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'O3'\n",
        "coeficiente_simetria = df_concatenado_diario['O3'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'O3': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['O3'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'O3'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'O3'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicamos una transformaciﾃｳn Box-Cox para ver si podemos reducir un poco el rango:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de Box-Cox\n",
        "O3_boxcox, lambda_O3 = boxcox(df_concatenado_diario['O3'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'O3'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'O3'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['O3'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'O3' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'O3'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'O3_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(O3_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'O3' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'O3' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que el rango se reduce un poco, y ademﾃ｡s, aunque la variable original ya se acercaba bastante a una normal, vemos que con la transformaciﾃｳn Box-Cox se aprecia una ligera mejora en este sentido tambiﾃｩn, como se puede observar en el Q-Q plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'O3'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'O3'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['O3'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'O3' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'O3_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(O3_boxcox, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'O3' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['O3'] = O3_boxcox"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RS\n",
        "\n",
        "La media es $4272.294776$, lo cual sugiere que, en promedio, los valores de `RS` son altos. Ademﾃ｡s, el valor de $2050.740902$ para la desviaciﾃｳn estﾃ｡ndar muestra que hay bastante dispersiﾃｳn en los datos, lo que significa que los valores de `RS` tienen una gran variabilidad.\n",
        "\n",
        "El valor mﾃｭnimo es $144.594595$ y el mﾃ｡ximo $14407.320611$. Esto sugiere una gran variabilidad en los datos, ya que el rango de valores es bastante extenso.\n",
        "\n",
        "Ademﾃ｡s, el coeficiente de simetrﾃｭa $B_1 = 0.395 > 0$ indica que la distribuciﾃｳn tiene una ligera asimetrﾃｭa positiva. Es decir, la cola de la distribuciﾃｳn es un poco mﾃ｡s larga en el lado derecho de la media, aunque no es extremadamente asimﾃｩtrica."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'RS'\n",
        "coeficiente_simetria = df_concatenado_diario['RS'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'RS': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esto mismo tambiﾃｩn puede observarse en el histograma siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['RS'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'RS'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'RS'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Probemos en primer lugar con una transformaciﾃｳn Box-Cox, ya que es la mﾃ｡s flexible de todas, y puede ser eficiente a la hora de aproximar por una distribuciﾃｳn normal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de Box-Cox\n",
        "RS_boxcox, lambda_RS = boxcox(df_concatenado_diario['RS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'RS'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'RS'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['RS'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'RS' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'RS'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'RS_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(RS_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'RS' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'RS' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Vemos que el rango de valores, aunque se reduce, sigue siendo considerablemente alto, por lo que tal vez pueda haber otras transformaciones mejores que Box-Cox en este caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'RS'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'RS'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['RS'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'RS' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'RS_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(RS_boxcox, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'RS' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En cuanto a normalidad, parece mejorar un poco, pero tampoco logra aproximar completamente por una distribuciﾃｳn normal.\n",
        "\n",
        "Por ello, vamos a probar ahora con una transformaciﾃｳn logarﾃｭtmica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn logarﾃｭtmica\n",
        "RS_log = np.log(df_concatenado_diario['RS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'RS'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'RS'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['RS'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'RS' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'RS'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'RS_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(RS_log, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'RS' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'RS' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este caso, se ha conseguido reducir considerablemente el rango."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'RS'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'RS'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['RS'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'RS' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'RS_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(RS_log, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'RS' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No obstante, no parece aproximar de forma adecuada por una distribuciﾃｳn normal.\n",
        "\n",
        "Por ﾃｺltimo, vamos a probar con una transformaciﾃｳn de raﾃｭz cuadrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de raﾃｭz cuadrada\n",
        "RS_sqrt = np.sqrt(df_concatenado_diario['RS'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'RS'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'RS'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['RS'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'RS' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'RS'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'RS_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(RS_sqrt, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'RS' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'RS' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esta transformaciﾃｳn logra disminuir el rango, aunque no tanto como la transformaciﾃｳn logarﾃｭtimca. Sin embargo, con la transformaciﾃｳn de raﾃｭz cuadrada se logra una distribuciﾃｳn mﾃ｡s simﾃｩtrica y mﾃ｡s cercana a la normal, como se puede observar con el siguiente Q-Q plot:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'RS'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'RS'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['RS'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'RS' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'RS_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(RS_sqrt, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'RS' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por lo tanto, finalmente nos quedamos con la transformaciﾃｳn de raﾃｭz cuadrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['RS'] = RS_sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### DD\n",
        "\n",
        "La media de `DD` es de $191.505376$ y la desviaciﾃｳn estﾃ｡ndar es de $103.012220$, lo que indica una dispersiﾃｳn considerable de los datos. El rango de los valores va desde un valor mﾃｭnimo de $0.005307$ hasta un mﾃ｡ximo de $359.815207$.\n",
        "\n",
        "Ademﾃ｡s, el coeficiente de simetrﾃｭa para la variable `DD` es de $B_1 = -0.403 < 0$, lo que indica una asimetrﾃｭa negativa moderada. Esto significa que la distribuciﾃｳn tiene una mayor concentraciﾃｳn de valores en el lado derecho de la media."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calcular el coeficiente de simetrﾃｭa de la variable 'DD'\n",
        "coeficiente_simetria = df_concatenado_diario['DD'].skew()\n",
        "print(f\"Coeficiente de simetrﾃｭa para la variable 'DD': {coeficiente_simetria}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos en el histograma que aparecen dos mﾃ｡ximos. Esto sugiere que los datos podrﾃｭan estar formados por dos poblaciones o un fenﾃｳmeno que genera dos comportamientos distintos en los valores."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(df_concatenado_diario['DD'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'DD'\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'DD'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Esto puede ser mﾃ｡s complicado de transformar de forma eficiente, por lo que vamos a ir probando cada una de las tres transformaciones consideradas, hasta encontrar la que mejor funcione.\n",
        "\n",
        "Comenzamos con la transformaciﾃｳn de raﾃｭz cuadrada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de raﾃｭz cuadrada\n",
        "DD_sqrt = np.sqrt(df_concatenado_diario['DD'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'DD'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'DD'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['DD'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'DD' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'DD'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'DD_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(DD_sqrt, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'DD' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'DD' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos que se logra reducir bastante el rango de la variable, lo cual puede ser un punto a favor para la transformaciﾃｳn de raﾃｭz cuadrada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'DD'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'DD'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['DD'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'DD' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'DD_sqrt'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(DD_sqrt, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'DD' (Transformaciﾃｳn de raﾃｭz cuadrada)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "No obstante, no logra aproximar la distribuciﾃｳn de la variable por una normal de forma consistente."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pasamos ahora con la transformaciﾃｳn logarﾃｭtmica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn logarﾃｭtmica\n",
        "DD_log = np.log(df_concatenado_diario['DD'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'DD'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'DD'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['DD'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'DD' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'DD'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'DD_log'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(DD_log, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'DD' (Transformaciﾃｳn logarﾃｭtmica)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'DD' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Claramente en este caso se obtiene una distribuciﾃｳn muy asimﾃｩtrica, ademﾃ｡s de tomar valores negativos, por lo que no parece la logarﾃｭtmica no parece la transformaciﾃｳn mﾃ｡s adecuada.\n",
        "\n",
        "Por ﾃｺltimo, vamos a probar con la transformaciﾃｳn Box-Cox:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aplica la transformaciﾃｳn de Box-Cox\n",
        "DD_boxcox, lambda_DD = boxcox(df_concatenado_diario['DD'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el grﾃ｡fico de distribuciﾃｳn para la variable 'DD'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Histograma de la variable original 'DD'\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.histplot(df_concatenado_diario['DD'], kde=True, color='skyblue', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'DD' (Original)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'DD'\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Histograma de la variable transformada 'DD_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.histplot(DD_boxcox, kde=True, color='lightcoral', bins=30)\n",
        "plt.title(\"Distribuciﾃｳn de la variable 'DD' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "plt.xlabel(\"Valores de 'DD' Transformada\", fontsize=14)\n",
        "plt.ylabel(\"Frecuencia\", fontsize=14)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El rango de la variable no consigue reducirse significativamente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear el Q-Q plot para la variable 'DD'\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Q-Q plot para la variable original 'DD'\n",
        "plt.subplot(1, 2, 1)\n",
        "stats.probplot(df_concatenado_diario['DD'], dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'DD' (Original)\", fontsize=16)\n",
        "\n",
        "# Q-Q plot para la variable transformada 'DD_boxcox'\n",
        "plt.subplot(1, 2, 2)\n",
        "stats.probplot(DD_boxcox, dist=\"norm\", plot=plt)\n",
        "plt.title(\"Q-Q plot de 'DD' (Transformaciﾃｳn Box-Cox)\", fontsize=16)\n",
        "\n",
        "# Mostrar los grﾃ｡ficos\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La transformaciﾃｳn Box-Cox tampoco consigue aproximar la distribuciﾃｳn por una normal.\n",
        "\n",
        "Por ello, como conclusiﾃｳn parece que la mejor transformaciﾃｳn es la raﾃｭz cuadrada, ya que por lo menos consigue reducir considerablemente el rango de la variable `DD`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario_transformed['DD'] = DD_sqrt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IlufAZcRbd3G"
      },
      "source": [
        "## Modelos"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF5_8kbVbd3H"
      },
      "source": [
        "#### Mﾃｩtricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para la evaluaciﾃｳn de los modelos, usaremos dos mﾃｩtricas diferentes. Sea $X_t$ el valor real y $hat X_t$ el valor predicho, y sea $e_t = X_t - \\hat X_t$ el error que cometemos con dicha predicciﾃｳn. Entonces, a la hora de buscar un error global, se pueden usar las siguientes mﾃｩtricas:\n",
        "\n",
        "- Mean Absolute Percent Error (MAPE): mide el error promedio en tﾃｩrminos relativos como un porcentaje del valor real. Al estar expresado como un porcentaje, es fﾃ｡cil de interpretar. Ademﾃ｡s, dicho porcentaje indica en quﾃｩ medida la predicciﾃｳn se desvﾃｭa del valor real. Sin embargo, este error es sensible a valores cercanos a cero, ya que al dividir entre un nﾃｺmero cercano a cero, se amplifica el error. Se define como:\n",
        "\n",
        "$$MAPE = \\frac{1}{H}\\sum_{t=1}^H\\frac{100|e_t|}{X_t}$$\n",
        "\n",
        "- Root Mean Squared Error (RMSE): mide el promedio del error en tﾃｩrminos absolutos, ponderando en mayor medida los errores grandes debido al cuadrado de los tﾃｩrminos $e_t$. Por esta razﾃｳn, si hay outliers en los datos, puede estar sesgado. Sin embargo, al usar la raﾃｭz cuadrada, la penalizaciﾃｳn de los errores grandes es algo mﾃ｡s severa que en el caso de MSE (dado que $RMSE = \\sqrt{MSE}$). Como tiene las mismas unidades que la variable estudiada, se puede comparar directamente con ella. No obstante, es mﾃ｡s difﾃｭcil de interpretar en comparaciﾃｳn con el MAPE, especialmente cuando la variable estudiada tiene una escala grande. Se define como:\n",
        "\n",
        "$$RMSE = \\sqrt{\\frac{1}{H}\\sum_{t=1}^He_t^2}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RFoE32gWbd3H"
      },
      "outputs": [],
      "source": [
        "def mean_absolute_percent_error(y_true, y_pred):\n",
        "    # Evitar divisiﾃｳn por cero\n",
        "    epsilon = 1e-8\n",
        "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + epsilon)) * 100)\n",
        "    if mape > 100 or mape == np.inf:\n",
        "        return 100\n",
        "    else:\n",
        "        return mape\n",
        "\n",
        "def root_mean_squared_error(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8qiNXT8bd3G"
      },
      "source": [
        "### Solo con la variable NOX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para empezar, vamos a hacer algunas predicciones iniciales con modelos simples que se entrenan y hacen las predicciones ﾃｺnicamente usando la variable `NOX`. Inicialmente, emplearemos un modelo baseline muy simple, que nos permitirﾃ｡ obtener una primera aproximaciﾃｳn de los errores y evaluar el comportamiento general de los datos. Este enfoque servirﾃ｡ como punto de referencia para comparar el desempeﾃｱo de modelos mﾃ｡s avanzados.\n",
        "\n",
        "Ademﾃ｡s, utilizaremos un modelo con ventana deslizante, en el que se incorpora estructura temporal para capturar patrones en los datos de `NOX`. Este modelo, aunque todavﾃｭa sencillo, permite integrar informaciﾃｳn de perﾃｭodos anteriores de la variable `NOX`, ademﾃ｡s de ofrecer unos resultados mﾃ｡s robustos para nuestras predicciones iniciales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modelo: baseLine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En primer lugar, vamos a probar como modelo baseline el modelo `ForecasterEquivalentDate`, tambiﾃｩn conocido como *Seasonal Naive Forecasting* que simplemente devuelve el valor observado en el mismo periodo de la temporada anterior. Dado que hemos observado cierta estacionalidad anual, se asume que el comportamiento de la serie en el mismo perﾃｭodo de tiempo en el aﾃｱo anterior es una buena aproximaciﾃｳn para el futuro. De esta forma, la predicciﾃｳn obtenida mediante este modelo para el valor de la variable `NOX` para un dﾃｭa serﾃ｡ simplemente el valor del mismo dﾃｭa en el aﾃｱo anterior. Por tanto, es evidente que este modelo utiliza ﾃｺnicamente la variable `NOX` para realizar las predicciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Definimos el modelo y lo entrenamos con los datos de entrenamiento. Los parﾃ｡metros de este modelo son los siguientes:\n",
        "\n",
        "- `offset`: Define el desplazamiento temporal utilizado para identificar las fechas equivalentes. En este caso, `pd.DateOffset(days=365)` significa que se buscarﾃ｡n datos del mismo dﾃｭa del aﾃｱo anterior.\n",
        "- `n_offset`: Especifica el nﾃｺmero de desplazamientos que se considerarﾃ｡n para hacer las predicciones. Por ejemplo, en nuestro caso, `n_offsets = 1` indica que el modelo utilizarﾃ｡ ﾃｺnicamente el valor del aﾃｱo inmediatamente anterior como predicciﾃｳn.\n",
        "- `agg_func`: Se refiere a la funciﾃｳn usada para agregar los valores de las fechas equivalentes cuando `n_offset` es mayor que $1$. Como en nuestro caso `n_offsets = 1`, esta funciﾃｳn no se usarﾃ｡."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseLine = ForecasterEquivalentDate(\n",
        "                 offset    = pd.DateOffset(days=365),\n",
        "                 n_offsets = 1,\n",
        "                 agg_func  = 'mean',\n",
        "             )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaciﾃｳn, entrenamos el modelo con la serie temporal de train formada ﾃｺnicamente por la variable `NOX`. Ademﾃ｡s, se generan predicciones para el mismo perﾃｭodo que los datos de test. Por ﾃｺltimo, se filtran los datos para restringir la visualizaciﾃｳn a partir de una fecha especﾃｭfica, facilitando la intrepretaciﾃｳn de los resultados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseLine.fit(y = train_NOX)\n",
        "baseLine_pred = baseLine.predict(steps = len(test_NOX))\n",
        "\n",
        "# Filtrar los datos para mostrar solo a partir de 2022\n",
        "train_NOX_filtered = train_NOX[train_NOX.index >= '2022-01-01']\n",
        "test_NOX_filtered = test_NOX[test_NOX.index >= '2022-01-01']\n",
        "baseLine_pred_filtered = baseLine_pred[baseLine_pred.index >= '2022-01-01']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el siguiente cﾃｳdigo se generan las predicciones de `NOX` en los 7 dﾃｭas futuros, posteriores a los datos de nuestro dataset, usando el modelo `baseLine`. Ademﾃ｡s, se visualizan los resultados en un grﾃ｡fico en el que se muestran las predicciones del conjunto de test junto a los valores reales de test, ademﾃ｡s de las predicciones para los 7 dﾃｭas futuros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crear ﾃｭndice de fechas para los prﾃｳximos 7 dﾃｭas\n",
        "future_dates = pd.date_range(start=test_NOX.index[-1] + pd.Timedelta(days=1), periods=7, freq='D')\n",
        "\n",
        "# Crear un DataFrame para representar las nuevas fechas\n",
        "future_df = pd.DataFrame(index=future_dates)\n",
        "\n",
        "# Hacer predicciones para los prﾃｳximos 7 dﾃｭas con ForecasterEquivalentDate\n",
        "baseLine_future_pred = baseLine.predict(steps=len(future_dates))\n",
        "\n",
        "# Crear un DataFrame con las predicciones futuras\n",
        "future_predictions_df = pd.DataFrame({'Fecha': future_dates, 'predicciones': baseLine_future_pred})\n",
        "future_predictions_df.set_index('Fecha', inplace=True)\n",
        "\n",
        "# Mostrar las predicciones futuras\n",
        "print(future_predictions_df)\n",
        "\n",
        "# Graficar los resultados incluyendo las predicciones futuras\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "train_NOX_filtered.plot(ax=ax, label='train')\n",
        "test_NOX_filtered.plot(ax=ax, label='test')\n",
        "baseLine_pred_filtered.plot(ax=ax, label='baseline predictions')\n",
        "future_predictions_df['predicciones'].plot(ax=ax, label='future predictions', color='red')\n",
        "ax.legend()\n",
        "plt.title('Baseline Predictions Including Future Dates')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el grﾃ｡fico se puede observar que las predicciones obtenidas por el modelo `baseLine` en el conjunto de test, si bien no se alejan de una forma exagerada de los valores reales, no logran captar con precisiﾃｳn las variaciones presentes en los datos originales. Esto es esperable dado que el modelo `baseLine` calcula las predicciones basﾃ｡ndose ﾃｺnicamente en el valor de la variable `NOX` en el aﾃｱo anterior, lo que lo hace un modelo muy simple e incapaz de capturar fluctuaciones mﾃ｡s complejas en los datos.\n",
        "\n",
        "Ademﾃ｡s, los predicciones futuras para los prﾃｳximos 7 dﾃｭas muestran una continuidad en la tendencia de los datos, dando una primera estimaciﾃｳn de los valores esperados, aunque con claras limitaciones, especialmente en situaciones donde el comportamiento de los datos pueda ser mﾃ｡s dinﾃ｡mico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Error Backtesting\n",
        "\n",
        "Para evaluar la robustez del modelo, utilizamos el mﾃｩtodo backtesting, que consiste en divir los datos histﾃｳricos en varias ventanas de tiempo. En cada iteraciﾃｳn, se entrena el modelo en un subconjunto inicial, y se evalﾃｺa su desempeﾃｱo en un subconjunto consecutivo. Este procedimimiento se repite, desplazando las ventanas hacia adelante en el tiempo.\n",
        "\n",
        "Gracias a este enfoque, se logran mﾃｩtricas de error que reflejan de una forma mﾃ｡s realista el desempeﾃｱo del modelo en datos que no ha visto durante su entrenamiento. En este caso, las mﾃｩtricas usadas para el backtesting serﾃ｡n el MAPE y el RMSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "cv = TimeSeriesFold(\n",
        "    steps = 7,\n",
        "    refit = False,\n",
        "    initial_train_size = 1000\n",
        ")\n",
        "\n",
        "baseLine_backtesting_error, predictions_back = backtesting_forecaster(\n",
        "    forecaster = baseLine,\n",
        "    y = train_NOX,\n",
        "    cv = cv,\n",
        "    metric = [mean_absolute_percent_error, root_mean_squared_error],\n",
        "    n_jobs = 'auto',\n",
        "    verbose = True,\n",
        "    show_progress = True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(baseLine_backtesting_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tras realizar el backtesting, observamos que las mﾃｩtricas de error obtenidas son las siguientes:\n",
        "\n",
        "- $MAPE = 49.114614\\%$\n",
        "\n",
        "    Este valor indica que, en promedio, las predicciones del modelo tienen un error relativo del $49.11\\%$ respecto de los valores reales. Un MAPE alto como este ($49.11$ sobre $100$) sugiere que el modelo tiene dificultades para ajustarse con precisiﾃｳn a la serie temporal. Esto es esperable, debido a la gran simplicidad del modelo `baseLine`.\n",
        "\n",
        "- $RMSE = 8.479244$\n",
        "\n",
        "    Para poder interpretar este valor, debemos conocer algunos estadﾃｭsticos para la variable `NOX`. Dado que el valor mﾃｭnimo de esta variable es $3.25$ y el mﾃ｡ximo es $60$, un $RMSE = 8.479244$ es un valor relativamente alto en comparaciﾃｳn con el rango de la variable. De hecho, en tﾃｩrminos relativos, el $RMSE$ es aproximadamente el $14\\%$ del valor mﾃ｡ximo de `NOX`, lo cual indica que las predicciones del modelo en promedio se estﾃ｡n desviando bastante de los valores reales, mﾃ｡s especﾃｭficamente se desvﾃｭan un $14\\%$ del mﾃ｡ximo de la variable.\n",
        "\n",
        "    Por otro lado, la desviaciﾃｳn estﾃ｡ndar ($7.493708$) de `NOX` es bastante alta, lo cual indica que hay una dispersiﾃｳn considerable en los datos. De esta forma, aunque el modelo no estﾃ｡ produciendo muy buenas predicciones, es importante tener en cuenta que las predicciones no estﾃ｡n tan desviadas del comportamiento esperado. La alta variabilidad puede atenuar la interpretaciﾃｳn del RMSE como un error excesivo, por lo que en nuestron caso, un error absoluto de $8.48$ podrﾃｭa ser ligeramente mﾃ｡s aceptable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_diario['NOX'].describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Error Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora, calculamos el error RMSE y MAPE, pero esta vez sobre las predicciones que calculamos anteriormente para el conjunto de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseLine_test_error_rmse = root_mean_squared_error(\n",
        "                y_true = test_NOX,\n",
        "                y_pred = baseLine_pred\n",
        "            )\n",
        "\n",
        "baseLine_test_error_mape = mean_absolute_percent_error(\n",
        "                y_true = test_NOX,\n",
        "                y_pred = baseLine_pred\n",
        "            )\n",
        "\n",
        "print(f\"Error RMSE: {baseLine_test_error_rmse}\")\n",
        "print(f\"Error MAPE: {baseLine_test_error_mape:.2f}%\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Observamos que en este caso, el error obtenido con ambas mﾃｩtricas es claramente menor que en el backtesting. Por un lado, el $RMSE = 3.0975$ indica que el modelo se desviﾃｳ en aproximadamente $3.0975$ unidades de `NOX` en el conjunto de prueba. Por otro lado, el $MAPE = 19.49\\%$ indica que en promedio, las predicciones estﾃ｡n desviadas en un $19.49\\%$ respecto a los valores reales.\n",
        "\n",
        "Una posible razﾃｳn por la que se podrﾃｭa dar esta diferencia tan significativa entre los errores de test y los de backtesting es que los datos seleccionados en el conjunto de test sean mﾃ｡s fﾃ｡ciles o representativos del comportamiento general del modelo, mientras que en los datos de backtesting, entre todas las ventanas de tiempo seleccionadas, podrﾃｭa haber datos mﾃ｡s complejos o outliers, haciendo que el modelo tuviera mﾃ｡s dificultades para ajustarse."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Modelo: RandomForestRegressor con ventana deslizante\n",
        "\n",
        "El siguiente modelo que vamos a utilizar va a ser un RandomForestRegressor con ventana deslizante, combinando los modelos de ﾃ｡rboles de decisiﾃｳn con un enfoque de series temporales. El modelo RandomForestRegressor estﾃ｡ formado por un conjunto de ﾃ｡rboles de decisiﾃｳn entrenados sobre subconjuntos aleatorios de los datos.\n",
        "\n",
        "Por otro lado, la ventana deslizante es una tﾃｩcnica que utiliza ventanas de tiempo en las cuales se van utilizando los datos pasados (valores en $t-1$, $t-2$, $t-3$, etc) para predecir el valor en $t+1$. Asﾃｭ, usaremos la siguiente funciﾃｳn para crear las variables lags:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_lagged_features(data, lags, horizons, target_column, dropna=False):\n",
        "    \"\"\"\n",
        "    Crea variables lags y variables para el horizonte temporal a partir de una columna objetivo. \n",
        "    \n",
        "    Parﾃ｡metros:\n",
        "    - data: pd.DataFrame - Dataframe original con la columna objetivo de la predicciﾃｳn. \n",
        "    - lags: list[int] - Lista de lags a introducir en el dataframe (e.g., [1, 2, 3] for t-1, t-2, t-3).\n",
        "    - horizons: list[int] - Lista de pasos en el horizonte temporal de predicciﾃｳn. lis (e.g., [1, 2, 3] for t+1, t+2, t+3).\n",
        "    - target_column: str - Nombre de la columna que contiene la serie a predecir. \n",
        "    \n",
        "    Devuelve:\n",
        "    - pd.DataFrame - Dataframe con las variables lags y las variables del horizonte temporal \n",
        "    \"\"\"\n",
        "    \n",
        "    df = data.copy()\n",
        "    \n",
        "    # Creaciﾃｳn de las variables lags\n",
        "    for lag in lags:\n",
        "    #    df[f'{target_column}_lag_{lag}'] = df[target_column].shift(lag)\n",
        "        df.insert(loc=0, column=f'{target_column}_lag_{lag}',value=df[target_column].shift(lag))\n",
        "    \n",
        "    # Creaciﾃｳn de las variables en el horizonte de predicciﾃｳn. \n",
        "    for horizon in horizons:\n",
        "        df[f'{target_column}_horizon_{horizon+1}'] = df[target_column].shift(-horizon)\n",
        "    \n",
        "    # Se eliminan los valores ausentes creados. \n",
        "    if dropna:\n",
        "        df = df.dropna()\n",
        "    \n",
        "    \n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Seleccionamos la variable `NOX` del dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOX = df_concatenado_diario['NOX']\n",
        "\n",
        "NOX_df = pd.DataFrame(NOX, index=NOX.index)\n",
        "NOX_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A continuaciﾃｳn se crean las variables lags, usando la funciﾃｳn anterior. Dado que hemos observado que nuestra serie tiene una cierta estacionariedad semanal, y que ademﾃ｡s el objetivo de la prﾃ｡ctica es predecir los prﾃｳximos 7 dﾃｭas de la serie para la variable `NOX`, vamos a probar a tomar 7 variables lags a partir de dicha variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_NOX_lagged = create_lagged_features(NOX_df,lags=range(1,8),horizons=range(1,1),target_column='NOX')\n",
        "df_NOX_lagged"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Al obtener el dataset con las variables lags, observamos que, como es de esperar, las primeras 7 filas contienen valores nulos (resultado de la creaciﾃｳn de la ventana con las variables lags). Por ello, vamos a eliminar estas filas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eliminar las primeras 7 filas (contienen valores NaN)\n",
        "df_NOX_lagged = df_NOX_lagged.iloc[7:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora, con el dataset resultante, que contiene la variable `NOX` y sus correspondientes variables lags, vamos a separar los datos en un conjunto de train y otro de test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "porcentaje_entrenamiento = 0.9\n",
        "test_steps = int(len(df_NOX_lagged) * porcentaje_entrenamiento)\n",
        "df_NOX_lagged_train = df_NOX_lagged[:test_steps]\n",
        "df_NOX_lagged_test = df_NOX_lagged[test_steps:]\n",
        "\n",
        "print(f'Con el porcentaje de %.2f tenemos:' %porcentaje_entrenamiento)\n",
        "print(f'Tamaﾃｱo del conjunto de training es %i' %len(df_NOX_lagged_train))\n",
        "print(f'Tamaﾃｱo del conjunto de test es %i' %len(df_NOX_lagged_test))\n",
        "print(f'El conjunto de training va de {min(df_NOX_lagged_train.index)} y {max(df_NOX_lagged_train.index)}')\n",
        "print(f'El conjunto de test va de {min(df_NOX_lagged_test.index)} y {max(df_NOX_lagged_test.index)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(8, 4))\n",
        "df_NOX_lagged_train['NOX'].plot(ax=ax, label='train')\n",
        "df_NOX_lagged_test['NOX'].plot(ax=ax, label='test')\n",
        "ax.legend(labels=['Train', 'Test'], loc='best')\n",
        "ax.set_xlabel('Aﾃｱo')\n",
        "ax.set_ylabel('NOX')\n",
        "ax.set_title('NOX mensual')\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Separamos las variables independientes de las dependientes en ambos conjuntos. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_NOX_lagged_train = df_NOX_lagged_train.drop(columns = 'NOX')\n",
        "y_NOX_lagged_train = df_NOX_lagged_train['NOX']\n",
        "\n",
        "X_NOX_lagged_test = df_NOX_lagged_test.drop(columns='NOX')\n",
        "y_NOX_lagged_test = df_NOX_lagged_test['NOX']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora que tenemos preparado el DataFrame, comenzamos a crear el modelo predictivo. Primero creamos una instancia del modelo que queremos entrenar, que en este caso, va a ser un random forest para regresiﾃｳn. Para buscar sus mejores parﾃ｡metros, usamos `RandomizedSearchCV` que realiza una bﾃｺsqueda aleatoria, en lugar de probar todas las combinaciones posibles entre los parﾃ｡metros indicados. Esto es conveniente para modelos mﾃ｡s complejos o con un grid con demasiados hiperparﾃ｡metros, ya que probar todas las combinaciones posibles puede ser muy costoso computacionalmente.\n",
        "\n",
        "Algunos de los parﾃ｡metros de este modelo son:\n",
        "\n",
        "- `n_estimators`: especifica el nﾃｺmero de ﾃ｡rboles que compondrﾃ｡n el modelo.\n",
        "- `max_depth`: define la profundidad mﾃ｡xima de los ﾃ｡rboles, es decir, el nﾃｺmero mﾃ｡ximo de divisiones que cada ﾃ｡rbol puede hacer. Si toma un valor demasiado alto, puede llevar a overfitting.\n",
        "- `min_samples_split`: establece el nﾃｺmero mﾃｭnimo de muestras que debe haber en un nodo para que se realice una divisiﾃｳn. Si se establece un valor bajo, el modelo serﾃ｡ mﾃ｡s propenso al sobreajuste.\n",
        "- `min_samples_leaf`: especifica el nﾃｺmero mﾃｭnimo de muestras que debe tener una hoja para que sea vﾃ｡lida. Si se establece un valor bajo, el modelo puede crear hojas con pocos datos, lo que puede hacer que el modelo se sobreajuste al ruido en los datos de entrenamiento.\n",
        "- `max_features`: determina el nﾃｺmero de caracterﾃｭsticas a considerar al dividir un nodo en los ﾃ｡rboles, por ejemplo, si es `\"sqrt\"` se selecciona la raﾃｭz cuadrada del total de caracterﾃｭsticas, y si es `\"log2\"` se selecciona el logaritmo en base $2$ del total de caracterﾃｭsticas.\n",
        "- `bootstrap`: indica si se utiliza el muestreo con o sin reemplazo al construir los ﾃ｡rboles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Definimos los rangos de los parﾃ｡metros\n",
        "param_dist = {\n",
        "    \"n_estimators\": [100, 200, 300, 500],  # Nﾃｺmero de ﾃ｡rboles\n",
        "    \"max_depth\": [2, 3],  # Profundidad mﾃ｡xima de los ﾃ｡rboles\n",
        "    \"min_samples_split\": [5, 10, 20],  # Mﾃｭnimas muestras para dividir\n",
        "    \"min_samples_leaf\": [4, 5, 10],  # Mﾃｭnimas muestras en hoja\n",
        "    \"max_features\": [\"sqrt\", \"log2\"],  # Selecciﾃｳn de caracterﾃｭsticas\n",
        "    \"bootstrap\": [True, False],  # Muestreo con o sin reemplazo\n",
        "}\n",
        "\n",
        "# Definimos el modelo base\n",
        "rf_forecaster = RandomForestRegressor(random_state = semilla)\n",
        "\n",
        "# Creamos un scoring personalizado con MAPE\n",
        "mape = make_scorer(mean_absolute_percent_error, greater_is_better=False)\n",
        "\n",
        "# Configuramos RandomizedSearchCV\n",
        "random_cv = RandomizedSearchCV(\n",
        "    estimator=rf_forecaster,\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=15,  # Nﾃｺmero de iteraciones aleatorias\n",
        "    scoring=mape,  # Mﾃｩtrica\n",
        "    cv=5,  # Nﾃｺmero de particiones para validaciﾃｳn cruzada\n",
        "    verbose=1,\n",
        "    random_state=semilla\n",
        ")\n",
        "\n",
        "# Realizamos la bﾃｺsqueda\n",
        "random_cv.fit(X_NOX_lagged_train, y_NOX_lagged_train)\n",
        "\n",
        "# Mejor modelo y parﾃ｡metros\n",
        "rf_forecaster_best = random_cv.best_estimator_\n",
        "print(\"Best parameters:\", random_cv.best_params_)\n",
        "print(\"Best score:\", random_cv.best_score_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para construir el modelo, procedemos con la funciﾃｳn `.fit()` sobre el conjunto de entrenamiento. A continuaciﾃｳn, realizamos las predicciones en el conjunto de test usando `.predict`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "rf_fit = rf_forecaster_best.fit(X_NOX_lagged_train, y_NOX_lagged_train)\n",
        "\n",
        "y_pred_test_rf = rf_fit.predict(X_NOX_lagged_test)\n",
        "y_pred_test_rf = pd.DataFrame(y_pred_test_rf, index=X_NOX_lagged_test.index, columns=['predicciones'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora mostramos un grﾃ｡fico con las predicciones y los valores reales de test. Se puede apreciar que con este modelo, las predicciones parecen algo mejores que con el modelo `baseLine` probado anteriormente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(15, 10))\n",
        "\n",
        "# Graficar los datos de entrenamiento, prueba y predicciones\n",
        "df_NOX_lagged_train['NOX'].plot(ax=ax, label='Entrenamiento')\n",
        "df_NOX_lagged_test['NOX'].plot(ax=ax, label='Prueba')\n",
        "y_pred_test_rf.plot(ax=ax, color='green', alpha=0.7, label='Predicciones')\n",
        "\n",
        "# Configurar el lﾃｭmite en el eje x para mostrar datos desde 2022-01-01\n",
        "ax.set_xlim(pd.Timestamp('2022-01-01'), None)  # Lﾃｭmite inferior, sin lﾃｭmite superior\n",
        "\n",
        "# Personalizar etiquetas y leyendas\n",
        "ax.set_xlabel('Aﾃｱo')\n",
        "ax.set_ylabel('NOX')\n",
        "ax.set_title('NOX mensual')\n",
        "ax.legend(labels=['Entrenamiento', 'Prueba', 'Predicciones'])\n",
        "\n",
        "# Ajustar diseﾃｱo\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Calculamos ahora los errores obtenidos en el conjunto de test. Los errores obtenidos son:\n",
        "\n",
        "- $MAPE = 27.00\\%$\n",
        "\n",
        "    El $MAPE$ mide el error absoluto relativo en porcentaje entre las predicciones y los valores reales. En este caso, el $MAPE$ es de $27.00\\%$, lo que significa que las predicciones se desvﾃｭan en promedio un $27\\%$ respecto a los valores reales. Este valor es relativamente alto, lo que indica que las predicciones del modelo no son tan precisas.\n",
        "\n",
        "- $RMSE = 1.9765$\n",
        "\n",
        "    El $RMSE$ mide la raﾃｭz cuadrada de la media de los errores al cuadrado. En este caso, el $RMSE$ es de $1.98$. El rango de `NOX` va de $3.25$ a $60$, por lo que un $RMSE$ de $1.98$ es relativamente bajo en comparaciﾃｳn con el rango de la variable. De hecho, este valor de $RMSE$ representa un $3.3\\%$ del valor mﾃ｡ximo de la variable. Este porcentaje tambiﾃｩn indica que, aunque el modelo tiene algunos errores, los errores no son excesivamente grandes en relaciﾃｳn con el rango de los datos.\n",
        "\n",
        "En resumen, aunque los errores no son extremadamente altos, hay espacio para mejorar las predicciones, especialmente en lo que respecta al $MAPE$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mape = mean_absolute_percent_error(y_NOX_lagged_test, y_pred_test_rf)\n",
        "rmse = root_mean_squared_error(y_NOX_lagged_test, y_pred_test_rf)\n",
        "\n",
        "print(f'MAPE: {mape}%')\n",
        "print('RMSE:', rmse)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por ﾃｺltimo, en la siguiente celda se calculan las predicciones para los 7 dﾃｭas futuros, posteriores a los datos de nuestro dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Inicializamos un DataFrame para almacenar predicciones futuras\n",
        "future_predictions = []\n",
        "\n",
        "# Copiar las ﾃｺltimas filas de X_NOX_lagged_test (las usaremos como base)\n",
        "X_future = X_NOX_lagged_test.iloc[-1:].copy()\n",
        "\n",
        "# Hacer predicciones para los 7 dﾃｭas futuros\n",
        "for i in range(7):\n",
        "    # Predecir el prﾃｳximo dﾃｭa\n",
        "    next_pred = rf_forecaster_best.predict(X_future)[0]\n",
        "\n",
        "    # Guardamos la predicciﾃｳn\n",
        "    future_predictions.append(next_pred)\n",
        "\n",
        "    # Actualizar las variables lagged\n",
        "    for lag in range(4, 1, -1):  # Desde NOX_lag_4 hasta NOX_lag_2\n",
        "        X_future[f'NOX_lag_{lag}'] = X_future[f'NOX_lag_{lag-1}']\n",
        "    X_future['NOX_lag_1'] = next_pred  # Actualizamos NOX_lag_1 con la nueva predicciﾃｳn\n",
        "\n",
        "# Crear DataFrame final con las fechas y predicciones\n",
        "future_dates = pd.date_range(start=X_NOX_lagged_test.index[-1] + pd.Timedelta(days=1), periods=7, freq='D')\n",
        "future_predictions_df = pd.DataFrame({'Fecha': future_dates, 'predicciones': future_predictions})\n",
        "future_predictions_df.set_index('Fecha', inplace=True)\n",
        "\n",
        "# Mostrar predicciones\n",
        "print(future_predictions_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicciﾃｳn recursiva"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preparamos los datasets para tener tanto sin transformar como transformados con solo variables del aire."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_concatenado_aire_diario = df_concatenado_diario.drop(['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'], axis=1)\n",
        "df_concatenado_aire_diario_transformed = df_concatenado_diario_transformed.drop(['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'], axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Usamos la funciﾃｳn `create_lagged_features` para crear las variables lagged de las series temporales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_lagged_features(data, lags, horizons, target_column, dropna=False):\n",
        "    \"\"\"\n",
        "    Crea variables lags y variables para el horizonte temporal a partir de una columna objetivo.\n",
        "\n",
        "    Parﾃ｡metros:\n",
        "    - data: pd.DataFrame - Dataframe original con la columna objetivo de la predicciﾃｳn.\n",
        "    - lags: list[int] - Lista de lags a introducir en el dataframe (e.g., [1, 2, 3] for t-1, t-2, t-3).\n",
        "    - horizons: list[int] - Lista de pasos en el horizonte temporal de predicciﾃｳn. lis (e.g., [1, 2, 3] for t+1, t+2, t+3).\n",
        "    - target_column: str - Nombre de la columna que contiene la serie a predecir.\n",
        "\n",
        "    Devuelve:\n",
        "    - pd.DataFrame - Dataframe con las variables lags y las variables del horizonte temporal\n",
        "    \"\"\"\n",
        "\n",
        "    df = data.copy()\n",
        "\n",
        "    # Creaciﾃｳn de las variables lags\n",
        "    for lag in lags:\n",
        "    #    df[f'{target_column}_lag_{lag}'] = df[target_column].shift(lag)\n",
        "        df.insert(loc=0, column=f'{target_column}_lag_{lag}',value=df[target_column].shift(lag))\n",
        "\n",
        "    # Creaciﾃｳn de las variables en el horizonte de predicciﾃｳn.\n",
        "    for horizon in horizons:\n",
        "        df[f'{target_column}_horizon_{horizon+1}'] = df[target_column].shift(-horizon)\n",
        "\n",
        "    # Se eliminan los valores ausentes creados.\n",
        "    if dropna:\n",
        "        df = df.dropna()\n",
        "\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para poder facilitar la bﾃｺsqueda de hiperparﾃ｡metros hemos definido la funciﾃｳn `process_variable` que para cada una de las variables y cada uno de las tres tﾃｩcnicas obtenga el mejor modelo optimizado. Lo guardamos de forma que podamos cargarlo en el futuro."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_variable(variable_name, df, test_size, param_grid, regressor, model_dir, train_mode=True):\n",
        "    \"\"\"\n",
        "    Procesa una variable para entrenamiento, optimizaciﾃｳn y predicciﾃｳn.\n",
        "\n",
        "    Args:\n",
        "        variable_name (str): Nombre de la variable objetivo.\n",
        "        df (pd.DataFrame): DataFrame con las series temporales.\n",
        "        test_size (int): Tamaﾃｱo del conjunto de prueba.\n",
        "        param_grid (dict): Grid de hiperparﾃ｡metros para optimizaciﾃｳn.\n",
        "        regressor: Regresor a usar en el modelo.\n",
        "        model_dir (str): Directorio donde guardar/cargar los modelos.\n",
        "        train_mode (bool): Si es True, entrena el modelo; si es False, utiliza un modelo preentrenado.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Predicciones finales.\n",
        "        float: MAPE del modelo.\n",
        "        float: RMSE del modelo.\n",
        "    \"\"\"\n",
        "    # Preparaciﾃｳn de datos\n",
        "    test_restante = test_size\n",
        "    train = df[variable_name].to_frame()\n",
        "    train_actual = train[:-test_restante]\n",
        "\n",
        "    # Crear el directorio de modelos si no existe\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_filename = os.path.join(model_dir, f\"model_{variable_name}.joblib\")\n",
        "\n",
        "    if train_mode:\n",
        "        # Instanciar el ForecasterAutoreg con el regresor especificado\n",
        "        forecaster = ForecasterRecursive(\n",
        "            regressor=regressor,\n",
        "            lags=range(1, 8)  # Lags de 1 a 7\n",
        "        )\n",
        "\n",
        "        # Realizar la bﾃｺsqueda de hiperparﾃ｡metros\n",
        "        cv = TimeSeriesFold(\n",
        "            steps=7,\n",
        "            initial_train_size=520,\n",
        "            refit=True\n",
        "        )\n",
        "\n",
        "        results = grid_search_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            y=train[variable_name],\n",
        "            param_grid=param_grid,\n",
        "            lags_grid=None,\n",
        "            cv=cv,\n",
        "            metric=[mean_absolute_percent_error, root_mean_squared_error],\n",
        "            n_jobs='auto',\n",
        "            verbose=False,\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        best_params = results.loc[results['mean_absolute_percent_error'].idxmin()]\n",
        "        print(f\"Mejores hiperparﾃ｡metros para {variable_name}: {best_params}\")\n",
        "        model_params = best_params['params']\n",
        "\n",
        "        # Actualizar el regresor con los mejores parﾃ｡metros\n",
        "        forecaster.regressor = regressor.__class__(**model_params)\n",
        "\n",
        "        # Guardar el modelo entrenado\n",
        "        save_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            file_name=model_filename,\n",
        "            save_custom_functions=True,\n",
        "            verbose=True\n",
        "        )\n",
        "        print(f\"Modelo guardado como {model_filename}\")\n",
        "    else:\n",
        "        if os.path.exists(model_filename):\n",
        "            forecaster = load_forecaster(model_filename)\n",
        "            print(f\"Modelo cargado desde {model_filename}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"El modelo preentrenado {model_filename} no se encontrﾃｳ.\")\n",
        "\n",
        "    # Predicciones\n",
        "    final_predictions = pd.DataFrame()\n",
        "\n",
        "    while test_restante > 0:\n",
        "        lagged_features = create_lagged_features(\n",
        "            train_actual, lags=range(1, 8), horizons=range(1, 1), target_column=variable_name\n",
        "        )\n",
        "\n",
        "        lagged_features.dropna(inplace=True)\n",
        "\n",
        "        X_train = lagged_features.drop(columns=variable_name)\n",
        "        y_train = lagged_features[variable_name]\n",
        "        forecaster.fit(y=y_train, exog=None)\n",
        "\n",
        "        if test_restante >= 7:\n",
        "            predictions = forecaster.predict(steps=7)\n",
        "        else:\n",
        "            predictions = forecaster.predict(steps=test_restante)\n",
        "\n",
        "        final_predictions = pd.concat([final_predictions, predictions], axis=0)\n",
        "        test_restante -= min(7, test_restante)\n",
        "        train_actual = train[:-test_restante] if test_restante > 0 else train\n",
        "\n",
        "    # Evaluaciﾃｳn\n",
        "    print(f\"Predicciones para {variable_name}:\\n{final_predictions}\")\n",
        "    test_actual = train[-test_size:]\n",
        "    mape = mean_absolute_percent_error(test_actual, final_predictions)\n",
        "    rmse = root_mean_squared_error(test_actual, final_predictions)\n",
        "\n",
        "    # Graficar resultados\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    test_actual.plot(ax=ax, label='Test (real)')\n",
        "    final_predictions.plot(ax=ax, label='Predicciones')\n",
        "    ax.legend()\n",
        "    plt.title(f'Predicciones {variable_name}')\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"MAPE para {variable_name}: {mape:.2%}\")\n",
        "    print(f\"RMSE para {variable_name}: {rmse:.4f}\")\n",
        "\n",
        "    return final_predictions, mape, rmse\n",
        "\n",
        "def process_variable_sin_mape(variable_name, df, test_size, param_grid, regressor, model_dir, train_mode=True):\n",
        "    \"\"\"\n",
        "    Procesa una variable para entrenamiento, optimizaciﾃｳn y predicciﾃｳn.\n",
        "\n",
        "    Args:\n",
        "        variable_name (str): Nombre de la variable objetivo.\n",
        "        df (pd.DataFrame): DataFrame con las series temporales.\n",
        "        test_size (int): Tamaﾃｱo del conjunto de prueba.\n",
        "        param_grid (dict): Grid de hiperparﾃ｡metros para optimizaciﾃｳn.\n",
        "        regressor: Regresor a usar en el modelo.\n",
        "        model_dir (str): Directorio donde guardar/cargar los modelos.\n",
        "        train_mode (bool): Si es True, entrena el modelo; si es False, utiliza un modelo preentrenado.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Predicciones finales.\n",
        "        float: MAPE del modelo.\n",
        "        float: RMSE del modelo.\n",
        "    \"\"\"\n",
        "    # Preparaciﾃｳn de datos\n",
        "    test_restante = test_size\n",
        "    train = df[variable_name].to_frame()\n",
        "    train_actual = train[:-test_restante]\n",
        "\n",
        "    # Crear el directorio de modelos si no existe\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_filename = os.path.join(model_dir, f\"model_{variable_name}.joblib\")\n",
        "\n",
        "    if train_mode:\n",
        "        # Instanciar el ForecasterAutoreg con el regresor especificado\n",
        "        forecaster = ForecasterRecursive(\n",
        "            regressor=regressor,\n",
        "            lags=range(1, 8)  # Lags de 1 a 7\n",
        "        )\n",
        "\n",
        "        # Realizar la bﾃｺsqueda de hiperparﾃ｡metros\n",
        "        cv = TimeSeriesFold(\n",
        "            steps=7,\n",
        "            initial_train_size=520,\n",
        "            refit=True\n",
        "        )\n",
        "\n",
        "        results = grid_search_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            y=train[variable_name],\n",
        "            param_grid=param_grid,\n",
        "            lags_grid=None,\n",
        "            cv=cv,\n",
        "            metric=[root_mean_squared_error],\n",
        "            n_jobs='auto',\n",
        "            verbose=False,\n",
        "            show_progress=True\n",
        "        )\n",
        "\n",
        "        best_params = results.loc[results['mean_absolute_percent_error'].idxmin()]\n",
        "        print(f\"Mejores hiperparﾃ｡metros para {variable_name}: {best_params}\")\n",
        "        model_params = best_params['params']\n",
        "\n",
        "        # Actualizar el regresor con los mejores parﾃ｡metros\n",
        "        forecaster.regressor = regressor.__class__(**model_params)\n",
        "\n",
        "        # Guardar el modelo entrenado\n",
        "        save_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            file_name=model_filename,\n",
        "            save_custom_functions=True,\n",
        "            verbose=True\n",
        "        )\n",
        "        print(f\"Modelo guardado como {model_filename}\")\n",
        "    else:\n",
        "        if os.path.exists(model_filename):\n",
        "            forecaster = load_forecaster(model_filename)\n",
        "            print(f\"Modelo cargado desde {model_filename}\")\n",
        "        else:\n",
        "            raise FileNotFoundError(f\"El modelo preentrenado {model_filename} no se encontrﾃｳ.\")\n",
        "\n",
        "    # Predicciones\n",
        "    final_predictions = pd.DataFrame()\n",
        "\n",
        "    while test_restante > 0:\n",
        "        lagged_features = create_lagged_features(\n",
        "            train_actual, lags=range(1, 8), horizons=range(1, 1), target_column=variable_name\n",
        "        )\n",
        "\n",
        "        lagged_features.dropna(inplace=True)\n",
        "\n",
        "        X_train = lagged_features.drop(columns=variable_name)\n",
        "        y_train = lagged_features[variable_name]\n",
        "        forecaster.fit(y=y_train, exog=None)\n",
        "\n",
        "        if test_restante >= 7:\n",
        "            predictions = forecaster.predict(steps=7)\n",
        "        else:\n",
        "            predictions = forecaster.predict(steps=test_restante)\n",
        "\n",
        "        final_predictions = pd.concat([final_predictions, predictions], axis=0)\n",
        "        test_restante -= min(7, test_restante)\n",
        "        train_actual = train[:-test_restante] if test_restante > 0 else train\n",
        "\n",
        "    # Evaluaciﾃｳn\n",
        "    print(f\"Predicciones para {variable_name}:\\n{final_predictions}\")\n",
        "    test_actual = train[-test_size:]\n",
        "    #mape = mean_absolute_percent_error(test_actual, final_predictions)\n",
        "    rmse = root_mean_squared_error(test_actual, final_predictions)\n",
        "\n",
        "    # Graficar resultados\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    test_actual.plot(ax=ax, label='Test (real)')\n",
        "    final_predictions.plot(ax=ax, label='Predicciones')\n",
        "    ax.legend()\n",
        "    plt.title(f'Predicciones {variable_name}')\n",
        "    plt.show()\n",
        "\n",
        "    #print(f\"MAPE para {variable_name}: {mape:.2%}\")\n",
        "    print(f\"RMSE para {variable_name}: {rmse:.4f}\")\n",
        "\n",
        "    return final_predictions, rmse\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### SIN TRANSFORMACION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Solo variables aire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para cada una de las variables vamos a predecir sus valores con los modelos optimizados hasta las fechas de test para poder usarlas como variables endﾃｳgenas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_directory = \"models/\"\n",
        "tipo_de_df = \"sin_transformacion\" #Cambiar con o sin transformaciﾃｳn dependiendo\n",
        "\n",
        "# Hiperparﾃ｡metros comunes\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "# Lista de regresores\n",
        "regressors = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "# Proceso para cada variable y cada regresor\n",
        "results = {}\n",
        "variables = [\"NO\", \"NO2\", \"SO2\", \"PM10\", \"O3\"]\n",
        "\n",
        "for var in variables:\n",
        "    for reg_name, regressor in regressors.items():\n",
        "        print(f\"Procesando {var} con {reg_name}...\")\n",
        "        predictions, mape, rmse = process_variable(\n",
        "            variable_name=var,\n",
        "            df=df_concatenado_aire_diario,\n",
        "            test_size=len(df_test),\n",
        "            param_grid=param_grid,\n",
        "            regressor=regressor,\n",
        "            model_dir=model_directory + reg_name + \"_\" + tipo_de_df,\n",
        "            train_mode=False  # Cambiar a False si deseas cargar modelos preentrenados\n",
        "        )\n",
        "        results[(var, reg_name)] = {\"predictions\": predictions, \"mape\": mape, \"rmse\": rmse}\n",
        "        print(f\"Resultados para {var} con {reg_name}: MAPE={mape:.2%}, RMSE={rmse:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df_test = {}\n",
        "\n",
        "for reg_name in regressors.keys():\n",
        "    print(f\"\\nProcesando predicciones para el modelo {reg_name}...\\n\")\n",
        "    \n",
        "    prediction_frames = []\n",
        "    \n",
        "    for var in variables:\n",
        "        predictions_df = results[(var, reg_name)]['predictions']\n",
        "        \n",
        "        if 'pred' in predictions_df.columns:\n",
        "            predictions_df = predictions_df[['pred']]  \n",
        "            predictions_df.columns = [var] \n",
        "        else:\n",
        "            print(f\"Advertencia: No se encontrﾃｳ la columna 'pred' en las predicciones para {var} con {reg_name}.\")\n",
        "            continue\n",
        "        \n",
        "        prediction_frames.append(predictions_df)\n",
        "    \n",
        "    model_df = pd.concat(prediction_frames, axis=1)\n",
        "    \n",
        "    predictions_df_test[reg_name] = model_df\n",
        "    \n",
        "    print(f\"Predicciones para el modelo {reg_name}:\")\n",
        "    print(model_df.head())  # Mostrar las primeras filas del DataFrame generado\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 250],\n",
        "    'max_depth': [3, 5, 8],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "results_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nRealizando grid search para {model_name}...\")\n",
        "\n",
        "    forecaster = ForecasterRecursive(\n",
        "        regressor=model,\n",
        "        lags=7\n",
        "    )\n",
        "\n",
        "    cv = TimeSeriesFold(\n",
        "        steps=7,\n",
        "        initial_train_size=int(len(y_train_aire) * 0.35),\n",
        "        refit=True\n",
        "    )\n",
        "\n",
        "    results_grid = grid_search_forecaster(\n",
        "        forecaster=forecaster,\n",
        "        y=y_train_aire,\n",
        "        cv=cv,\n",
        "        param_grid=param_grid,\n",
        "        metric=[mean_absolute_percent_error, root_mean_squared_error],\n",
        "        return_best=True,\n",
        "        n_jobs=-1,\n",
        "        exog=X_train_aire,  # Utilizar las variables exﾃｳgenas como X_train_aire\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    best_params = results_grid.loc[results_grid['mean_absolute_percent_error'].idxmin()]\n",
        "    print(f\"Mejores hiperparﾃ｡metros para {model_name}: {best_params}\")\n",
        "    \n",
        "    model_params = best_params['params']\n",
        "    \n",
        "    # Establecer los parﾃ｡metros ﾃｳptimos en el modelo\n",
        "    model.set_params(**model_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores parﾃ｡metros\n",
        "    forecaster.regressor = model\n",
        "\n",
        "    model_filename = \"models/\" + model_name + \"_sin_transformacion/\" + \"model_NOX.joblib\"\n",
        "    save_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            file_name=model_filename,\n",
        "            save_custom_functions=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    print(f\"Modelo guardado como {model_filename}\")\n",
        "\n",
        "    forecaster.fit(y=y_train_aire, exog=X_train_aire)\n",
        "    \n",
        "    # Obtener las predicciones con el modelo ajustado\n",
        "    predicciones = forecaster.predict(steps=len(X_test_aire), exog=predictions_df_test[model_name])\n",
        "\n",
        "    # Graficar predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    y_test_aire.plot(ax=ax, label='Real')\n",
        "    predicciones.plot(ax=ax, label='Predicciones')\n",
        "    ax.legend()\n",
        "    plt.title(f'Predicciones con el mejor modelo ({model_name})')\n",
        "    plt.show()\n",
        "\n",
        "    # Calcular mﾃｩtricas de error\n",
        "    mape = mean_absolute_percentage_error(y_test_aire, predicciones)\n",
        "    rmse = root_mean_squared_error(y_test_aire, predicciones)\n",
        "\n",
        "    print(f\"MAPE para {model_name}: {mape:.2%}\")\n",
        "    print(f\"RMSE para {model_name}: {rmse:.4f}\")\n",
        "\n",
        "    results_models[model_name] = {\n",
        "        \"mape\": mape,\n",
        "        \"rmse\": rmse,\n",
        "        \"predicciones\": predicciones\n",
        "    }\n",
        "\n",
        "# Mostrar resultados finales de todos los modelos\n",
        "for model_name, model_results in results_models.items():\n",
        "    print(f\"\\nResultados para {model_name}:\")\n",
        "    print(f\"MAPE: {model_results['mape']:.2%}\")\n",
        "    print(f\"RMSE: {model_results['rmse']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Predicciﾃｳn real sin transformaciﾃｳn con variables de aire"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Por falta de tiempo solo haremos predicciones reales usando variables de aire y sin transformaciﾃｳn, sin embargo, el proceso es el mismo para las demﾃ｡s predicciones con transformaciﾃｳn o sin ella, simplemente hay que cambiar el dataset y la ruta de los modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predicciﾃｳn real: GradientBoosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO_dft = df_concatenado_diario[\"NO\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO_lagged = create_lagged_features(NO_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO')\n",
        "NO_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO_lagged.drop(columns='NO')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO_lagged['NO']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_NO.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO = model.predict(steps=7)\n",
        "print(predictions_NO)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO2_dft = df_concatenado_diario[\"NO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO2_lagged = create_lagged_features(NO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO2')\n",
        "NO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO2_lagged.drop(columns='NO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO2_lagged['NO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_NO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO2 = model.predict(steps=7)\n",
        "print(predictions_NO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SO2_dft = df_concatenado_diario[\"SO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "SO2_lagged = create_lagged_features(SO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='SO2')\n",
        "SO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = SO2_lagged.drop(columns='SO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = SO2_lagged['SO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_SO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_SO2 = model.predict(steps=7)\n",
        "print(predictions_SO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "SO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_SO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones SO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PM10_dft = df_concatenado_diario[\"PM10\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "PM10_lagged = create_lagged_features(PM10_dft, lags=range(1, 8), horizons=range(1,1), target_column='PM10')\n",
        "PM10_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = PM10_lagged.drop(columns='PM10')  # Caracterﾃｭsticas (lags)\n",
        "y_train = PM10_lagged['PM10']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_PM10.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_PM10 = model.predict(steps=7)\n",
        "print(predictions_PM10)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "PM10_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_PM10.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones PM10')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "O3_dft = df_concatenado_diario[\"O3\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "O3_lagged = create_lagged_features(O3_dft, lags=range(1, 8), horizons=range(1,1), target_column='O3')\n",
        "O3_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = O3_lagged.drop(columns='O3')  # Caracterﾃｭsticas (lags)\n",
        "y_train = O3_lagged['O3']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_O3.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_O3 = model.predict(steps=7)\n",
        "print(predictions_O3)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "O3_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_O3.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones O3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'NO': predictions_NO,\n",
        "    'NO2': predictions_NO2,\n",
        "    'SO2': predictions_NO2,\n",
        "    'PM10': predictions_PM10,\n",
        "    'O3': predictions_O3\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df # Visualizamos el nuevo dataset predicho para los siguiente 7 dﾃｭas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['NOX','TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_NOX.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_df_concatenado_aire_diario, exog=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = model.predict(steps=7, exog=predictions_df)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predicciﾃｳn real: RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO_dft = df_concatenado_diario[\"NO\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO_lagged = create_lagged_features(NO_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO')\n",
        "NO_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO_lagged.drop(columns='NO')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO_lagged['NO']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_NO.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO = model.predict(steps=7)\n",
        "print(predictions_NO)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO2_dft = df_concatenado_diario[\"NO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO2_lagged = create_lagged_features(NO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO2')\n",
        "NO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO2_lagged.drop(columns='NO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO2_lagged['NO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_NO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO2 = model.predict(steps=7)\n",
        "print(predictions_NO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SO2_dft = df_concatenado_diario[\"SO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "SO2_lagged = create_lagged_features(SO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='SO2')\n",
        "SO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = SO2_lagged.drop(columns='SO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = SO2_lagged['SO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_SO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_SO2 = model.predict(steps=7)\n",
        "print(predictions_SO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "SO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_SO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones SO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PM10_dft = df_concatenado_diario[\"PM10\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "PM10_lagged = create_lagged_features(PM10_dft, lags=range(1, 8), horizons=range(1,1), target_column='PM10')\n",
        "PM10_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = PM10_lagged.drop(columns='PM10')  # Caracterﾃｭsticas (lags)\n",
        "y_train = PM10_lagged['PM10']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_PM10.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_PM10 = model.predict(steps=7)\n",
        "print(predictions_PM10)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "PM10_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_PM10.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones PM10')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "O3_dft = df_concatenado_diario[\"O3\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "O3_lagged = create_lagged_features(O3_dft, lags=range(1, 8), horizons=range(1,1), target_column='O3')\n",
        "O3_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = O3_lagged.drop(columns='O3')  # Caracterﾃｭsticas (lags)\n",
        "y_train = O3_lagged['O3']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_O3.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_O3 = model.predict(steps=7)\n",
        "print(predictions_O3)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "O3_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_O3.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones O3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'NO': predictions_NO,\n",
        "    'NO2': predictions_NO2,\n",
        "    'SO2': predictions_NO2,\n",
        "    'PM10': predictions_PM10,\n",
        "    'O3': predictions_O3\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df # Visualizamos el nuevo dataset predicho para los siguiente 7 dﾃｭas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['NOX','TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_NOX.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_df_concatenado_aire_diario, exog=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = model.predict(steps=7, exog=predictions_df)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predicciﾃｳn real: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO_dft = df_concatenado_diario[\"NO\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO_lagged = create_lagged_features(NO_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO')\n",
        "NO_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO_lagged.drop(columns='NO')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO_lagged['NO']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_NO.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO = model.predict(steps=7)\n",
        "print(predictions_NO)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO2_dft = df_concatenado_diario[\"NO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO2_lagged = create_lagged_features(NO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO2')\n",
        "NO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO2_lagged.drop(columns='NO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO2_lagged['NO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_NO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO2 = model.predict(steps=7)\n",
        "print(predictions_NO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SO2_dft = df_concatenado_diario[\"SO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "SO2_lagged = create_lagged_features(SO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='SO2')\n",
        "SO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = SO2_lagged.drop(columns='SO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = SO2_lagged['SO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_SO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_SO2 = model.predict(steps=7)\n",
        "print(predictions_SO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "SO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_SO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones SO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PM10_dft = df_concatenado_diario[\"PM10\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "PM10_lagged = create_lagged_features(PM10_dft, lags=range(1, 8), horizons=range(1,1), target_column='PM10')\n",
        "PM10_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = PM10_lagged.drop(columns='PM10')  # Caracterﾃｭsticas (lags)\n",
        "y_train = PM10_lagged['PM10']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_PM10.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_PM10 = model.predict(steps=7)\n",
        "print(predictions_PM10)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "PM10_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_PM10.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones PM10')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "O3_dft = df_concatenado_diario[\"O3\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "O3_lagged = create_lagged_features(O3_dft, lags=range(1, 8), horizons=range(1,1), target_column='O3')\n",
        "O3_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = O3_lagged.drop(columns='O3')  # Caracterﾃｭsticas (lags)\n",
        "y_train = O3_lagged['O3']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_O3.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_O3 = model.predict(steps=7)\n",
        "print(predictions_O3)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "O3_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_O3.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones O3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'NO': predictions_NO,\n",
        "    'NO2': predictions_NO2,\n",
        "    'SO2': predictions_NO2,\n",
        "    'PM10': predictions_PM10,\n",
        "    'O3': predictions_O3\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df # Visualizamos el nuevo dataset predicho para los siguiente 7 dﾃｭas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['NOX','TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_NOX.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_df_concatenado_aire_diario, exog=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = model.predict(steps=7, exog=predictions_df)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Con todas las variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Ahora vamos a hacer lo mismo pero con las variables que hemos hecho en la secciﾃｳn anterior pero esta vez con todas las variables meteorolﾃｳgicas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_directory = \"models/\"\n",
        "tipo_de_df = \"sin_transformacion\" #Cambiar con o sin transformaciﾃｳn dependiendo\n",
        "\n",
        "# Hiperparﾃ｡metros comunes\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "# Lista de regresores\n",
        "regressors = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "# Proceso para cada variable y cada regresor\n",
        "results = {}\n",
        "variables = [\"TMP\", \"HR\", \"DD\", \"PRB\", \"VV\", \"RS\"]\n",
        "\n",
        "for var in variables:\n",
        "    for reg_name, regressor in regressors.items():\n",
        "        print(f\"Procesando {var} con {reg_name}...\")\n",
        "        predictions, mape, rmse = process_variable(\n",
        "            variable_name=var,\n",
        "            df=df_concatenado_diario,\n",
        "            test_size=len(df_test),\n",
        "            param_grid=param_grid,\n",
        "            regressor=regressor,\n",
        "            model_dir=model_directory + reg_name + \"_\" + tipo_de_df,\n",
        "            train_mode=True  # Cambiar a False si deseas cargar modelos preentrenados\n",
        "        )\n",
        "        results[(var, reg_name)] = {\"predictions\": predictions, \"mape\": mape, \"rmse\": rmse}\n",
        "        print(f\"Resultados para {var} con {reg_name}: MAPE={mape:.2%}, RMSE={rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_directory = \"models/\"\n",
        "tipo_de_df = \"sin_transformacion\" #Cambiar con o sin transformaciﾃｳn dependiendo\n",
        "\n",
        "# Hiperparﾃ｡metros comunes\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "# Lista de regresores\n",
        "regressors = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "# Proceso para cada variable y cada regresor\n",
        "results = {}\n",
        "variables = [\"NO\", \"NO2\", \"SO2\", \"PM10\", \"O3\", \"TMP\", \"HR\", \"DD\", \"PRB\", \"VV\", \"RS\"]\n",
        "\n",
        "for var in variables:\n",
        "    for reg_name, regressor in regressors.items():\n",
        "        print(f\"Procesando {var} con {reg_name}...\")\n",
        "        predictions, mape, rmse = process_variable(\n",
        "            variable_name=var,\n",
        "            df=df_concatenado_diario,\n",
        "            test_size=len(df_test),\n",
        "            param_grid=param_grid,\n",
        "            regressor=regressor,\n",
        "            model_dir=model_directory + reg_name + \"_\" + tipo_de_df,\n",
        "            train_mode=False  # Cambiar a False si deseas cargar modelos preentrenados\n",
        "        )\n",
        "        results[(var, reg_name)] = {\"predictions\": predictions, \"mape\": mape, \"rmse\": rmse}\n",
        "        print(f\"Resultados para {var} con {reg_name}: MAPE={mape:.2%}, RMSE={rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df_test = {}\n",
        "\n",
        "for reg_name in regressors.keys():\n",
        "    print(f\"\\nProcesando predicciones para el modelo {reg_name}...\\n\")\n",
        "    \n",
        "    prediction_frames = []\n",
        "    \n",
        "    for var in variables:\n",
        "        predictions_df = results[(var, reg_name)]['predictions']\n",
        "        \n",
        "        if 'pred' in predictions_df.columns:\n",
        "            predictions_df = predictions_df[['pred']]  \n",
        "            predictions_df.columns = [var] \n",
        "        else:\n",
        "            print(f\"Advertencia: No se encontrﾃｳ la columna 'pred' en las predicciones para {var} con {reg_name}.\")\n",
        "            continue\n",
        "        \n",
        "        prediction_frames.append(predictions_df)\n",
        "    \n",
        "    model_df = pd.concat(prediction_frames, axis=1)\n",
        "    \n",
        "    predictions_df_test[reg_name] = model_df\n",
        "    \n",
        "    print(f\"Predicciones para el modelo {reg_name}:\")\n",
        "    print(model_df.head())  # Mostrar las primeras filas del DataFrame generado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 250],\n",
        "    'max_depth': [3, 5, 8],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "results_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nRealizando grid search para {model_name}...\")\n",
        "\n",
        "    forecaster = ForecasterRecursive(\n",
        "        regressor=model,\n",
        "        lags=7\n",
        "    )\n",
        "\n",
        "    cv = TimeSeriesFold(\n",
        "        steps=7,\n",
        "        initial_train_size=int(len(y_train) * 0.35),\n",
        "        refit=True\n",
        "    )\n",
        "\n",
        "    results_grid = grid_search_forecaster(\n",
        "        forecaster=forecaster,\n",
        "        y=y_train,\n",
        "        cv=cv,\n",
        "        param_grid=param_grid,\n",
        "        metric=[mean_absolute_percent_error, root_mean_squared_error],\n",
        "        return_best=True,\n",
        "        n_jobs=-1,\n",
        "        exog=X_train,  # Utilizar las variables exﾃｳgenas como X_train_aire\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    best_params = results_grid.loc[results_grid['mean_absolute_percent_error'].idxmin()]\n",
        "    print(f\"Mejores hiperparﾃ｡metros para {model_name}: {best_params}\")\n",
        "    \n",
        "    model_params = best_params['params']\n",
        "    \n",
        "    # Establecer los parﾃ｡metros ﾃｳptimos en el modelo\n",
        "    model.set_params(**model_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores parﾃ｡metros\n",
        "    forecaster.regressor = model\n",
        "\n",
        "    model_filename = \"models/\" + model_name + \"_sin_transformacion/\" + \"model_NOX_todo.joblib\"\n",
        "    save_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            file_name=model_filename,\n",
        "            save_custom_functions=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    print(f\"Modelo guardado como {model_filename}\")\n",
        "\n",
        "    forecaster.fit(y=y_train, exog=X_train)\n",
        "    \n",
        "    # Obtener las predicciones con el modelo ajustado\n",
        "    predicciones = forecaster.predict(steps=len(X_test_aire), exog=predictions_df_test[model_name])\n",
        "\n",
        "    # Graficar predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    y_test_aire.plot(ax=ax, label='Real')\n",
        "    predicciones.plot(ax=ax, label='Predicciones')\n",
        "    ax.legend()\n",
        "    plt.title(f'Predicciones con el mejor modelo ({model_name})')\n",
        "    plt.show()\n",
        "\n",
        "    # Calcular mﾃｩtricas de error\n",
        "    mape = mean_absolute_percentage_error(y_test_aire, predicciones)\n",
        "    rmse = root_mean_squared_error(y_test_aire, predicciones)\n",
        "\n",
        "    print(f\"MAPE para {model_name}: {mape:.2%}\")\n",
        "    print(f\"RMSE para {model_name}: {rmse:.4f}\")\n",
        "\n",
        "    results_models[model_name] = {\n",
        "        \"mape\": mape,\n",
        "        \"rmse\": rmse,\n",
        "        \"predicciones\": predicciones\n",
        "    }\n",
        "\n",
        "# Mostrar resultados finales de todos los modelos\n",
        "for model_name, model_results in results_models.items():\n",
        "    print(f\"\\nResultados para {model_name}:\")\n",
        "    print(f\"MAPE: {model_results['mape']:.2%}\")\n",
        "    print(f\"RMSE: {model_results['rmse']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "VV_dft = df_concatenado_diario[\"VV\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "VV_lagged = create_lagged_features(VV_dft, lags=range(1, 8), horizons=range(1,1), target_column='VV')\n",
        "VV_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = VV_lagged.drop(columns='VV')  # Caracterﾃｭsticas (lags)\n",
        "y_train = VV_lagged['VV']               # Objetivo (VV)\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_VV.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_VV = model.predict(steps=7)\n",
        "print(predictions_VV)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "VV_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_VV.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones VV')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RS_dft = df_concatenado_diario[\"RS\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "RS_lagged = create_lagged_features(RS_dft, lags=range(1, 8), horizons=range(1,1), target_column='RS')\n",
        "RS_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = RS_lagged.drop(columns='RS')  # Caracterﾃｭsticas (lags)\n",
        "y_train = RS_lagged['RS']               # Objetivo (RS)\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_RS.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_RS = model.predict(steps=7)\n",
        "print(predictions_RS)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "RS_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_RS.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones RS')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'NO': predictions_NO,\n",
        "    'NO2': predictions_NO2,\n",
        "    'SO2': predictions_NO2,\n",
        "    'PM10': predictions_PM10,\n",
        "    'O3': predictions_O3\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df # Visualizamos el nuevo dataset predicho para los siguiente 7 dﾃｭas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['NOX','TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "model = load_forecaster('./models/GradientBoosting_sin_transformacion/model_NOX.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_df_concatenado_aire_diario, exog=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = model.predict(steps=7, exog=predictions_df)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predicciﾃｳn real: RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO_dft = df_concatenado_diario[\"NO\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO_lagged = create_lagged_features(NO_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO')\n",
        "NO_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO_lagged.drop(columns='NO')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO_lagged['NO']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_NO.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO = model.predict(steps=7)\n",
        "print(predictions_NO)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO2_dft = df_concatenado_diario[\"NO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO2_lagged = create_lagged_features(NO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO2')\n",
        "NO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO2_lagged.drop(columns='NO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO2_lagged['NO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_NO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO2 = model.predict(steps=7)\n",
        "print(predictions_NO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SO2_dft = df_concatenado_diario[\"SO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "SO2_lagged = create_lagged_features(SO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='SO2')\n",
        "SO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = SO2_lagged.drop(columns='SO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = SO2_lagged['SO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_SO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_SO2 = model.predict(steps=7)\n",
        "print(predictions_SO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "SO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_SO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones SO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PM10_dft = df_concatenado_diario[\"PM10\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "PM10_lagged = create_lagged_features(PM10_dft, lags=range(1, 8), horizons=range(1,1), target_column='PM10')\n",
        "PM10_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = PM10_lagged.drop(columns='PM10')  # Caracterﾃｭsticas (lags)\n",
        "y_train = PM10_lagged['PM10']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_PM10.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_PM10 = model.predict(steps=7)\n",
        "print(predictions_PM10)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "PM10_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_PM10.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones PM10')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "O3_dft = df_concatenado_diario[\"O3\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "O3_lagged = create_lagged_features(O3_dft, lags=range(1, 8), horizons=range(1,1), target_column='O3')\n",
        "O3_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = O3_lagged.drop(columns='O3')  # Caracterﾃｭsticas (lags)\n",
        "y_train = O3_lagged['O3']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_O3.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_O3 = model.predict(steps=7)\n",
        "print(predictions_O3)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "O3_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_O3.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones O3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'NO': predictions_NO,\n",
        "    'NO2': predictions_NO2,\n",
        "    'SO2': predictions_NO2,\n",
        "    'PM10': predictions_PM10,\n",
        "    'O3': predictions_O3\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df # Visualizamos el nuevo dataset predicho para los siguiente 7 dﾃｭas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['NOX','TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "model = load_forecaster('./models/RandomForest_sin_transformacion/model_NOX.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_df_concatenado_aire_diario, exog=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = model.predict(steps=7, exog=predictions_df)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Predicciﾃｳn real: XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO_dft = df_concatenado_diario[\"NO\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO_lagged = create_lagged_features(NO_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO')\n",
        "NO_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO_lagged.drop(columns='NO')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO_lagged['NO']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_NO.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO = model.predict(steps=7)\n",
        "print(predictions_NO)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NO2_dft = df_concatenado_diario[\"NO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "NO2_lagged = create_lagged_features(NO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='NO2')\n",
        "NO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = NO2_lagged.drop(columns='NO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = NO2_lagged['NO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_NO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_NO2 = model.predict(steps=7)\n",
        "print(predictions_NO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "NO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_NO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "SO2_dft = df_concatenado_diario[\"SO2\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "SO2_lagged = create_lagged_features(SO2_dft, lags=range(1, 8), horizons=range(1,1), target_column='SO2')\n",
        "SO2_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = SO2_lagged.drop(columns='SO2')  # Caracterﾃｭsticas (lags)\n",
        "y_train = SO2_lagged['SO2']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_SO2.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_SO2 = model.predict(steps=7)\n",
        "print(predictions_SO2)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "SO2_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_SO2.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones SO2')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "PM10_dft = df_concatenado_diario[\"PM10\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "PM10_lagged = create_lagged_features(PM10_dft, lags=range(1, 8), horizons=range(1,1), target_column='PM10')\n",
        "PM10_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = PM10_lagged.drop(columns='PM10')  # Caracterﾃｭsticas (lags)\n",
        "y_train = PM10_lagged['PM10']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_PM10.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_PM10 = model.predict(steps=7)\n",
        "print(predictions_PM10)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "PM10_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_PM10.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones PM10')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "O3_dft = df_concatenado_diario[\"O3\"].to_frame()  # Convertir la Serie a DataFrame\n",
        "\n",
        "# Preparar los datos para entrenar el modelo\n",
        "O3_lagged = create_lagged_features(O3_dft, lags=range(1, 8), horizons=range(1,1), target_column='O3')\n",
        "O3_lagged.dropna(inplace=True)\n",
        "\n",
        "X_train = O3_lagged.drop(columns='O3')  # Caracterﾃｭsticas (lags)\n",
        "y_train = O3_lagged['O3']               # Objetivo (NO)\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_O3.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_train, exog=None)\n",
        "\n",
        "# Predicciﾃｳn de los prﾃｳximos 7 dﾃｭas\n",
        "predictions_O3 = model.predict(steps=7)\n",
        "print(predictions_O3)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "O3_dft[-7:].plot(ax=ax, label='Test (real)')\n",
        "predictions_O3.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones O3')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df = pd.DataFrame({\n",
        "    'NO': predictions_NO,\n",
        "    'NO2': predictions_NO2,\n",
        "    'SO2': predictions_NO2,\n",
        "    'PM10': predictions_PM10,\n",
        "    'O3': predictions_O3\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df # Visualizamos el nuevo dataset predicho para los siguiente 7 dﾃｭas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['NOX','TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "model = load_forecaster('./models/XGBoost_sin_transformacion/model_NOX.joblib') # Cargamos el modelo\n",
        "\n",
        "# Entrenar el modelo\n",
        "model.fit(y=y_df_concatenado_aire_diario, exog=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = model.predict(steps=7, exog=predictions_df)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### CON TRANSFORMACION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Solo variables aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "porcentaje_entrenamiento = 0.9\n",
        "df_train_transformed = df_concatenado_diario_transformed[:int(len(df_concatenado_diario_transformed) * porcentaje_entrenamiento)]\n",
        "df_test_transformed = df_concatenado_diario_transformed[int(len(df_concatenado_diario_transformed) * porcentaje_entrenamiento):]\n",
        "\n",
        "print(f'Con el porcentaje de %.2f tenemos:' %porcentaje_entrenamiento)\n",
        "print(f'Tamaﾃｱo del conjunto de training es %i' %len(df_train))\n",
        "print(f'Tamaﾃｱo del conjunto de test es %i' %len(df_test))\n",
        "print(f'El conjunto de training va de {min(df_train.index)} y {max(df_train.index)}')\n",
        "print(f'El conjunto de test va de {min(df_test.index)} y {max(df_test.index)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_transformed = df_train_transformed.drop(columns = 'NOX')\n",
        "y_train_transformed = df_train_transformed['NOX']\n",
        "\n",
        "X_test_transformed = df_test_transformed.drop(columns='NOX')\n",
        "y_test_transformed = df_test_transformed['NOX']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "columnas_variables_metereologicas = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS']\n",
        "\n",
        "X_train_aire_transformed = X_train_transformed.drop(columns=columnas_variables_metereologicas)\n",
        "y_train_aire_transformed = df_train_transformed['NOX']\n",
        "\n",
        "X_test_aire_transformed = X_test_transformed.drop(columns=columnas_variables_metereologicas)\n",
        "y_test_aire_transformed = df_test_transformed['NOX']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_directory = \"models/\"\n",
        "tipo_de_df = \"con_transformacion\" #Cambiar con o sin transformaciﾃｳn dependiendo\n",
        "\n",
        "# Hiperparﾃ｡metros comunes\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "# Lista de regresores\n",
        "regressors = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "# Proceso para cada variable y cada regresor\n",
        "results = {}\n",
        "variables = [\"NO\", \"NO2\", \"SO2\", \"PM10\", \"O3\"]\n",
        "\n",
        "for var in variables:\n",
        "    for reg_name, regressor in regressors.items():\n",
        "        print(f\"Procesando {var} con {reg_name}...\")\n",
        "        predictions, mape, rmse = process_variable(\n",
        "            variable_name=var,\n",
        "            df=df_concatenado_aire_diario_transformed,\n",
        "            test_size=len(df_test),\n",
        "            param_grid=param_grid,\n",
        "            regressor=regressor,\n",
        "            model_dir=model_directory + reg_name + \"_\" + tipo_de_df,\n",
        "            train_mode=True  # Cambiar a False si deseas cargar modelos preentrenados\n",
        "        )\n",
        "        results[(var, reg_name)] = {\"predictions\": predictions, \"mape\": mape, \"rmse\": rmse}\n",
        "        print(f\"Resultados para {var} con {reg_name}: MAPE={mape:.2%}, RMSE={rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df_test = {}\n",
        "\n",
        "for reg_name in regressors.keys():\n",
        "    print(f\"\\nProcesando predicciones para el modelo {reg_name}...\\n\")\n",
        "    \n",
        "    prediction_frames = []\n",
        "    \n",
        "    for var in variables:\n",
        "        predictions_df = results[(var, reg_name)]['predictions']\n",
        "        \n",
        "        if 'pred' in predictions_df.columns:\n",
        "            predictions_df = predictions_df[['pred']]  \n",
        "            predictions_df.columns = [var] \n",
        "        else:\n",
        "            print(f\"Advertencia: No se encontrﾃｳ la columna 'pred' en las predicciones para {var} con {reg_name}.\")\n",
        "            continue\n",
        "        \n",
        "        prediction_frames.append(predictions_df)\n",
        "    \n",
        "    model_df = pd.concat(prediction_frames, axis=1)\n",
        "    \n",
        "    predictions_df_test[reg_name] = model_df\n",
        "    \n",
        "    print(f\"Predicciones para el modelo {reg_name}:\")\n",
        "    print(model_df.head())  # Mostrar las primeras filas del DataFrame generado\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 250],\n",
        "    'max_depth': [3, 5, 8],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "results_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nRealizando grid search para {model_name}...\")\n",
        "\n",
        "    forecaster = ForecasterRecursive(\n",
        "        regressor=model,\n",
        "        lags=7\n",
        "    )\n",
        "\n",
        "    cv = TimeSeriesFold(\n",
        "        steps=7,\n",
        "        initial_train_size=int(len(y_train_aire_transformed) * 0.35),\n",
        "        refit=True\n",
        "    )\n",
        "\n",
        "    results_grid = grid_search_forecaster(\n",
        "        forecaster=forecaster,\n",
        "        y=y_train_aire_transformed,\n",
        "        cv=cv,\n",
        "        param_grid=param_grid,\n",
        "        metric=[mean_absolute_percent_error, root_mean_squared_error],\n",
        "        return_best=True,\n",
        "        n_jobs=-1,\n",
        "        exog=X_train_aire_transformed,  # Utilizar las variables exﾃｳgenas como X_train_aire\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    best_params = results_grid.loc[results_grid['mean_absolute_percent_error'].idxmin()]\n",
        "    print(f\"Mejores hiperparﾃ｡metros para {model_name}: {best_params}\")\n",
        "    \n",
        "    model_params = best_params['params']\n",
        "    \n",
        "    # Establecer los parﾃ｡metros ﾃｳptimos en el modelo\n",
        "    model.set_params(**model_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores parﾃ｡metros\n",
        "    forecaster.regressor = model\n",
        "\n",
        "    model_filename = \"models/\" + model_name + \"_con_transformacion/\" + \"model_NOX.joblib\"\n",
        "    save_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            file_name=model_filename,\n",
        "            save_custom_functions=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    print(f\"Modelo guardado como {model_filename}\")\n",
        "\n",
        "    forecaster.fit(y=y_train_aire_transformed, exog=X_train_aire_transformed)\n",
        "    \n",
        "    # Obtener las predicciones con el modelo ajustado\n",
        "    predicciones = forecaster.predict(steps=len(X_test_aire_transformed), exog=predictions_df_test[model_name])\n",
        "\n",
        "    # Graficar predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    y_test_aire.plot(ax=ax, label='Real')\n",
        "    predicciones.plot(ax=ax, label='Predicciones')\n",
        "    ax.legend()\n",
        "    plt.title(f'Predicciones con el mejor modelo ({model_name})')\n",
        "    plt.show()\n",
        "\n",
        "    # Calcular mﾃｩtricas de error\n",
        "    mape = mean_absolute_percentage_error(y_test_aire, predicciones)\n",
        "    rmse = root_mean_squared_error(y_test_aire, predicciones)\n",
        "\n",
        "    print(f\"MAPE para {model_name}: {mape:.2%}\")\n",
        "    print(f\"RMSE para {model_name}: {rmse:.4f}\")\n",
        "\n",
        "    results_models[model_name] = {\n",
        "        \"mape\": mape,\n",
        "        \"rmse\": rmse,\n",
        "        \"predicciones\": predicciones\n",
        "    }\n",
        "\n",
        "# Mostrar resultados finales de todos los modelos\n",
        "for model_name, model_results in results_models.items():\n",
        "    print(f\"\\nResultados para {model_name}:\")\n",
        "    print(f\"MAPE: {model_results['mape']:.2%}\")\n",
        "    print(f\"RMSE: {model_results['rmse']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Con todas las variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_directory = \"models/\"\n",
        "tipo_de_df = \"con_transformacion\" #Cambiar con o sin transformaciﾃｳn dependiendo\n",
        "\n",
        "# Hiperparﾃ｡metros comunes\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "# Lista de regresores\n",
        "regressors = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "# Proceso para cada variable y cada regresor\n",
        "results = {}\n",
        "variables = [\"TMP\", \"HR\", \"DD\",\"PRB\", \"VV\", \"RS\"]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for var in variables:\n",
        "    for reg_name, regressor in regressors.items():\n",
        "        print(f\"Procesando {var} con {reg_name}...\")\n",
        "        predictions, mape, rmse = process_variable(\n",
        "            variable_name=var,\n",
        "            df=df_concatenado_diario_transformed,\n",
        "            test_size=len(df_test),\n",
        "            param_grid=param_grid,\n",
        "            regressor=regressor,\n",
        "            model_dir=model_directory + reg_name + \"_\" + tipo_de_df,\n",
        "            train_mode=True  # Cambiar a False si deseas cargar modelos preentrenados\n",
        "        )\n",
        "        results[(var, reg_name)] = {\"predictions\": predictions, \"mape\": mape, \"rmse\": rmse}\n",
        "        print(f\"Resultados para {var} con {reg_name}: MAPE={mape:.2%}, RMSE={rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_directory = \"models/\"\n",
        "tipo_de_df = \"con_transformacion\" #Cambiar con o sin transformaciﾃｳn dependiendo\n",
        "\n",
        "# Hiperparﾃ｡metros comunes\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "\n",
        "# Lista de regresores\n",
        "regressors = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "# Proceso para cada variable y cada regresor\n",
        "results = {}\n",
        "variables = [\"PRB\", \"NO\", \"NO2\", \"SO2\", \"PM10\", \"O3\", \"TMP\", \"HR\", \"DD\", \"VV\", \"RS\"]\n",
        "df_concatenado_diario_transformed[\"PRB\"] = df_concatenado_diario[\"PRB\"]\n",
        "for var in variables:\n",
        "    for reg_name, regressor in regressors.items():\n",
        "        print(f\"Procesando {var} con {reg_name}...\")\n",
        "        predictions, mape, rmse = process_variable(\n",
        "            variable_name=var,\n",
        "            df=df_concatenado_diario_transformed,\n",
        "            test_size=len(df_test),\n",
        "            param_grid=param_grid,\n",
        "            regressor=regressor,\n",
        "            model_dir=model_directory + reg_name + \"_\" + tipo_de_df,\n",
        "            train_mode=False  # Cambiar a False si deseas cargar modelos preentrenados\n",
        "        )\n",
        "        results[(var, reg_name)] = {\"predictions\": predictions, \"mape\": mape, \"rmse\": rmse}\n",
        "        print(f\"Resultados para {var} con {reg_name}: MAPE={mape:.2%}, RMSE={rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "predictions_df_test = {}\n",
        "\n",
        "for reg_name in regressors.keys():\n",
        "    print(f\"\\nProcesando predicciones para el modelo {reg_name}...\\n\")\n",
        "    \n",
        "    prediction_frames = []\n",
        "    \n",
        "    for var in variables:\n",
        "        predictions_df = results[(var, reg_name)]['predictions']\n",
        "        \n",
        "        if 'pred' in predictions_df.columns:\n",
        "            predictions_df = predictions_df[['pred']]  \n",
        "            predictions_df.columns = [var] \n",
        "        else:\n",
        "            print(f\"Advertencia: No se encontrﾃｳ la columna 'pred' en las predicciones para {var} con {reg_name}.\")\n",
        "            continue\n",
        "        \n",
        "        prediction_frames.append(predictions_df)\n",
        "    \n",
        "    model_df = pd.concat(prediction_frames, axis=1)\n",
        "    \n",
        "    predictions_df_test[reg_name] = model_df\n",
        "    \n",
        "    print(f\"Predicciones para el modelo {reg_name}:\")\n",
        "    print(model_df.head())  # Mostrar las primeras filas del DataFrame generado"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"RandomForest\": RandomForestRegressor(random_state=semilla),\n",
        "    \"GradientBoosting\": GradientBoostingRegressor(random_state=semilla),\n",
        "    \"XGBoost\": XGBRegressor(random_state=semilla)\n",
        "}\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 150, 250],\n",
        "    'max_depth': [3, 5, 8],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "results_models = {}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    print(f\"\\nRealizando grid search para {model_name}...\")\n",
        "\n",
        "    forecaster = ForecasterRecursive(\n",
        "        regressor=model,\n",
        "        lags=7\n",
        "    )\n",
        "\n",
        "    cv = TimeSeriesFold(\n",
        "        steps=7,\n",
        "        initial_train_size=int(len(y_train) * 0.35),\n",
        "        refit=True\n",
        "    )\n",
        "\n",
        "    X_train_transformed['PRB'] = df_train['PRB']\n",
        "    X_test_transformed['PRB'] = df_test['PRB']\n",
        "\n",
        "\n",
        "\n",
        "    results_grid = grid_search_forecaster(\n",
        "        forecaster=forecaster,\n",
        "        y=y_train,\n",
        "        cv=cv,\n",
        "        param_grid=param_grid,\n",
        "        metric=[mean_absolute_percent_error, root_mean_squared_error],\n",
        "        return_best=True,\n",
        "        n_jobs=-1,\n",
        "        exog=X_train_transformed,  # Utilizar las variables exﾃｳgenas como X_train_aire\n",
        "        verbose=True\n",
        "    )\n",
        "\n",
        "    best_params = results_grid.loc[results_grid['mean_absolute_percent_error'].idxmin()]\n",
        "    print(f\"Mejores hiperparﾃ｡metros para {model_name}: {best_params}\")\n",
        "    \n",
        "    model_params = best_params['params']\n",
        "    \n",
        "    # Establecer los parﾃ｡metros ﾃｳptimos en el modelo\n",
        "    model.set_params(**model_params)\n",
        "\n",
        "    # Ajustar el modelo con los mejores parﾃ｡metros\n",
        "    forecaster.regressor = model\n",
        "\n",
        "    model_filename = \"models/\" + model_name + \"_con_transformacion/\" + \"model_NOX_todo.joblib\"\n",
        "    save_forecaster(\n",
        "            forecaster=forecaster,\n",
        "            file_name=model_filename,\n",
        "            save_custom_functions=True,\n",
        "            verbose=True\n",
        "        )\n",
        "    print(f\"Modelo guardado como {model_filename}\")\n",
        "\n",
        "    forecaster.fit(y=y_train_transformed, exog=X_train_transformed)\n",
        "    \n",
        "    # Obtener las predicciones con el modelo ajustado\n",
        "    predicciones = forecaster.predict(steps=len(X_test_aire_transformed), exog=predictions_df_test[model_name])\n",
        "\n",
        "    # Graficar predicciones vs valores reales\n",
        "    fig, ax = plt.subplots(figsize=(6, 3))\n",
        "    y_test_aire.plot(ax=ax, label='Real')\n",
        "    predicciones.plot(ax=ax, label='Predicciones')\n",
        "    ax.legend()\n",
        "    plt.title(f'Predicciones con el mejor modelo ({model_name})')\n",
        "    plt.show()\n",
        "\n",
        "    # Calcular mﾃｩtricas de error\n",
        "    mape = mean_absolute_percentage_error(y_test_aire, predicciones)\n",
        "    rmse = root_mean_squared_error(y_test_aire, predicciones)\n",
        "\n",
        "    print(f\"MAPE para {model_name}: {mape:.2%}\")\n",
        "    print(f\"RMSE para {model_name}: {rmse:.4f}\")\n",
        "\n",
        "    results_models[model_name] = {\n",
        "        \"mape\": mape,\n",
        "        \"rmse\": rmse,\n",
        "        \"predicciones\": predicciones\n",
        "    }\n",
        "\n",
        "# Mostrar resultados finales de todos los modelos\n",
        "for model_name, model_results in results_models.items():\n",
        "    print(f\"\\nResultados para {model_name}:\")\n",
        "    print(f\"MAPE: {model_results['mape']:.2%}\")\n",
        "    print(f\"RMSE: {model_results['rmse']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Predicciﾃｳn directa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ForecasterDirectMultiVariate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se va a realizar el ejercicio mediante ForecasterDirectMultiVariate. Esta  estrategia resulta interesante debido a que lo que realiza es una predicciﾃｳn directa  y multivariable. Por tanto, predice los siguientes x dﾃｭas y lo hace teniendo en cuenta todas las series que le introduzcamos. El error dejarﾃｭa de ser acumulativo y permite manejar las relaciones entre varias variables aunque tiene mayor carga computacional que la predicciﾃｳn recursiva.\n",
        "\n",
        "Se va a utilizar esta estrategia para cada tﾃｩcnica (GradientBoosting, RandomForest y XGBoost). Por cada uno de ellos se realizan cuatro predicciones:\n",
        "\n",
        "- **Test con variables de aire:** Busca estudiar las mﾃｩtricas de los modelos de la tﾃｩcnica aplicando solo las variables de aire.\n",
        "- **Test con variables de aire y meteorolﾃｳgicas:** Busca estudiar las mﾃｩtricas de los modelos de la tﾃｩcnica aplicando todas las variables, es decir, las de aire y las meteorolﾃｳgicas.\n",
        "- **Predicciﾃｳn real con variables de aire:** Busca predecir realmente los siguientes 7 dﾃｭas aplicando solo las variables de aire.\n",
        "- **Predicciﾃｳn real con variables de aire y meteorolﾃｳgicas:** Busca predecir realmente los siguientes 7 dﾃｭas aplicando todas las variables, es decir, las de aire y las meteorolﾃｳgicas.\n",
        "\n",
        "Ademﾃ｡s, se va a realizar utilizando los datasets sin transformaciﾃｳn y con transformaciﾃｳn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ForecasterDirectMultiVariate: GradientBoosting sin transformaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting test con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_aire[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_aire[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_aire[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting test con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_aire[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_aire[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_aire[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting predicciﾃｳn real con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting predicciﾃｳn real con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ForecasterDirectMultiVariate: RandomForest sin transformaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest test con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_aire[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_aire[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_aire[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest test con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_aire[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_aire[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_aire[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest predicciﾃｳn real con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest predicciﾃｳn real con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ForecasterDirectMultiVariate: XGBoost sin transformaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost test con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_aire[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_aire[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_aire[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost test con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_aire[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_aire[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_aire[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost predicciﾃｳn real con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost predicciﾃｳn real con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ForecasterDirectMultiVariate: GradientBoosting com transformaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting test con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train_transformed .drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_transformed[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_transformed[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_transformed[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting test con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train_transformed\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_transformed[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_transformed[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_transformed[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting predicciﾃｳn real con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario_transformed.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario_transformed['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GradientBoosting predicciﾃｳn real con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario_transformed\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario_transformed['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = GradientBoostingRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ForecasterDirectMultiVariate: RandomForest con transformaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest test con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train_transformed.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_transformed[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_transformed[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_transformed[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest test con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train_transformed\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_transformed[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_transformed[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_transformed[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest predicciﾃｳn real con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario_transformed.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario_transformed['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### RandomForest predicciﾃｳn real con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario_transformed\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario_transformed['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = RandomForestRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### ForecasterDirectMultiVariate: XGBoost con transformaciﾃｳn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost test con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train_transformed.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_transformed[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_transformed[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_transformed[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost test con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_train_transformed\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=15,\n",
        "                 lags= 7\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=15)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_test_transformed[:15].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()\n",
        "\n",
        "# Calcular mﾃｩtricas de error\n",
        "mape = mean_absolute_percentage_error(y_test_transformed[:15], predicciones)\n",
        "rmse = root_mean_squared_error(y_test_transformed[:15], predicciones)\n",
        "\n",
        "print(f\"MAPE: {mape:.2%}\")\n",
        "print(f\"RMSE: {rmse:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost predicciﾃｳn real con variables de aire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario_transformed.drop(columns = ['TMP', 'HR', 'DD', 'PRB', 'VV', 'RS'])\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario_transformed['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### XGBoost predicciﾃｳn real con variables de aire y meteorolﾃｳgicas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_df_concatenado_aire_diario = df_concatenado_diario_transformed\n",
        "y_df_concatenado_aire_diario = df_concatenado_diario_transformed['NOX']\n",
        "\n",
        "\n",
        "forecaster = ForecasterDirectMultiVariate(\n",
        "                 regressor = XGBRegressor(random_state=semilla),\n",
        "                 level= 'NOX',\n",
        "                 steps=7,\n",
        "                 lags= 7,\n",
        "                 window_features= RollingFeatures(stats=['coef_variation'], window_sizes=[7])\n",
        "\n",
        "             )\n",
        "\n",
        "# Entrenar el modelo\n",
        "forecaster.fit(series=X_df_concatenado_aire_diario)\n",
        "\n",
        "# Realizar predicciones\n",
        "predicciones = forecaster.predict(steps=7)\n",
        "\n",
        "# Imprimir predicciones\n",
        "print('Predicciones para los prﾃｳximos dﾃｭas:')\n",
        "print(predicciones)\n",
        "\n",
        "# Graficar los datos reales y predicciones\n",
        "fig, ax = plt.subplots(figsize=(6, 3))\n",
        "y_df_concatenado_aire_diario[-7:].plot(ax=ax, label='Test (real)')\n",
        "predicciones.plot(ax=ax, label='Predicciones')\n",
        "ax.legend()\n",
        "plt.title('Predicciones NOX')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discusiﾃｳn de resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Predicciﾃｳn recursiva**\n",
        "\n",
        "\\begin{array}{|c|c|c|c|c|c|}\n",
        "\\hline\n",
        "\\textbf{Modelo} & \\textbf{Transformaciﾃｳn} & \\textbf{Caracterﾃｭsticas} & \\textbf{MAPE} & \\textbf{RMSE} & \\textbf{Hiperparﾃ｡metros} \\\\ \\hline\n",
        "RandomForest & Sin & Aire & 13.07\\% & 2.0226 & \\text{max\\_depth=8, min\\_samples\\_split=5, n\\_estimators=250} \\\\ \\hline\n",
        "GradientBoosting & Sin & Aire & 12.41\\% & 2.0091 & \\text{max\\_depth=5, min\\_samples\\_split=10, n\\_estimators=100} \\\\ \\hline\n",
        "XGBoost & Sin & Aire & 13.32\\% & 2.0930 & \\text{max\\_depth=5, min\\_samples\\_split=5, n\\_estimators=100} \\\\ \\hline\n",
        "RandomForest & Sin & Todo & 13.54\\% & 2.0691 & \\text{max\\_depth=8, min\\_samples\\_split=2, n\\_estimators=250} \\\\ \\hline\n",
        "GradientBoosting & Sin & Todo & 12.57\\% & 2.0267 & \\text{max\\_depth=5, min\\_samples\\_split=10, n\\_estimators=150} \\\\ \\hline\n",
        "XGBoost & Sin & Todo & 13.30\\% & 2.0894 & \\text{max\\_depth=5, min\\_samples\\_split=5, n\\_estimators=150} \\\\ \\hline\n",
        "RandomForest & Con & Aire & 11.79\\% & 1.9137 & \\text{max\\_depth=8, min\\_samples\\_split=5, n\\_estimators=250} \\\\ \\hline\n",
        "GradientBoosting & Con & Aire & 12.19\\% & 1.9806 & \\text{max\\_depth=5, min\\_samples\\_split=10, n\\_estimators=100} \\\\ \\hline\n",
        "XGBoost & Con & Aire & 12.92\\% & 2.0832 & \\text{max\\_depth=5, min\\_samples\\_split=5, n\\_estimators=100} \\\\ \\hline\n",
        "\\textbf{RandomForest} & Con & Todo & 11.61\\% & 1.9123 & \\text{max\\_depth=8, min\\_samples\\_split=2, n\\_estimators=250} \\\\ \\hline\n",
        "GradientBoosting & Con & Todo & 12.00\\% & 1.9727 & \\text{max\\_depth=5, min\\_samples\\_split=10, n\\_estimators=150} \\\\ \\hline\n",
        "XGBoost & Con & Todo & 12.91\\% & 2.0796 & \\text{max\\_depth=5, min\\_samples\\_split=5, n\\_estimators=150} \\\\ \\hline\n",
        "\\end{array}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el anﾃ｡lisis realizado, el modelo **Random Forest** destaca como el mejor en tﾃｩrminos de precisiﾃｳn al aplicar una transformaciﾃｳn a los datos y usar todas las caracterﾃｭsticas disponibles, obteniendo un **MAPE** de 11.61% y un **RMSE** de 1.9123. La transformaciﾃｳn de los datos mejora notablemente el desempeﾃｱo de todos los modelos, siendo mﾃ｡s efectiva para Random Forest, que tambiﾃｩn muestra mejor generalizaciﾃｳn en diferentes configuraciones. Por otro lado, **Gradient Boosting** es competitivo, aunque generalmente inferior a Random Forest despuﾃｩs de la transformaciﾃｳn, mientras que **XGBoost** tiene el desempeﾃｱo mﾃ｡s bajo. Usar todas las caracterﾃｭsticas en lugar de solo \"Aire\" tambiﾃｩn contribuye a una ligera mejora en la precisiﾃｳn general con transformaciﾃｳn, sin embargo cuando no hay transformaciﾃｳn da mejores resultados solo usando variables relacionadas con la **calidad del aire**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Predicciﾃｳn directa**\n",
        "| Modelo           | Transformaciﾃｳn    | Configuraciﾃｳn | MAPE    | RMSE   |\n",
        "|------------------|-------------------|---------------|---------|--------|\n",
        "| GradientBoosting | Sin Transformaciﾃｳn | Con Aire      | 29.14%  | 3.8512 |\n",
        "| GradientBoosting | Sin Transformaciﾃｳn | Con Todo      | 18.77%  | 2.5044 |\n",
        "| RandomForest     | Sin Transformaciﾃｳn | Con Aire      | 4.92%  | 0.6415 |\n",
        "| RandomForest     | Sin Transformaciﾃｳn | Con Todo      | 5.04%   | 0.6472 |\n",
        "| XGBoost          | Sin Transformaciﾃｳn | Con Aire      | 14.93%  | 1.8036 |\n",
        "| XGBoost          | Sin Transformaciﾃｳn | Con Todo      | 10.11%  | 1.2591 |\n",
        "| GradientBoosting | Con Transformaciﾃｳn | Con Aire      | 29.94%  | 3.9333 |\n",
        "| GradientBoosting | Con Transformaciﾃｳn | Con Todo      | 18.86%  | 2.4919 |\n",
        "| RandomForest     | Con Transformaciﾃｳn | Con Aire      | 4.81%   | 0.6347 |\n",
        "| RandomForest     | Con Transformaciﾃｳn | Con Todo      | 4.92%   | 0.6391 |\n",
        "| XGBoost          | Con Transformaciﾃｳn | Con Aire      | 14.93%  | 1.8036 |\n",
        "| XGBoost          | Con Transformaciﾃｳn | Con Todo      | 10.11%  | 1.2591 |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los resultados de la predicciﾃｳn directa son bastante dispares. Se puede concluir que  la transformaciﾃｳn no mejora prﾃ｡cticamente nada, es mﾃ｡s, producen prﾃ｡cticamente los mismos resultados.  Destaca tambiﾃｩn que en GradientBoosting y en XGBoost los modelos con todas las variables dan bastante mejores resultados que los modelos con solo las variables de aire. Ocurre lo contrario con RandomForest, donde solo variables de aire da mejores resultados. Se puede ver que  GradientBoosting da unos resultados bastante malos con 3.8-3.9 de RMSE con solo aire y con un 2.5 de  RMSE con todas las variables. Los mejores modelos son los de RandomForest con un 0.64 de RMSE aproximadamente en todos los modelos. Los modelos de XGBoost no da resultados muy malos, pero en comparaciﾃｳn con RandomForest tampoco se pueden considerar que den resultados buenos, su RMSE es de 1.8 con aire y de 1.25 con todo. \n",
        "\n",
        "Por tanto, en este caso nos quedarﾃｭamos con los modelos de RandomForest, en concreto, nos quedarﾃｭamos con el modelo de RandomForest con transformaciﾃｳn y con solo las variables de aire, teniendo un MAPE de 4.81% y un RMSE de 0.6347. \n",
        "\n",
        "Hay que tener en cuenta que no se ha llegado a realizar grid de hiperparﾃ｡metros para estos modelos por falta de tiempo y  recursos (el tiempo de entrenamiento era muy elevado). Tal vez si se hubiese configurado un buen grid los resultados de los modelos hubieran mejorado, en especial los de GradientBoosting ya que se ve claramente que sus resultados no son para nada buenos.\n",
        "\n",
        "----\n",
        "\n",
        "Una vez comparadas las dos estrategias, nos quedarﾃｭamos con el modelo de RandomForest de predicciﾃｳn directa ya que presenta solo un 0.64 de RMSE en comparaciﾃｳn con el 1.91 del de recursiva. No obstante, hay que destacar que el modelo recursivo ha predecido la totalidad del conjunto de test, el de predicciﾃｳn directa solo ha predecido los 15 primeros dﾃｭas del conjunto de test. Esto claramente ha podido influir en los resultados y tal vez fuera mﾃ｡s conveniente a la larga usar el recursivo si se van a realizar predicciones de muchos dﾃｭas, el modelo directo puede resultarnos ﾃｺtil y dar buenos resultados en predicciones de pocos dﾃｭas, como es nuestro caso donde predecimos los siguientes 7 dﾃｭas. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Conclusiﾃｳn\n",
        "\n",
        "A lo largo de esta prﾃ｡ctica, hemos podido observar el comportamiento de distintos modelos para la predicciﾃｳn de series temporales, y mﾃ｡s concretamente para la predicciﾃｳn de la variable `NOX`. Entre los modelos estudiados, hemos probado algunos en los que ﾃｺnicamente utilizﾃ｡bamos dicha variable para entrenar el modelo y hacer las predicciones. Estos modelos, al ser mﾃ｡s simples, han dado peores resultados. Por un lado, el modelo baseLine, que predecﾃｭa el valor para un dﾃｭa como el valor del mismo dﾃｭa en el aﾃｱo anterior, ha dado resultados bastante mejorables, lo cual era esperable, debido a su gran simplicidad.\n",
        "\n",
        "Por otro lado, el modelo de RandomForest con ventana deslizante, si bien ha dado resultados algo mejores que el modelo baseLine, especialmente para el error $RMSE$, seguﾃｭa teniendo ciertas dificultades para predecir los valores mﾃ｡s complejos de nuestro dataset.\n",
        "\n",
        "A continuaciﾃｳn, hemos probado otros modelos en los que ya se podﾃｭan incluir el resto de variables de nuestro dataset, consiguiendo asﾃｭ una mayor informaciﾃｳn para realizar las predicciones. Hemos observado que en general, estos modelos han dado mejores resultados que los anteriores modelos en los que ﾃｺnicamente utilizﾃ｡bamos la variable `NOX`. Comenzando por incluir las variables referentes a los datos de calidad del aire, hemos probado un modelo recursivo y otro directo, usando primero los datos originales y luego los transformados, y probando distintas tﾃｩcnicas, y los resultados obtenidos han mejorado considerablemente, especialmente con los modelos recursivos.\n",
        "\n",
        "Luego, hemos vuelto a usar los mismos modelos, pero esta vez aﾃｱadiendo ademﾃ｡s los datos meteorolﾃｳgicos. En este caso, los resultados han empeorado ligeramente para los modelos sin transformaciﾃｳn respecto a usar ﾃｺnicamente los datos de calidad del aire, sin embargo para los modelos con transformaciﾃｳn ha sido beneficioso usar todas las variables en vez de las relacionadas ﾃｺnicamente con la calidad del aire. Aunque la incorporaciﾃｳn de datos adicionales puede, en principio, mejorar las predicciones al aportar mﾃ｡s informaciﾃｳn relevante, en el caso de los modelos sin transformaciﾃｳn, la inclusiﾃｳn de las variables meteorolﾃｳgicas no ha producido los resultados esperados. Algunas razones por las que los datos meteorolﾃｳgicos podrﾃｭan haber empeorado los resultados son:\n",
        "\n",
        "- **Ruido**: Los datos meteorolﾃｳgicos, aunque pueden ser ﾃｺtiles en ciertas circunstancias, podrﾃｭan haber introducido ruido en el modelo. Si alguna de las variables meteorolﾃｳgicas no estﾃ｡ directamente relacionada con la variable objetivo (`NOX`), estas nuevas caracterﾃｭsticas pueden haber confundido al modelo, dificultando la tarea de identificar patrones relevantes y, como consecuencia, aumentando el error.\n",
        "  \n",
        "- **Overfitting**: Tambiﾃｩn es posible que la inclusiﾃｳn de mﾃ｡s variables haya aumentado la complejidad del modelo, lo que puede haber dado lugar a un mayor overfitting. En ciertos modelos, un mayor nﾃｺmero de caracterﾃｭsticas puede hacer que el modelo se ajuste demasiado a los datos de entrenamiento, capturando incluso las fluctuaciones aleatorias que no son representativas de las relaciones generales.\n",
        "\n",
        "En cuanto a los **modelos con transformaciﾃｳn**, hemos observado que la mejora en el rendimiento al incluir todas las variables (tanto de calidad del aire como meteorolﾃｳgicas) podrﾃｭa explicarse por varias razones:\n",
        "\n",
        "1. **Relaciﾃｳn no lineal entre variables y respuesta**: Al incluir mﾃ｡s variables, algunas de ellas podrﾃｭan tener una relaciﾃｳn no lineal con la variable objetivo (`NOX`). Las transformaciones permiten que los modelos capturen estas relaciones complejas que no se pueden modelar de manera directa.\n",
        "\n",
        "2. **Escala y unidades de las variables**: Al aﾃｱadir variables meteorolﾃｳgicas, algunas de ellas pueden tener escalas muy diferentes a las de las variables relacionadas con el aire. Las transformaciones, como la normalizaciﾃｳn o estandarizaciﾃｳn, ayudan a igualar la influencia de cada variable en el modelo, evitando que algunas variables dominen sobre otras.\n",
        "\n",
        "3. **Multicolinealidad**: Al usar todas las variables, es posible que algunas de ellas estﾃｩn correlacionadas entre sﾃｭ. Las transformaciones pueden ayudar a reducir la multicolinealidad, facilitando que el modelo se enfoque en las relaciones mﾃ｡s importantes sin que las variables redundantes interfieran.\n",
        "\n",
        "4. **Mejor manejo de la complejidad**: Las transformaciones permiten que modelos mﾃ｡s complejos, como los modelos recursivos, puedan ajustarse mejor a la nueva informaciﾃｳn, capturando patrones y relaciones mﾃ｡s detalladas entre las variables.\n",
        "\n",
        "En resumen, aunque los datos meteorolﾃｳgicos podrﾃｭan haber mejorado las predicciones al proporcionar informaciﾃｳn adicional relevante, en este caso parece que su incorporaciﾃｳn ha tenido un impacto negativo en el rendimiento de algunos modelos sin transformaciﾃｳn. Sin embargo, para los modelos con transformaciﾃｳn, el uso de todas las variables ha demostrado ser beneficioso al permitir que las transformaciones manejen mejor las complejidades y relaciones no lineales entre las variables, mejorando asﾃｭ la precisiﾃｳn del modelo. Esto resalta la importancia de la selecciﾃｳn y transformaciﾃｳn adecuadas de las caracterﾃｭsticas, analizando cﾃｳmo la inclusiﾃｳn de nuevas variables puede mejorar o empeorar el rendimiento del modelo, y cﾃｳmo las transformaciones pueden ayudar a que los modelos capturen patrones mﾃ｡s complejos y no lineales.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "ML",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01c253124dae4fddb98db81f63d5b373": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4aec853fa0114664bde62ca49b20b48f",
              "IPY_MODEL_164e546a85c1483e80f137677eab4543",
              "IPY_MODEL_f867d34b0bd74017aa02412d74de7350"
            ],
            "layout": "IPY_MODEL_b4f5e1d1087d46f8bf6b0d0fd4927230"
          }
        },
        "04e702172a0449b0b25880c328c34d6b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "164e546a85c1483e80f137677eab4543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_491be8e09b7c425fbb273d6784577a02",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f74c8dfe7c74466687c174d0461a018c",
            "value": 116
          }
        },
        "171cab892e864575a4afb5b0443ebf4d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1e64f843256d408a9f43404fa5bc6ea9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2746601c9d4a4706a8121d97a8d06b8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "317fbc5ae3344fd4a88add339d1245da": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_82cc6a3bf9854ffb90b8e34475dcd17a",
              "IPY_MODEL_e4ee9d73815e4f7ca0329e8bfd6a008a",
              "IPY_MODEL_df9915dfc2eb4c339704007c5d739c2a"
            ],
            "layout": "IPY_MODEL_ea31aaff2ca541a28ce65cfff3640fde"
          }
        },
        "491be8e09b7c425fbb273d6784577a02": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4aec853fa0114664bde62ca49b20b48f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d639a9402a084df4b4b83bb305a1ea04",
            "placeholder": "窶",
            "style": "IPY_MODEL_2746601c9d4a4706a8121d97a8d06b8c",
            "value": "100%"
          }
        },
        "536ef95b27c54cc892783d8797084781": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_929623aaa1ad48e39ad9f35c27f84345",
            "placeholder": "窶",
            "style": "IPY_MODEL_04e702172a0449b0b25880c328c34d6b",
            "value": "100%"
          }
        },
        "661db2f36a5446ada56a51a1c587e6f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69dded658f994dc883a463719a3938d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b0b8a48afeb4595bf57f041bd91b866": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e45814a4ebb426fbaf541e36f8e4816": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70df1ca357044be386e1449b8e9537e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "724bbef240b74dec90417f9e9957e018": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7738fab5a3ce441581683f6c8f5c0d29": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f758b7f6fa0649bc8549183bdadf416d",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_724bbef240b74dec90417f9e9957e018",
            "value": 116
          }
        },
        "77c376a865ce4582bafc955c64bc89fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "82cc6a3bf9854ffb90b8e34475dcd17a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_171cab892e864575a4afb5b0443ebf4d",
            "placeholder": "窶",
            "style": "IPY_MODEL_caf4de2757d14c4887404d206e53c398",
            "value": "100%"
          }
        },
        "90ad10ab5ee144ae84e3d1efdb763cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e45814a4ebb426fbaf541e36f8e4816",
            "placeholder": "窶",
            "style": "IPY_MODEL_69dded658f994dc883a463719a3938d5",
            "value": "窶116/116窶[00:00&lt;00:00,窶161.77it/s]"
          }
        },
        "929623aaa1ad48e39ad9f35c27f84345": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b1a464f32bb24a1fa64c252f68924dcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b4f5e1d1087d46f8bf6b0d0fd4927230": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "caf4de2757d14c4887404d206e53c398": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d639a9402a084df4b4b83bb305a1ea04": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "df9915dfc2eb4c339704007c5d739c2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5629120d9d746ba8785f11efd644e82",
            "placeholder": "窶",
            "style": "IPY_MODEL_661db2f36a5446ada56a51a1c587e6f5",
            "value": "窶116/116窶[00:00&lt;00:00,窶176.23it/s]"
          }
        },
        "e4ee9d73815e4f7ca0329e8bfd6a008a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70df1ca357044be386e1449b8e9537e9",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_77c376a865ce4582bafc955c64bc89fc",
            "value": 116
          }
        },
        "e5629120d9d746ba8785f11efd644e82": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea31aaff2ca541a28ce65cfff3640fde": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74c8dfe7c74466687c174d0461a018c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f758b7f6fa0649bc8549183bdadf416d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f867d34b0bd74017aa02412d74de7350": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e64f843256d408a9f43404fa5bc6ea9",
            "placeholder": "窶",
            "style": "IPY_MODEL_b1a464f32bb24a1fa64c252f68924dcd",
            "value": "窶116/116窶[00:00&lt;00:00,窶163.62it/s]"
          }
        },
        "fa11b972054443699bd76502015016f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_536ef95b27c54cc892783d8797084781",
              "IPY_MODEL_7738fab5a3ce441581683f6c8f5c0d29",
              "IPY_MODEL_90ad10ab5ee144ae84e3d1efdb763cab"
            ],
            "layout": "IPY_MODEL_6b0b8a48afeb4595bf57f041bd91b866"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
