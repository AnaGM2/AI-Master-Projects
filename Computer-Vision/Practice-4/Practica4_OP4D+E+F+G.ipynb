{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# $\\hspace{1.5 cm}$ **Atención: Esta Notebook debe ejecutarse usando Google Colab**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbrGBCecWKWD"
      },
      "source": [
        "# Práctica 4 - Visión Artificial\n",
        "\n",
        "**Autores:** José María García Ortiz, Levi Malest Villareal y Ana Gil Molina"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yjXEv9sWKWG"
      },
      "source": [
        "## Importación de las librerías necesarias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hn_XbvBaWKWH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "# Cargar modelos comprimidos desde tensorflow_hub\n",
        "os.environ['TFHUB_MODEL_LOAD_FORMAT'] = 'COMPRESSED'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yos3zYiSWKWI"
      },
      "outputs": [],
      "source": [
        "import IPython.display as display\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12, 12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9kqRYxReWKWJ"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import tensorflow_hub as hub\n",
        "import threading\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display as ipy_display\n",
        "from io import BytesIO\n",
        "from functools import partial\n",
        "from keras.utils import get_custom_objects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r61g7zR1WKWK"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mcWDNSvWKWK",
        "outputId": "b8b1c5ff-b9fc-4899-e05a-0a430fdbbb3a"
      },
      "outputs": [],
      "source": [
        "# Python ≥3.5 requerido\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 requerido\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "try:\n",
        "    # %tensorflow_version sólo existe en Colab.\n",
        "    %tensorflow_version 2.x\n",
        "    IS_COLAB = True\n",
        "except Exception:\n",
        "    IS_COLAB = False\n",
        "\n",
        "# TensorFlow ≥2.0 requerido\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "assert tf.__version__ >= \"2.0\"\n",
        "\n",
        "if not tf.config.list_physical_devices('GPU'):\n",
        "    print(\"No GPU detectada. CNNs serán muy lentas sin GPU.\")\n",
        "    if IS_COLAB:\n",
        "        print(\"Cambiar runtime y seleccionar una GPU como acelerador hardware.\")\n",
        "\n",
        "# Import comunes:\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "# Para hacer el notebook repetible:\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "\n",
        "# Para graficar las figuras:\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Dónde salvar las figuras:\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"cnn\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOtKJRk3WKWM"
      },
      "source": [
        "Utilidades para graficar imágenes RGB y/o gris:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0udFUDZWKWM"
      },
      "outputs": [],
      "source": [
        "def plot_image(image):\n",
        "    plt.imshow(image, cmap=\"gray\", interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")\n",
        "\n",
        "def plot_color_image(image):\n",
        "    plt.imshow(image, interpolation=\"nearest\")\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pk09qzPRQrtQ",
        "outputId": "4889b2f8-5ebf-407e-dde8-37b99f298477"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import shutil\n",
        "import gdown\n",
        "import zipfile\n",
        "\n",
        "# IDs de los archivos compartidos (reemplaza con los IDs de tus archivos .zip)\n",
        "file_ids = ['1nK9azVnkKjTMB49v3UcafTQRW20r20Fa', '1HtZK1V6KE7l1QLXs22SYzV0ElUyZHVgM', '1n4o8WcjgqUS8DKqYGrRMciUvYZUWPTdn', '1dTHW4lgvOVX4nuJcOvGPVqLffEREHCKx']\n",
        "\n",
        "# Nombres locales para los archivos descargados y descomprimidos\n",
        "download_paths = ['/model.zip', '/Estilos OP4G.zip', '/Contenidos OP4G.zip', '/models.zip']\n",
        "extract_paths = ['', '', '', '']\n",
        "\n",
        "# Descargar y descomprimir cada archivo\n",
        "for file_id, download_path, extract_path in zip(file_ids, download_paths, extract_paths):\n",
        "    # Descargar archivo\n",
        "    gdown.download(f'https://drive.google.com/uc?id={file_id}', download_path, quiet=False)\n",
        "\n",
        "    # Descomprimir el archivo ZIP\n",
        "    with zipfile.ZipFile(download_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_path)\n",
        "        print(f\"Archivos descomprimidos en '{extract_path}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJseYTegWKWM"
      },
      "source": [
        "## <div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>Ejercicio OP4D (0.2 pts)</strong></div>\n",
        "\n",
        "Intentar replicar el entrenamiento y posterior prueba del modelo de clasificación para el *dataset Fashion-MNIST* con la arquitectura ResNet-34 del notebook `VC_using_CNNs.ipynb` (en lugar del modelo original)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXe5qCVcWKWN"
      },
      "source": [
        "Para comenzar, cargamos el dataset y separamos los datos en un conjunto de entrenamiento, uno de validación y uno de test."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Cwte3M2WKWO",
        "outputId": "7a253f4a-8abe-4d8c-fbaf-de47e2987713"
      },
      "outputs": [],
      "source": [
        "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
        "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]\n",
        "\n",
        "# Normalización estándar y particionamiento de los datos en subconjuntos de train / validación / test:\n",
        "X_mean = X_train.mean(axis=0, keepdims=True)\n",
        "X_std = X_train.std(axis=0, keepdims=True) + 1e-7\n",
        "X_train = (X_train - X_mean) / X_std\n",
        "X_valid = (X_valid - X_mean) / X_std\n",
        "X_test = (X_test - X_mean) / X_std\n",
        "\n",
        "X_train = X_train[..., np.newaxis]\n",
        "X_valid = X_valid[..., np.newaxis]\n",
        "X_test = X_test[..., np.newaxis]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HglN-zoxWKWO"
      },
      "source": [
        "A continuación, definimos las etiquetas que corresponden a las clases del dataset *Fashion-MNIST*. Este conjunto de datos contiene imágenes de ropa etiquetadas con las siguientes categorías:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2F-I7fjWWKWO"
      },
      "outputs": [],
      "source": [
        "# Define the text labels\n",
        "fashion_mnist_labels = [\"T-shirt/top\",  # index 0\n",
        "                        \"Trouser\",      # index 1\n",
        "                        \"Pullover\",     # index 2\n",
        "                        \"Dress\",        # index 3\n",
        "                        \"Coat\",         # index 4\n",
        "                        \"Sandal\",       # index 5\n",
        "                        \"Shirt\",        # index 6\n",
        "                        \"Sneaker\",      # index 7\n",
        "                        \"Bag\",          # index 8\n",
        "                        \"Ankle boot\"]   # index 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F3Z_44o3WKWO"
      },
      "source": [
        "Inspeccionamos algunos ejemplos extraidos del dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "id": "L-kFBboGWKWP",
        "outputId": "5c3c6949-1f05-476c-957b-abc2ed7d2fb9"
      },
      "outputs": [],
      "source": [
        "# Plot a random sample of 10 test images, and their ground truth labels:\n",
        "figure = plt.figure(figsize=(15, 6))\n",
        "for i, index in enumerate(np.random.choice(X_test.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    # Display each image\n",
        "    ax.imshow(np.squeeze(X_test[index]))\n",
        "    true_index = y_test[index]\n",
        "    # Set the title for each image\n",
        "    ax.set_title(\"{}\".format(fashion_mnist_labels[true_index]), color=(\"green\"))\n",
        "print(\"X_train.shape:\",X_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyW3FweoalWY"
      },
      "source": [
        "Empezamos a definir el modelo. En primer lugar, creamos una capa de tipo residual, las cuales, como hemos visto en clase, permiten flujos de gradientes más eficientes durante *training*, y posibilitan la construcción de redes más profundas sin *overfitting* ni *gradient overflow/underflow*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0YySx7QKWKWP"
      },
      "outputs": [],
      "source": [
        "DefaultConv2D = partial(keras.layers.Conv2D, kernel_size=3, strides=1,\n",
        "                        padding=\"SAME\", use_bias=False)\n",
        "\n",
        "class ResidualUnit(keras.layers.Layer):\n",
        "    def __init__(self, filters, strides=1, activation=\"relu\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.activation = keras.activations.get(activation)\n",
        "        self.main_layers = [\n",
        "            DefaultConv2D(filters, strides=strides),\n",
        "            keras.layers.BatchNormalization(),\n",
        "            self.activation,\n",
        "            DefaultConv2D(filters),\n",
        "            keras.layers.BatchNormalization()]\n",
        "        self.skip_layers = []\n",
        "        if strides > 1:\n",
        "            self.skip_layers = [\n",
        "                DefaultConv2D(filters, kernel_size=1, strides=strides),\n",
        "                keras.layers.BatchNormalization()]\n",
        "\n",
        "    def call(self, inputs):\n",
        "        Z = inputs\n",
        "        for layer in self.main_layers:\n",
        "            Z = layer(Z)\n",
        "        skip_Z = inputs\n",
        "        for layer in self.skip_layers:\n",
        "            skip_Z = layer(skip_Z)\n",
        "        return self.activation(Z + skip_Z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPlCtLyGbtGK"
      },
      "source": [
        "A continuación, se cosntruye el modelo, que consiste en un modelo secuencial definido mediante el método `.add` junto con un bucle `for` para ir añadiendo capas, cada una con el número de filtros deseados."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5HZiC_TWKWP",
        "outputId": "a1081a17-db27-4113-ee10-1469c7c61aaa"
      },
      "outputs": [],
      "source": [
        "model = keras.models.Sequential()\n",
        "model.add(DefaultConv2D(64, kernel_size=7, strides=2,\n",
        "                        input_shape=[224, 224, 3]))\n",
        "model.add(keras.layers.BatchNormalization())\n",
        "model.add(keras.layers.Activation(\"relu\"))\n",
        "model.add(keras.layers.MaxPool2D(pool_size=3, strides=2, padding=\"same\"))\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "    strides = 1 if filters == prev_filters else 2\n",
        "    model.add(ResidualUnit(filters, strides=strides))\n",
        "    prev_filters = filters\n",
        "model.add(keras.layers.GlobalAvgPool2D())\n",
        "model.add(keras.layers.Flatten())\n",
        "model.add(keras.layers.Dense(10, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3uwtpsWWKWQ"
      },
      "source": [
        "Con `model.summary()` podemos inspeccionar fácilmente las capas y el número de parámetros de nuestro modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        },
        "id": "ydfnDDc5WKWQ",
        "outputId": "2555f0ad-2226-42a2-bd42-9af1d4380298"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CWuoJF_tWKWQ"
      },
      "source": [
        "Como el modelo espera imágenes de entrada con un tamaño de `(224,224,3)`, pero el dataset con el que estamos trabajando en este ejercicio, *Fashion-MNIST*, contiene imágenes en escala de grises con tamaño `(28,28,1)`, debemos redimensionar dichas imágenes a tamaño `(224,224,3)` antes de pasarlas al modelo, de forma que las dimensiones sean compatibles con la arquitectura de la red. Usamos `tf.data.Dataset` para procesar las imágenes de forma más eficiente."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KljsOfchWKWQ"
      },
      "outputs": [],
      "source": [
        "def preprocess(image, label):\n",
        "    image_rgb = tf.image.grayscale_to_rgb(image)  # Convertir a 3 canales\n",
        "    resized_image = tf.image.resize(image_rgb, [224, 224])  # Redimensionar\n",
        "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
        "    return final_image, label\n",
        "\n",
        "# Crear el dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.map(preprocess).batch(32).prefetch(1)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
        "valid_dataset = valid_dataset.map(preprocess).batch(32).prefetch(1)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.map(preprocess).batch(32).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leyPPXokcmhW"
      },
      "source": [
        "Una vez definido el modelo (su arquitectura), simplemente queda:\n",
        "1. **Compilarlo**: añadir al grafo de computación nodos para computar la función de pérdida, la(s) métrica(s) de funcionamiento, y la lógica de control para el optimizador. Se hace con el método `.compile()`.\n",
        "2. **Entrenarlo**: se le pasan los ejemplos etiquetados (tanto de entrenamiento como de validación, pero NO de test). Se hace con el método `.fit()`, que desencadena todo el proceso de ajuste iterativo de pesos mediante gradiente descendente.\n",
        "3. **Evaluarlo**: sobre el subconjunto de test previamente apartado. Se hace con el método `.evaluate()`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhEfVSNaZV4o"
      },
      "source": [
        "**¡Atención, la siguiente celda tarda casi una hora en colab (con GPU)!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67qKoiMQWKWR",
        "outputId": "d6e6f4b9-3016-432e-cf52-42c231ca881a"
      },
      "outputs": [],
      "source": [
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
        "history = model.fit(train_dataset, epochs=10, validation_data=valid_dataset)\n",
        "score = model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EPBolT_kqVLJ"
      },
      "outputs": [],
      "source": [
        "# Verificar si la carpeta \"models\" existe, si no, crearla\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"models/resnet34_OP4D.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I88RyU6oqCQ8"
      },
      "outputs": [],
      "source": [
        "# Registrar la capa personalizada\n",
        "get_custom_objects().update({'ResidualUnit': ResidualUnit})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTnVz0ssiVw6",
        "outputId": "9adc02a0-b001-40ad-e977-9f8557a95b30"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo\n",
        "model = keras.models.load_model(\"models/resnet34_OP4D.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqar-MJKddyk"
      },
      "source": [
        "Comparamos la precisión (_accuracy_) sobre los conjuntos de entrenamiento y de test. Es de esperar que la precisión en el conjunto de test sea algo menor, ya que son datos desconocidos para el modelo. Sin embargo, como se ha mantenido separado el conjunto de test, es ya un estimador bastante decente de lo que cabe esperar del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHF-netpWKWR",
        "outputId": "fc9d070e-15c9-4709-fbd8-6e0b4a00f7ab"
      },
      "outputs": [],
      "source": [
        "train_loss, train_accuracy = model.evaluate(train_dataset)\n",
        "train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FA5hEJXmWKWS",
        "outputId": "a9147492-78bf-4819-f920-7dc650418f01"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TmQQYrpDetMg"
      },
      "source": [
        "Podemos graficar los valores guardados en la variable `history` devuelta por el método `.fit()`, que muestran la evolución del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "CgUpIthfWKWS",
        "outputId": "60425b39-8fc3-4e29-e65b-890987e7c284"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, samples=10, init_phase_samples=None):\n",
        "    epochs = history.params['epochs']\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "    axs[0].plot(history.history['accuracy'], 'bo-', label='Training accuracy')\n",
        "    axs[0].plot(history.history['val_accuracy'], 'go-', label='Validation accuracy')\n",
        "    axs[0].set_title('Training and validation accuracy')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(history.history['loss'], 'b+-', label='Loss')\n",
        "    axs[1].plot(history.history['val_loss'], 'g+-', label='Validation loss')\n",
        "    axs[1].set_title('Training and validation loss')\n",
        "    axs[1].legend()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4bvcrjSrA0o"
      },
      "source": [
        "En las anteriores gráficas podemos apreciar como el accuracy en el conjunto de entrenamiento va mejorando a lo largo de las épocas, aunque en el conjunto de validación se queda algo estancado a partir de la segunda época y aumenta muy levemente. Por otro lado, la pérdida en el conjunto de entrenamiento mejora considerablemente a lo largo de las épocas, ya que va disminuyendo, pero en el conjunto de validación se estanca o incluso empeora al aumentar las épocas. Esto podría indicar que sería conveniente entrenar el modelo con menos épocas, ya que no se ven mejoras significativas a partir de la segunda o tercera época.\n",
        "\n",
        "Finalmente, podemos predecir unas cuantas imágenes del conjunto de test, y comprobar los resultados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 925
        },
        "id": "p3n2rAjZWKWS",
        "outputId": "375268c2-6e31-48d1-9ad6-f1be8bb33f4d"
      },
      "outputs": [],
      "source": [
        "# Seleccionar un pequeño subconjunto de 15 imágenes aleatorias del test_dataset\n",
        "test_subset = test_dataset.take(1)  # Esto toma el primer lote de datos\n",
        "\n",
        "# Convertimos el dataset de test_subset a una lista para que podamos manipular las imágenes\n",
        "X_test_resized = []\n",
        "y_test_resized = []\n",
        "for image_batch, label_batch in test_subset:\n",
        "    X_test_resized.append(image_batch)\n",
        "    y_test_resized.append(label_batch)\n",
        "\n",
        "# Convertimos las listas a arrays\n",
        "X_test_resized = np.concatenate(X_test_resized, axis=0)\n",
        "y_test_resized = np.concatenate(y_test_resized, axis=0)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred = model.predict(X_test_resized)\n",
        "\n",
        "# Dibujamos una muestra aleatoria de 15 imágenes de test, junto con la etiqueta predicha y la correcta (ground truth):\n",
        "figure = plt.figure(figsize=(20, 8))\n",
        "for i, index in enumerate(np.random.choice(X_test_resized.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(X_test_resized[index]))\n",
        "    predict_index = np.argmax(y_pred[index])\n",
        "    true_index = y_test_resized[index]\n",
        "    ax.set_title(\"Pred:{} (True:{})\".format(fashion_mnist_labels[predict_index],\n",
        "                                  fashion_mnist_labels[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92zVWWe9K-_7"
      },
      "source": [
        "De las $15$ imágenes seleccionadas, solo $3$ han sido predichas incorrectamente. Esto podría corresponderse con el accuracy obtenido en el conjunto de test, ligeramente inferior a $0.9$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nYjnwSUWKWS"
      },
      "source": [
        "## <div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>Ejercicio OP4E (0.2 pts)</strong></div>\n",
        "\n",
        "Ídem para el ejemplo de *transfer learning* de dicho notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wrFANgCNWKWS"
      },
      "source": [
        "En este ejercicio, se pide usar la técnica de *transfer learning*, que consiste en emplear el _backbone_ de una CNN preentrenada para, añadiendo unas pocas capas adicionales, reentrenarla con un _dataset_ completamente nuevo. Ya hemos cargado el dataset *Fashion-MNIST* y separado en conjunto de entrenamiento, validación y prueba en el ejercicio anterior.\n",
        "\n",
        "Por otro lado, debemos realizar un preprocesamiento de los datos, ya que el modelo que vamos a usar espera imágenes de entrada con un tamaño de `(224,224,3)`, por lo que debemos redimensionar nuestras imágenes a tamaño `(224,224,3)`, para que sus dimensiones sean compatibles con la arquitectura de la red.\n",
        "\n",
        "Además, realizamos un _data augmentation_, que consiste en hacer pequeños cambios sobre las imágenes de entrada para multiplicar el número de ejemplos a usar en el entrenamiento. En este ejemplo, hacemos recortes aleatorios de las imágenes de entrada, y las _flipamos_ de izquierda a derecha, como esquema básico para realizar dicho _data augmentation_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1Pq9r7cgZ_M"
      },
      "outputs": [],
      "source": [
        "def central_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]])\n",
        "    top_crop = (shape[0] - min_dim) // 4\n",
        "    bottom_crop = shape[0] - top_crop\n",
        "    left_crop = (shape[1] - min_dim) // 4\n",
        "    right_crop = shape[1] - left_crop\n",
        "    return image[top_crop:bottom_crop, left_crop:right_crop]\n",
        "\n",
        "def random_crop(image):\n",
        "    shape = tf.shape(image)\n",
        "    min_dim = tf.reduce_min([shape[0], shape[1]]) * 90 // 100\n",
        "    return tf.image.random_crop(image, [min_dim, min_dim, 3])\n",
        "\n",
        "def preprocess(image, label, randomize=False):\n",
        "    if tf.shape(image)[-1] == 1:  # Convertir a RGB si tiene un solo canal\n",
        "        image = tf.image.grayscale_to_rgb(image)\n",
        "\n",
        "    if randomize:\n",
        "        cropped_image = random_crop(image)\n",
        "        cropped_image = tf.image.random_flip_left_right(cropped_image)\n",
        "    else:\n",
        "        cropped_image = central_crop(image)\n",
        "\n",
        "    resized_image = tf.image.resize(cropped_image, [224, 224])\n",
        "    final_image = keras.applications.xception.preprocess_input(resized_image)\n",
        "    return final_image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTY1SIxcgROM"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "\n",
        "# Crear el dataset\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(1000).repeat()\n",
        "train_dataset = train_dataset.map(partial(preprocess, randomize=True)).batch(batch_size).prefetch(1)\n",
        "\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
        "valid_dataset = valid_dataset.map(preprocess).batch(batch_size).prefetch(1)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test))\n",
        "test_dataset = test_dataset.map(preprocess).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37r6zeANgQR0"
      },
      "source": [
        "Usaremos **Xception** como CNN _backbone_, la cual es una red con más de 100 capas, cuyos pesos \"congelaremos\", y a la que simplemente añadiremos una capa `GlobalAveragePooling2D` seguida de una densa (`Dense`) con activación `softmax`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emjubASBcgBo",
        "outputId": "6061f16e-b5f7-4ba6-eb7c-71eabaebe253"
      },
      "outputs": [],
      "source": [
        "n_classes = len(fashion_mnist_labels)\n",
        "\n",
        "base_model = keras.applications.xception.Xception(weights=\"imagenet\",\n",
        "                                                  include_top=False)\n",
        "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
        "output = keras.layers.Dense(n_classes, activation=\"softmax\")(avg)\n",
        "model = keras.models.Model(inputs=base_model.input, outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pl0TDatCcgBo"
      },
      "source": [
        "Enumeramos las capas de la red:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gZ4_alRcgBu",
        "outputId": "672bac52-cedd-4fee-e118-788b0cc0677e"
      },
      "outputs": [],
      "source": [
        "for index, layer in enumerate(base_model.layers):\n",
        "    print(index, layer.name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UGsXa2WcgBu"
      },
      "source": [
        "Al estar `congelados` la mayoría de los pesos, el entrenamiento puede ser (relativamente) mucho más rápido (por ahora sólo involucra a la capa superior):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGo8gsTSY3oS"
      },
      "source": [
        "**¡Atención, la siguiente celda tarda unos 20 minutos en colab (con GPU)!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5_hRkJ9cgBu",
        "outputId": "62f47548-c27e-4868-c262-3f8721f1d9e0"
      },
      "outputs": [],
      "source": [
        "dataset_size = len(X_train)\n",
        "batch_size = 32\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "              metrics=[\"accuracy\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
        "                    validation_data=valid_dataset,\n",
        "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
        "                    epochs=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yos0vKz-dPTX"
      },
      "source": [
        "Evaluamos el modelo sobre el conjunto de test, mediante el método `.evaluate()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v24sRDrgcgBu",
        "outputId": "51793114-3680-4a8e-dce7-73ab4eeea567"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LIBk72pcgBv"
      },
      "outputs": [],
      "source": [
        "# Verificar si la carpeta \"models\" existe, si no, crearla\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"models/Xception_one_layer_OP4E.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsYiJ5DkcgBv"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo\n",
        "model = keras.models.load_model(\"models/Xception_one_layer_OP4E.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WtPWSqERdmjd"
      },
      "source": [
        "Comparamos la precisión (_accuracy_) sobre los conjuntos de entrenamiento y de test. Ya sabemos que es normal que la precisión en el conjunto de test sea algo menor, ya que son datos desconocidos para el modelo. Sin embargo, como se ha mantenido separado el conjunto de test, es un buen estimador para el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ul1DZROlcgBv",
        "outputId": "ab9798dd-d84e-40d8-c411-571c87d29bba"
      },
      "outputs": [],
      "source": [
        "# Dataset de evaluación sin `.repeat()`\n",
        "train_eval_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_eval_dataset = train_eval_dataset.map(partial(preprocess, randomize=False)).batch(batch_size).prefetch(1)\n",
        "\n",
        "train_loss, train_accuracy = model.evaluate(train_eval_dataset)\n",
        "train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGAwdg7ncgBv",
        "outputId": "1b65f7fc-75f4-4c12-8f6e-d8aef102ece4"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4SXdLJ0ikW-"
      },
      "source": [
        "A continuación, graficamos los valores guardados en la variable `history` devuelta por el método `.fit()`, que muestran la evolución del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "DUykdQ_AcgBv",
        "outputId": "2879d87b-55a3-4fb5-8922-db8e97591c79"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, samples=10, init_phase_samples=None):\n",
        "    epochs = history.params['epochs']\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "    axs[0].plot(history.history['accuracy'], 'bo-', label='Training accuracy')\n",
        "    axs[0].plot(history.history['val_accuracy'], 'go-', label='Validation accuracy')\n",
        "    axs[0].set_title('Training and validation accuracy')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(history.history['loss'], 'b+-', label='Loss')\n",
        "    axs[1].plot(history.history['val_loss'], 'g+-', label='Validation loss')\n",
        "    axs[1].set_title('Training and validation loss')\n",
        "    axs[1].legend()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exUAufMLq94m"
      },
      "source": [
        "En las gráficas anteriores, podemos apreciar que el accuracy en el conjunto de entrenamiento va aumentando a lo largo de las épocas, mientras que en el de validación es variable y no sigue una distribución clara. Vemos que en la segunda época disminuye, luego aumenta en la tercera época, y a partir de ahí empieza a disminuir otra vez. Esto podría ser una señal de overfitting, ya que la precisión en el conjunto de validación no mejora de forma consistente, mientras que en el conjunto de entrenamiento sí lo hace. Por otro lado, observando la gráfica con la pérdida, vemos que en el conjunto de entrenamiento va disminuyendo a lo largo de las épocas, y por tanto mejorando, mientras que en el de validación fluctúa y finalmente empeora. Esto también podría ser un indicio de sobreajuste, ya que el modelo no logra generalizar bien.\n",
        "\n",
        "Finalmente, podemos predecir unas cuantas imágenes del conjunto de test, y comprobar los resultados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuAs2F46cgBv",
        "outputId": "6dfd9c97-fe4e-47f1-9eb9-37d37ab06cf0"
      },
      "outputs": [],
      "source": [
        "test_subset = test_dataset.take(1)  # Esto toma el primer lote de datos\n",
        "\n",
        "# Convertimos el dataset de test_subset a una lista para que podamos manipular las imágenes y etiquetas\n",
        "X_test_resized = []\n",
        "y_test_resized = []\n",
        "for image_batch, label_batch in test_subset:\n",
        "    X_test_resized.append(image_batch)\n",
        "    y_test_resized.append(label_batch)\n",
        "\n",
        "# Convertimos las listas a arrays\n",
        "X_test_resized = np.concatenate(X_test_resized, axis=0)\n",
        "y_test_resized = np.concatenate(y_test_resized, axis=0)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred = model.predict(X_test_resized)\n",
        "\n",
        "# Crear una lista con las etiquetas reales y predichas\n",
        "for i in range(len(y_test_resized)):\n",
        "    predict_index = np.argmax(y_pred[i])\n",
        "    true_index = y_test_resized[i]\n",
        "    pred_label = fashion_mnist_labels[predict_index]\n",
        "    true_label = fashion_mnist_labels[true_index]\n",
        "\n",
        "    # Verificar si la predicción es correcta\n",
        "    if predict_index == true_index:\n",
        "        print(f\"Image {i + 1}: Predicted: {pred_label} (Correct), True: {true_label}\")\n",
        "    else:\n",
        "        print(f\"Image {i + 1}: Predicted: {pred_label} (Incorrect), True: {true_label}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t6POBTtcgBv"
      },
      "source": [
        "Vemos que de las $32$ imágenes seleccionadas del conjunto de test, $8$ se han predicho mal, lo cual tiene sentido, ya que antes hemos obtenido que el accuracy en test era de $0.6944$.\n",
        "\n",
        "Sin embargo, para obtener mejores resultados, conviene ahora \"descongelar\" los pesos del _backbone_, y hacer un ajuste de todos los pesos de la red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM7NQ2WeYfMr"
      },
      "source": [
        "**¡Atención, la siguiente celda tarda más de media hora en colab (con GPU)!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NtXdiA7cgBw",
        "outputId": "bb0d3dd2-ce4b-4971-ddf5-e7e459494ebc"
      },
      "outputs": [],
      "source": [
        "dataset_size = len(X_train)\n",
        "batch_size = 32\n",
        "\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "optimizer = keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True)\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,\n",
        "                metrics=[\"accuracy\"])\n",
        "history = model.fit(train_dataset,\n",
        "                    steps_per_epoch=int(0.75 * dataset_size / batch_size),\n",
        "                    validation_data=valid_dataset,\n",
        "                    validation_steps=int(0.15 * dataset_size / batch_size),\n",
        "                    epochs=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r97VkW7r2ZH"
      },
      "source": [
        "Evaluamos el modelo sobre el conjunto de test, mediante el método `.evaluate()`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TqQHBQPecgBw",
        "outputId": "fbeeeed9-3117-4a83-e9fb-1b060bd8bd02"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C2m1bbfvcgBw"
      },
      "outputs": [],
      "source": [
        "# Verificar si la carpeta \"models\" existe, si no, crearla\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"models/Xception_all_layers_OP4E.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IB6AxYRUcgBw"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo\n",
        "model = keras.models.load_model(\"models/Xception_all_layers_OP4E.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06LtHeier4On"
      },
      "source": [
        "De nuevo, comparamos la precisión (_accuracy_) sobre los conjuntos de entrenamiento y de test. Vemos que mejoran los valores respecto al modelo anterior, donde usábamos todos los pesos congelados excepto los de la capa superior"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EaaoZkdfPVhy"
      },
      "outputs": [],
      "source": [
        "# Dataset de evaluación sin `.repeat()`\n",
        "train_eval_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
        "train_eval_dataset = train_eval_dataset.map(partial(preprocess, randomize=False)).batch(batch_size).prefetch(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5ifUQF7cgBw",
        "outputId": "ec32758f-7e85-455f-d8d8-b6119442d3e9"
      },
      "outputs": [],
      "source": [
        "train_loss, train_accuracy = model.evaluate(train_eval_dataset)\n",
        "train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41VLiJkvcgBw",
        "outputId": "9a1ec79a-a898-4cc8-880e-06913e523768"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(test_dataset)\n",
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YgAFckeNsV2t"
      },
      "source": [
        "Otra vez, graficamos los valores guardados en la variable `history` devuelta por el método `.fit()`, que muestran la evolución del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W_1KMn0DRN5U"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, samples=10, init_phase_samples=None):\n",
        "    epochs = history.params['epochs']\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "    axs[0].plot(history.history['accuracy'], 'bo-', label='Training accuracy')\n",
        "    axs[0].plot(history.history['val_accuracy'], 'go-', label='Validation accuracy')\n",
        "    axs[0].set_title('Training and validation accuracy')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(history.history['loss'], 'b+-', label='Loss')\n",
        "    axs[1].plot(history.history['val_loss'], 'g+-', label='Validation loss')\n",
        "    axs[1].set_title('Training and validation loss')\n",
        "    axs[1].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "n4CUE_-ccgBx",
        "outputId": "56d9f11c-6763-4486-d2a9-b552e5f09ef8"
      },
      "outputs": [],
      "source": [
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NaZj0UwgrLcf"
      },
      "source": [
        "En las anteriores gráficas observamos que tanto el accuracy como la pérdida en el conjunto de entrenamiento fluctúan, pero sin lograr una gran mejora con las épocas. Lo mismo ocurre con el conjunto de validación. El rendimiento del modelo parece haberse estabilizado. Sin embargo, si lo comparamos con el modelo obtenido al ajustar únicamente los pesos de la capa superior, observamos que los valores obtenidos en este caso son algo mejores.\n",
        "\n",
        "Finalmente, podemos predecir unas cuantas imágenes del conjunto de test, y comprobar los resultados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KMvSQyXR4Ih",
        "outputId": "e670d923-32eb-4ae3-b23e-2a974c128af0"
      },
      "outputs": [],
      "source": [
        "test_subset = test_dataset.take(1)  # Esto toma el primer lote de datos\n",
        "\n",
        "# Convertimos el dataset de test_subset a una lista para que podamos manipular las imágenes y etiquetas\n",
        "X_test_resized = []\n",
        "y_test_resized = []\n",
        "for image_batch, label_batch in test_subset:\n",
        "    X_test_resized.append(image_batch)\n",
        "    y_test_resized.append(label_batch)\n",
        "\n",
        "# Convertimos las listas a arrays\n",
        "X_test_resized = np.concatenate(X_test_resized, axis=0)\n",
        "y_test_resized = np.concatenate(y_test_resized, axis=0)\n",
        "\n",
        "# Realizar predicciones\n",
        "y_pred = model.predict(X_test_resized)\n",
        "\n",
        "# Crear una lista con las etiquetas reales y predichas\n",
        "for i in range(len(y_test_resized)):\n",
        "    predict_index = np.argmax(y_pred[i])\n",
        "    true_index = y_test_resized[i]\n",
        "    pred_label = fashion_mnist_labels[predict_index]\n",
        "    true_label = fashion_mnist_labels[true_index]\n",
        "\n",
        "    # Verificar si la predicción es correcta\n",
        "    if predict_index == true_index:\n",
        "        print(f\"Image {i + 1}: Predicted: {pred_label} (Correct), True: {true_label}\")\n",
        "    else:\n",
        "        print(f\"Image {i + 1}: Predicted: {pred_label} (Incorrect), True: {true_label}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6TN2-HU6qah"
      },
      "source": [
        "Observamos que al hacer un ajuste más fino, descongelando los pesos del *backbone*, para ajustar todos los pesos de la red, los resultados y las métricas que se obtienen mejoran ligeramente. Sin embargo, esta mejora tampoco es demasiado grande, ya que se aprecia como en el segundo entrenamiento del modelo (con todos los pesos descongelados) el rendimiento se estabiliza y fluctúa, sin lograr grandes mejoras a lo largo de las épocas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS1wXl55spiv"
      },
      "source": [
        "## <div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>Ejercicio OP4F (0.2 pts)</strong></div>\n",
        "\n",
        "Ídem para el ejemplo de MNIST."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Imh-4EXt26ik"
      },
      "source": [
        "Ya tenemos el dataset *Fashion-MNIST* cargado y separado en conjunto de entrenamiento, validación y prueba del ejercicio OP4D. Por lo tanto, podemos pasar directamente a definir el modelo, compilarlo con el método `.compile()`, entrenarlo con el método `.fit()` y  evaluarlo con el método `.evaluate()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAmbv4_a3Z-p",
        "outputId": "83cbbd2a-2537-4fa0-d349-f201fb2dd962"
      },
      "outputs": [],
      "source": [
        "keras.backend.clear_session()\n",
        "tf.random.set_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
        "    keras.layers.MaxPool2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dropout(0.25),\n",
        "    keras.layers.Dense(128, activation=\"relu\"),\n",
        "    keras.layers.Dropout(0.5),\n",
        "    keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))\n",
        "model.evaluate(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Rq8kAIb44QA"
      },
      "outputs": [],
      "source": [
        "# Verificar si la carpeta \"models\" existe, si no, crearla\n",
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "# Guardar el modelo\n",
        "model.save(\"models/CNN_OP4F.keras\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7obrO2T5Srn"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo\n",
        "model = keras.models.load_model(\"models/CNN_OP4F.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiTiTMi939NX"
      },
      "source": [
        "Comparamos la precisión (_accuracy_) sobre los conjuntos de entrenamiento y de test. De nuevo, obtenemos que la precisión en el conjunto de test es algo menor que en el de entrenamiento, ya que los datos de test son desconocidos para el modelo. Sin embargo, como se ha mantenido separado el conjunto de test, es un buen estimador para el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aydrZAR95ihc",
        "outputId": "c02197db-18a5-4d96-fa91-3f7db1f30a2c"
      },
      "outputs": [],
      "source": [
        "train_loss, train_accuracy = model.evaluate(X_train, y_train)\n",
        "train_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzTELKI25kfe",
        "outputId": "e09f51ab-2232-4d3e-fce4-0a33e3d5601e"
      },
      "outputs": [],
      "source": [
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LaYM19H453ah"
      },
      "source": [
        "A continuación, graficamos los valores guardados en la variable `history` devuelta por el método `.fit()`, que muestran la evolución del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395
        },
        "id": "_qK7e37Q5q6w",
        "outputId": "3a063b9b-59af-40b0-df18-f07d9473013d"
      },
      "outputs": [],
      "source": [
        "def plot_history(history, samples=10, init_phase_samples=None):\n",
        "    epochs = history.params['epochs']\n",
        "\n",
        "    fig, axs = plt.subplots(1,2,figsize=(12,4))\n",
        "\n",
        "    axs[0].plot(history.history['accuracy'], 'bo-', label='Training accuracy')\n",
        "    axs[0].plot(history.history['val_accuracy'], 'go-', label='Validation accuracy')\n",
        "    axs[0].set_title('Training and validation accuracy')\n",
        "    axs[0].legend()\n",
        "\n",
        "    axs[1].plot(history.history['loss'], 'b+-', label='Loss')\n",
        "    axs[1].plot(history.history['val_loss'], 'g+-', label='Validation loss')\n",
        "    axs[1].set_title('Training and validation loss')\n",
        "    axs[1].legend()\n",
        "\n",
        "plot_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPiUEmAV6Aom"
      },
      "source": [
        "Observando las anteriores gráficas, apreciamos que el modelo empieza con una precisión de $0.8036$ en la primera época en el conjunto de entrenamiento, y va mejorando progresivamente hasta alcanzar $0.9561$ en la décima época. Esto indica que el modelo está mejorando y aprendiendo con el tiempo. Además, la pérdida de entrenamiento disminuye continuamente hasta el final del entrenamiento, lo cual indica que el modelo está aprendiendo efectivamente durante las épocas. Por otro lado, la precisión en el conjunto de validación aumenta más levemente a lo largo de las épocas, aunque hay una ligera caída en la última época, lo cual podría indicar que el modelo está empezando a sobreajustarse. En cuanto a la pérdida, vemos que en el conjunto de validación sigue una tendencia similar a la precisión, pues mejora levemente con pequeñas fluctuaciones en el camino. Esta muestra que el modelo generaliza bien, aunque las pequeñas fluctuaciones podrían indicar que el modelo está cerca de su límite en cuanto a su capacidad de generalización.\n",
        "\n",
        "Finalmente, predecimos unas cuantas imágenes del conjunto de test, y comprobamos los resultados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "id": "x5B0kHJ05ubh",
        "outputId": "c72cbe89-dfdd-4bc8-d5e6-c281e2bd6d83"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Dibujamos una muestra aleatoria de 15 imágenes de test, junto con la etiqueta predicha y la correcta (ground truth):\n",
        "figure = plt.figure(figsize=(20, 8))\n",
        "for i, index in enumerate(np.random.choice(X_test.shape[0], size=15, replace=False)):\n",
        "    ax = figure.add_subplot(3, 5, i + 1, xticks=[], yticks=[])\n",
        "    ax.imshow(np.squeeze(X_test[index]))\n",
        "    predict_index = np.argmax(y_pred[index])\n",
        "    true_index = y_test[index]\n",
        "    ax.set_title(\"Pred:{} (True:{})\".format(fashion_mnist_labels[predict_index],\n",
        "                                  fashion_mnist_labels[true_index]),\n",
        "                                  color=(\"green\" if predict_index == true_index else \"red\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KeWWfZzsK_AR"
      },
      "source": [
        "Mirando las predicciones anteriores, podemos observar que de las $15$ imágenes escogidas, tan solo una se ha predicho incorrectamente, lo cual se corresponde con el elevado accuracy obtenido, superior al $0.9$ tanto en el conjunto de entrenamiento como en el de test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZL-waLWHWKWT"
      },
      "source": [
        "## <div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>Ejercicio OP4G (0.1 pts)</strong></div>\n",
        "\n",
        "Generar algunas imágenes de ejemplo de arte digital a partir de imágenes propias usando el notebook \"Neural Style Transfer\" referenciado al final del notebook `VC_using_CNNs.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCZgFK4pWKWT"
      },
      "source": [
        "En este ejercicio se explora una técnica de la visión artificial conocida como ***Neural Style Transfer***, la cual permite combinar dos imágenes de manera que el contenido de una de ellas se mantenga, mientras se aplica el estilo artístico de la otra. Este algoritmo, propuesto en <a href=\"https://arxiv.org/abs/1508.06576\" class=\"external\">A Neural Algorithm of Artistic Style</a> (Gatys et al.), utiliza redes neuronales profundas para transferir el estilo de una imagen (como una obra de arte famosa) a una imagen de contenido (como una fotografía personal).\n",
        "\n",
        "La idea detrás de *Neural Style Transfer* es optimizar una imagen de salida para que combine las características del contenido de una imagen con las características estilísticas de otra. Para lograr esto, el algoritmo utiliza redes neuronales convolucionales para extraer las estadísticas de contenido y estilo de las imágenes, y luego fusionarlas de manera que la imagen resultante mantenga la estructura de la imagen de contenido pero adquiera el estilo artístico de la imagen de referencia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P7nbscr9WKWT"
      },
      "outputs": [],
      "source": [
        "# Cargar el modelo de estilo desde TensorFlow Hub\n",
        "#hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "hub_model = hub.load('model')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "beX8Ch3aWKWT"
      },
      "source": [
        "## Ejemplos de Imágenes de Arte Digital\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byWhFtSNWKWU"
      },
      "source": [
        "### Funciones auxiliares\n",
        "\n",
        "Para comenzar, definimos una función `load_img` que carga una imagen desde una ruta, la redimensiona y la convierte en un tensor de TensorFlow con forma $(1, altura, ancho, canales)$, donde la dimensión inicial, que en este caso toma el valor $1$, representa el batch size:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lYCssiVsWKWU"
      },
      "outputs": [],
      "source": [
        "def load_img(path_to_img):\n",
        "    '''\n",
        "    Carga una imagen desde una ruta, la redimensiona y la convierte en un tensor de TensorFlow.\n",
        "\n",
        "    Parámetros:\n",
        "        path_to_img: Ruta a la imagen.\n",
        "\n",
        "    Devuelve:\n",
        "        img: Tensor de la imagen procesada con forma (1, altura, ancho, canales).\n",
        "    '''\n",
        "\n",
        "    # Tamaño máximo para la dimensión más larga de la imagen\n",
        "    max_dim = 512\n",
        "\n",
        "    # Leer el archivo desde la ruta como un tensor con 3 canales (RGB) y normalizar a [0, 1]\n",
        "    img = tf.io.read_file(path_to_img)\n",
        "    img = tf.image.decode_image(img, channels=3)\n",
        "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
        "\n",
        "    # Dimensiones de la imagen\n",
        "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
        "    long_dim = max(shape)\n",
        "    scale = max_dim / long_dim\n",
        "\n",
        "    # Nuevas dimensiones de la imagen\n",
        "    new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "    # Redimensionar la imagen y añadir una nueva dimensión para representar el batch size\n",
        "    img = tf.image.resize(img, new_shape)\n",
        "    img = img[tf.newaxis, :]\n",
        "    return img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQ2vbPJ3WKWU"
      },
      "source": [
        "Por otro lado, se define la función `imshow`, que muestra una imagen usando `Matplotlib`, y de forma opcional le añade un título:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHfpYy5ZWKWU"
      },
      "outputs": [],
      "source": [
        "def imshow(image, title=None):\n",
        "    '''\n",
        "    Muestra una imagen usando Matplotlib.\n",
        "\n",
        "    Parámetros:\n",
        "        image: Imagen de la forma (altura, ancho, canales) o (1, altura, ancho, canales).\n",
        "        title (str, opcional): Título de la imagen.\n",
        "    '''\n",
        "\n",
        "    # Si la imagen tiene más de 3 dimensiones, se elimina la dimensión del batch size\n",
        "    if len(image.shape) > 3:\n",
        "        image = tf.squeeze(image, axis=0)\n",
        "\n",
        "    # Mostrar la imagen y el título (si es el caso)\n",
        "    plt.imshow(image)\n",
        "    if title:\n",
        "        plt.title(title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "od1g-RpgWKWV"
      },
      "source": [
        "A continuación, se define la función `tensor_to_image`, que convierte un tensor de `TensorFlow` que representa a una imagen en un objeto de imagen manejable por la librería `PIL`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fqh1pXhhWKWa"
      },
      "outputs": [],
      "source": [
        "def tensor_to_image(tensor):\n",
        "    '''\n",
        "    Convierte un tensor de TensorFlow en un objeto de imagen PIL.\n",
        "\n",
        "    Parámetros:\n",
        "        tensor: Tensor de TensorFlow que representa una imagen, con valores en [0, 1].\n",
        "\n",
        "    Devuelve:\n",
        "        PIL.Image: Objeto de imagen PIL resultado de convertir el tensor.\n",
        "    '''\n",
        "\n",
        "    # Escalar los valores del tensor de [0, 1] a [0, 255]\n",
        "    tensor = tensor*255\n",
        "\n",
        "    # Convertir el tensor en un array de NumPy (uint8)\n",
        "    tensor = np.array(tensor, dtype=np.uint8)\n",
        "\n",
        "    # Si el tensor tiene más de 3 dimensiones, significa que tiene un batch de imágenes\n",
        "    if np.ndim(tensor)>3:\n",
        "\n",
        "        # Asegurarse de que el tensor contenga una sola imagen\n",
        "        assert tensor.shape[0] == 1\n",
        "\n",
        "        # Extraer la única imagen del tensor\n",
        "        tensor = tensor[0]\n",
        "\n",
        "    # Crear un objeto de imagen PIL a partir del array\n",
        "    return PIL.Image.fromarray(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrJ8lEJwWKWa"
      },
      "source": [
        "### Ejemplo 1: Estilo del \"Guernica\" de Picasso\n",
        "\n",
        "En este ejemplo, utilizamos una imagen propia como imagen de contenido y le aplicamos el estilo artístico de la obra \"Guernica\" de Pablo Picasso, combinando los detalles de nuestra imagen con el estilo cubista del pintor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CYoZu1WeWKWa"
      },
      "outputs": [],
      "source": [
        "# Rutas de las imágenes de contenido y de estilo\n",
        "content_path = 'Contenidos OP4G/Neo_nieve.jpeg'\n",
        "style_path = 'Estilos OP4G/picasso.png'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        },
        "id": "9UWoZ9NYWKWa",
        "outputId": "c0bf1a7f-af4d-4ec5-a344-1664487f9928"
      },
      "outputs": [],
      "source": [
        "# Cargamos las imágenes como tensores de TensorFlow\n",
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "# Mostramos las imágenes de contenido y de estilo\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Imagen de contenido')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Imagen de estilo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "nU8oHea0WKWb",
        "outputId": "958250ad-a525-44f4-ec94-aa6cf5347281"
      },
      "outputs": [],
      "source": [
        "# Aplicamos el modelo con las imágenes cargadas y mostramos la imagen resultante\n",
        "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "tensor_to_image(stylized_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuz8HipbWKWb"
      },
      "source": [
        "### Ejemplo 2: Estilo abstracto\n",
        "\n",
        "En este otro ejemplo, utilizamos una imagen propia como imagen de contenido y le aplicamos el estilo abstracto propio del pintor Frankz Kline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mzeZggWNWKWb"
      },
      "outputs": [],
      "source": [
        "# Rutas de las imágenes de contenido y de estilo\n",
        "content_path = 'Contenidos OP4G/estatua_berlin.jpeg'\n",
        "style_path = 'Estilos OP4G/frankz_kline.jpg'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        },
        "id": "qcGEXCLCWKWc",
        "outputId": "54fa46d4-a780-4544-90d5-2c3408f8f309"
      },
      "outputs": [],
      "source": [
        "# Cargamos las imágenes como tensores de TensorFlow\n",
        "content_image = load_img(content_path)\n",
        "style_image = load_img(style_path)\n",
        "\n",
        "# Mostramos las imágenes de contenido y de estilo\n",
        "plt.subplot(1, 2, 1)\n",
        "imshow(content_image, 'Imagen de contenido')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "imshow(style_image, 'Imagen de estilo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        },
        "id": "K5TUe7r_WKWc",
        "outputId": "f4ab62cc-da27-4035-e595-fdc7b1b617ee"
      },
      "outputs": [],
      "source": [
        "# Aplicamos el modelo con las imágenes cargadas y mostramos la imagen resultante\n",
        "stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "tensor_to_image(stylized_image)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMGWXMADWKWd"
      },
      "source": [
        "## Ejemplo de Vídeos de Arte Digital\n",
        "\n",
        "Ahora podemos extender la técnica de *Neural Style Transfer* para aplicarla a un vídeo. El proceso consiste en seleccionar una imagen de estilo, la cual se aplicará a cada uno de los frames del vídeo, tratándolos como imágenes de contenido. De esta forma, se obtiene un vídeo que conserva el contenido original, pero transformado con un nuevo estilo artístico."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knRVndC-WKWd"
      },
      "source": [
        "### Funciones auxiliares\n",
        "\n",
        "En primer lugar, se define una función que aplica la técnica de transferencia de estilo a un vídeo completo, procesando cada frame por separado. La función, llamada `apply_style_to_video`, carga el modelo de estilo desde `TensorFlow Hub`, lee el vídeo frame a frame, aplica el estilo escogido a cada frame utilizando el modelo, y finalmente guarda el vídeo al que se le ha aplicado el estilo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X61u0Q1HWKWd"
      },
      "outputs": [],
      "source": [
        "def apply_style_to_video(video_path, style_image_path, output_path):\n",
        "    \"\"\"\n",
        "    Aplica el modelo de transferencia de estilo a un video, frame por frame.\n",
        "\n",
        "    Parámetros:\n",
        "        video_path (str): Ruta al video original.\n",
        "        style_image_path (str): Ruta a la imagen de estilo.\n",
        "        output_path (str): Ruta donde se guardará el video una vez aplicado el estilo.\n",
        "    \"\"\"\n",
        "    # Cargar el modelo de estilo de TensorFlow Hub\n",
        "    #hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "    hub_model = hub.load('model')\n",
        "\n",
        "    # Cargar la imagen de estilo y convertirla en tensor\n",
        "    style_image = load_img(style_image_path)  # Usar la función `load_img` que definimos antes\n",
        "\n",
        "    # Leer el video original\n",
        "    videoreader = cv2.VideoCapture(video_path)\n",
        "    fps = int(videoreader.get(cv2.CAP_PROP_FPS))  # Frames por segundo\n",
        "    frame_width = int(videoreader.get(cv2.CAP_PROP_FRAME_WIDTH))  # Ancho del frame\n",
        "    frame_height = int(videoreader.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Alto del frame\n",
        "\n",
        "    # Inicializar el writer para guardar el video de salida\n",
        "    videowriter = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (frame_width, frame_height))\n",
        "\n",
        "    print(\"Procesando video...\")\n",
        "\n",
        "    while videoreader.isOpened():\n",
        "        (grabbed, frame) = videoreader.read()  # Leer el siguiente frame\n",
        "        if not grabbed:\n",
        "            break  # Salir cuando no hay más frames\n",
        "\n",
        "        # Convertir el frame de BGR (OpenCV) a RGB\n",
        "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Normalizar el frame y agregar dimensión para el batch size\n",
        "        frame_tensor = tf.convert_to_tensor(frame_rgb, dtype=tf.float32) / 255.0\n",
        "        frame_tensor = tf.image.resize(frame_tensor, (512, 512))  # Ajustar tamaño\n",
        "        frame_tensor = frame_tensor[tf.newaxis, ...]\n",
        "\n",
        "        # Aplicar el modelo de transferencia de estilo\n",
        "        stylized_frame_tensor = hub_model(frame_tensor, style_image)[0]\n",
        "\n",
        "        # Convertir el tensor estilizado a imagen numpy\n",
        "        stylized_frame = tensor_to_image(stylized_frame_tensor)  # Usamos la función `tensor_to_image`\n",
        "\n",
        "        # Redimensionar el frame estilizado al tamaño original del video\n",
        "        stylized_frame = np.array(stylized_frame)\n",
        "        stylized_frame = cv2.resize(stylized_frame, (frame_width, frame_height))\n",
        "\n",
        "        # Convertir de RGB a BGR (para OpenCV) y escribir el frame procesado\n",
        "        videowriter.write(cv2.cvtColor(stylized_frame, cv2.COLOR_RGB2BGR))\n",
        "\n",
        "    # Liberar recursos\n",
        "    videoreader.release()\n",
        "    videowriter.release()\n",
        "    print(\"Video procesado y guardado en:\", output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zj195qybWKWe"
      },
      "source": [
        "La función `play_video` nos valdrá para mostrar los vídeos en el propio notebook (sacada del Notebook 01 visto en clase):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBsZruzzWKWe"
      },
      "outputs": [],
      "source": [
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "def play_video(videofilename):\n",
        "    mp4 = open(videofilename,'rb').read()\n",
        "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "    return HTML(\"\"\"\n",
        "    <video width=400 controls>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "    </video>\n",
        "    \"\"\" % data_url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkAg57QrWKWf"
      },
      "source": [
        "### Ejemplo: Estilo modernista\n",
        "\n",
        "En este ejemplo, partimos de un vídeo que muestra un paisaje natural y aplicamos el estilo artístico característico del modernismo de Georgia O'Keeffe. Este proceso transforma cada frame del video, combinando la esencia del paisaje original con la expresividad y el estilo del arte de O'Keeffe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VimWvDqHWKWf"
      },
      "outputs": [],
      "source": [
        "# Ruta del video original, imagen de estilo y salida\n",
        "video_path = 'Contenidos OP4G/paisaje.mp4'\n",
        "style_image_path = 'Estilos OP4G/okeeffe.jpeg'\n",
        "output_path = 'paisaje_estilizado.avi'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "3XqlP5fbWKWf",
        "outputId": "e0f06b62-b0ba-4868-9476-a5a6ae331c1f"
      },
      "outputs": [],
      "source": [
        "# Mostramos el vídeo original\n",
        "play_video(video_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "mYCCurA1WKWg",
        "outputId": "305557da-7d1b-4cef-89fb-85b4326e6410"
      },
      "outputs": [],
      "source": [
        "# Mostramos la imagen de estilo\n",
        "style_image = load_img(style_image_path)\n",
        "imshow(style_image, 'Imagen de estilo')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgYJFyL0X6aR"
      },
      "source": [
        "**¡Atención, la siguiente celda tarda una media hora en colab!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bAR135eWKWh",
        "outputId": "34864ec8-7400-4c04-bcc0-675c178f22d1"
      },
      "outputs": [],
      "source": [
        "# Aplicar el modelo al video\n",
        "apply_style_to_video(video_path, style_image_path, output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIObFJtSWKWh"
      },
      "source": [
        "Usamos el siguiente comando para convertir el vídeo resultante a formato `.mp4`, legible con la librería `OpenCV`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKncxuvDWKWh",
        "outputId": "7072ae68-2e70-46ab-fdc7-345bd7d72940"
      },
      "outputs": [],
      "source": [
        "!ffmpeg -y -i paisaje_estilizado.avi -vcodec h264 -acodec mp2 paisaje_estilizado.mp4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_voNrSCzWKWi"
      },
      "source": [
        "Para reproducir el vídeo, usamos la función `play_video` sacada del Notebook 01:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "iTc85l3qWKWi",
        "outputId": "ea686976-4195-443f-8331-655e0ef6d869"
      },
      "outputs": [],
      "source": [
        "# Mostramos el vídeo resultante\n",
        "play_video(\"paisaje_estilizado.mp4\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jcnGs24WKWi"
      },
      "source": [
        "## Interfaz interactiva con Webcam\n",
        "\n",
        "**¡Atención, la interfaz con la Webcam no funciona en Colab!**\n",
        "\n",
        "En esta sección, implementaremos una interfaz interactiva que permita aplicar estilos artísticos en tiempo real a través de la Webcam. Se podrá seleccionar un estilo artístico, y los frames capturados por la cámara se irán transformando, de forma que los elementos del entorno se reinterpretarán con un toque artístico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od71XtluWKWi"
      },
      "outputs": [],
      "source": [
        "# Variables globales\n",
        "style_image = None\n",
        "stop_camera = False\n",
        "#hub_model = hub.load('https://tfhub.dev/google/magenta/arbitrary-image-stylization-v1-256/2')\n",
        "hub_model = hub.load('model')\n",
        "image_widget = widgets.Image(format='png')  # Widget para mostrar la imagen de la cámara"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miCQyC4VWKWi"
      },
      "source": [
        "En primer lugar, creamos una función `apply_style_to_frame` que aplique el estilo artístico seleccionado a un frame de vídeo o a una imagen utilizando el modelo de transferencia de estilo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J56PV0FLWKWi"
      },
      "outputs": [],
      "source": [
        "def apply_style_to_frame(frame):\n",
        "    \"\"\"Aplica el estilo seleccionado a un frame.\"\"\"\n",
        "    global style_image\n",
        "    if style_image is None:\n",
        "        return frame  # Si no hay estilo seleccionado, devuelve el frame original\n",
        "\n",
        "    # Preprocesar el frame\n",
        "    content_image = tf.image.convert_image_dtype(frame, tf.float32) # Convertir los valores del frame a [0, 1]\n",
        "    content_image = tf.image.resize(content_image, (512, 512))      # Redimensionar el frame a 512x512 píxeles\n",
        "    content_image = content_image[tf.newaxis, :]    # Añadir una nueva dimensión para representar el batch size\n",
        "\n",
        "    # Aplicar el estilo\n",
        "    stylized_image = hub_model(tf.constant(content_image), tf.constant(style_image))[0]\n",
        "\n",
        "    # Convertir a formato OpenCV\n",
        "    stylized_image = tf.image.convert_image_dtype(stylized_image, tf.uint8)\n",
        "    stylized_image = stylized_image.numpy()\n",
        "\n",
        "    return stylized_image[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJTXsFK_WKWj"
      },
      "source": [
        "La siguiente función, `show_image_in_notebook`, se usa para mostrar cada frame en el Jupyter Notebook usando un widget de imagen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z31wVrhAWKWj"
      },
      "outputs": [],
      "source": [
        "def show_image_in_notebook(frame):\n",
        "    \"\"\"Muestra una imagen en el Jupyter Notebook.\"\"\"\n",
        "    _, img_encoded = cv2.imencode('.png', frame)    # Codificar la imagen en formato .png\n",
        "    img_bytes = img_encoded.tobytes()       # Covertir la imagen codificada a una secuencia de bytes\n",
        "    img_pil = PIL.Image.open(BytesIO(img_bytes))    # Convertir la secuencia de bytes en una imagen\n",
        "\n",
        "    # Mostrar imagen en el widget sin limpiar la salida\n",
        "    with BytesIO() as buffer:\n",
        "        img_pil.save(buffer, format=\"PNG\")\n",
        "        image_widget.value = buffer.getvalue()  # Asignar imagen al widget"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNdk-bLeWKWj"
      },
      "source": [
        "La función que se define a continuación, `capture_video`, se encarga de capturar el vídeo desde la Webcam, para luego aplicar el estilo artístico seleccionado a cada frame antes de mostrarlo en el Jupyter Notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tfW7dGblWKWj"
      },
      "outputs": [],
      "source": [
        "def capture_video():\n",
        "    \"\"\"Inicia la captura de video desde la Webcam y aplica el estilo en tiempo real.\"\"\"\n",
        "    global stop_camera\n",
        "    videoreader = cv2.VideoCapture(0)\n",
        "\n",
        "    # Capturar frames de la Webcam hasta que `stop_camera = True`\n",
        "    while not stop_camera:\n",
        "        (grabbed, frame) = videoreader.read()   # Leer el siguiente frame\n",
        "        if not grabbed:\n",
        "            break\n",
        "\n",
        "        # Convertir el frame a RGB\n",
        "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Aplicar el estilo al frame\n",
        "        styled_frame = apply_style_to_frame(frame)\n",
        "\n",
        "        # Convertir el frame de vuelta a BGR para mostrarlo con OpenCV\n",
        "        styled_frame = cv2.cvtColor(styled_frame, cv2.COLOR_RGB2BGR)\n",
        "\n",
        "        # Mostrar el fotograma estilizado en el widget\n",
        "        show_image_in_notebook(styled_frame)\n",
        "\n",
        "    videoreader.release()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DbNpu4xsWKWk"
      },
      "source": [
        "La función `start_webcam` se encarga de iniciar la ejecución de la cámara."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6tjAo00WKWk"
      },
      "outputs": [],
      "source": [
        "def start_webcam(change):\n",
        "    \"\"\"Inicia la ejecución de la cámara.\"\"\"\n",
        "    global stop_camera\n",
        "    stop_camera = False\n",
        "    thread = threading.Thread(target=capture_video)\n",
        "    thread.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qiomj6oZWKWk"
      },
      "source": [
        "Análogamente, la función `stop_webcam` es la encargada de detener la ejecución de la cámara."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BAk12r7LWKWk"
      },
      "outputs": [],
      "source": [
        "def stop_webcam(change):\n",
        "    \"\"\"Detiene la ejecución de la cámara.\"\"\"\n",
        "    global stop_camera\n",
        "    stop_camera = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJSNhL6RWKWl"
      },
      "source": [
        "Por último, la función `load_style` tiene como propósito cargar la imagen de estilo seleccionada."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl3XYum5WKWl"
      },
      "outputs": [],
      "source": [
        "def load_style(change):\n",
        "    \"\"\"Cargar la imagen de estilo.\"\"\"\n",
        "    global file_upload\n",
        "    global style_image\n",
        "\n",
        "    # Acceder al diccionario que está dentro de la tupla\n",
        "    uploaded_file = file_upload.value[0]\n",
        "\n",
        "    # Acceder al contenido de la imagen\n",
        "    content = uploaded_file['content']\n",
        "\n",
        "    # Guardar el archivo temporalmente\n",
        "    with open(\"temp_style_image.png\", \"wb\") as f:\n",
        "        f.write(content)\n",
        "\n",
        "    # Cargar la imagen\n",
        "    style_image = load_img(\"temp_style_image.png\")\n",
        "    print(\"Estilo cargado.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWfDrO4WWKWl"
      },
      "source": [
        "Una vez definidas todas estas funciones, ya podemos crear la interfaz interactiva. En la siguiente celda se configura dicha interfaz. Mediante `file_upload` se puede cargar la imagen de estilo. Además, los botones `start_button` y `stop_button` permiten iniciar y detener la Webcam, permitiendo la captura de vídeo en tiempo real. Finalmente, los resultados estilizados de la cámara se muestran en tiempo real a través de `image_widget`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231,
          "referenced_widgets": [
            "389ce237f86d4ee48d391f6da4eb5bfe",
            "e26f496f885f458aa4c0aed0240ad886",
            "8bcd0f207f1b4831960915fbaab4251a",
            "5c36105147cf48168cc07db646cddc57",
            "3fbc06bd2a434597893698f4a2ff150f",
            "2a15a63039434855aa6d5c6c5c7c0f95",
            "8ea0a0f34e344d54ac49abb7364099ab",
            "d5a79eaa4ba44e8597ce6db06263b369",
            "2140705545d24413b3913b94f57e43bd",
            "056277ba65a54e489b088261014f0a36",
            "59742750af904fffaf59e7b268b014fe",
            "56ffdd112be846dba6b58b50cf771d5a",
            "e4e2f6886bdd4e84aa3f3f978c192209",
            "6908a306120642179d2321409afad4a7",
            "44b6734e5cb54ee7ac9e6eb1444c0301",
            "501d28311c5a489ca66f622ba5bc7bd9",
            "258e15faf45048288b40dff627d4c15f"
          ]
        },
        "id": "aWribzKTWKWl",
        "outputId": "49e50bc0-f9a7-417b-db0c-33a55ce92cc9"
      },
      "outputs": [],
      "source": [
        "# Crear widgets\n",
        "file_upload = widgets.FileUpload(accept='image/*', multiple=False)\n",
        "start_button = widgets.Button(description=\"Iniciar Cámara\", button_style='success')\n",
        "stop_button = widgets.Button(description=\"Detener Cámara\", button_style='danger')\n",
        "\n",
        "# Conectar widgets con funciones\n",
        "file_upload.observe(load_style, names='value')\n",
        "start_button.on_click(start_webcam)\n",
        "stop_button.on_click(stop_webcam)\n",
        "\n",
        "# Mostrar widgets y el widget de imagen\n",
        "ipy_display(widgets.HTML(\"<h3>Interfaz de Estilización en Tiempo Real</h3>\"))\n",
        "ipy_display(widgets.HTML(\"<p>Carga una imagen para el estilo y presiona 'Iniciar Cámara'.</p>\"))\n",
        "ipy_display(file_upload)\n",
        "ipy_display(start_button)\n",
        "ipy_display(stop_button)\n",
        "ipy_display(image_widget)  # Mostrar el widget de imagen"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "056277ba65a54e489b088261014f0a36": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "success",
            "description": "Iniciar Cámara",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_59742750af904fffaf59e7b268b014fe",
            "style": "IPY_MODEL_56ffdd112be846dba6b58b50cf771d5a",
            "tooltip": ""
          }
        },
        "2140705545d24413b3913b94f57e43bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "258e15faf45048288b40dff627d4c15f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a15a63039434855aa6d5c6c5c7c0f95": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "389ce237f86d4ee48d391f6da4eb5bfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e26f496f885f458aa4c0aed0240ad886",
            "placeholder": "​",
            "style": "IPY_MODEL_8bcd0f207f1b4831960915fbaab4251a",
            "value": "<h3>Interfaz de Estilización en Tiempo Real</h3>"
          }
        },
        "3fbc06bd2a434597893698f4a2ff150f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44b6734e5cb54ee7ac9e6eb1444c0301": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "501d28311c5a489ca66f622ba5bc7bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ImageModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ImageModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ImageView",
            "format": "png",
            "height": "",
            "layout": "IPY_MODEL_258e15faf45048288b40dff627d4c15f",
            "width": ""
          }
        },
        "56ffdd112be846dba6b58b50cf771d5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "59742750af904fffaf59e7b268b014fe": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c36105147cf48168cc07db646cddc57": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fbc06bd2a434597893698f4a2ff150f",
            "placeholder": "​",
            "style": "IPY_MODEL_2a15a63039434855aa6d5c6c5c7c0f95",
            "value": "<p>Carga una imagen para el estilo y presiona 'Iniciar Cámara'.</p>"
          }
        },
        "6908a306120642179d2321409afad4a7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bcd0f207f1b4831960915fbaab4251a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ea0a0f34e344d54ac49abb7364099ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FileUploadModel",
          "state": {
            "_counter": 0,
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FileUploadModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "FileUploadView",
            "accept": "image/*",
            "button_style": "",
            "data": [],
            "description": "Upload",
            "description_tooltip": null,
            "disabled": false,
            "error": "",
            "icon": "upload",
            "layout": "IPY_MODEL_d5a79eaa4ba44e8597ce6db06263b369",
            "metadata": [],
            "multiple": false,
            "style": "IPY_MODEL_2140705545d24413b3913b94f57e43bd"
          }
        },
        "d5a79eaa4ba44e8597ce6db06263b369": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e26f496f885f458aa4c0aed0240ad886": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4e2f6886bdd4e84aa3f3f978c192209": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "danger",
            "description": "Detener Cámara",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6908a306120642179d2321409afad4a7",
            "style": "IPY_MODEL_44b6734e5cb54ee7ac9e6eb1444c0301",
            "tooltip": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
